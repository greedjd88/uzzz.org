<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>deeplearning &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/category/deeplearning/feed" rel="self" type="application/rss+xml" />
	<link>http://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Sat, 30 Jun 2018 10:54:16 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>deeplearning &#8211; 有组织在!</title>
	<link>http://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Learning to See in the Dark</title>
		<link>https://uzzz.org/article/1322.html</link>
				<pubDate>Sat, 30 Jun 2018 10:54:16 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[deeplearning]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1322.html</guid>
				<description><![CDATA[Learning to See in the Dark ref：http://web.engr.illinois.edu/~cchen156/SID.html 介绍 去噪、去模糊都有，不过在低光照下还是比较难的。高ISO可以提亮，但也会放大噪声。直接缩放或者拉伸直方图，也有一定效果，不过并不能弥补光子不足的缺陷。增加曝光时间，如果移动了会变模糊。这篇文章就是喜欢严重缺少亮度的低光照条件，最好曝光时间还短。传统的去噪方法不行，拍一系列暗图也不行，这些在极低光照条件下基本都会失效。 本文采用的是数据驱动的方法，设计了一个深度神经网络，能实现颜色转换、去马赛克、减少噪声和图像增强。这种端到端的设计能减少噪声的放大和误差的积累。这篇文章提到以往大家做低光照的研究都是用的合成图像，或者没有ground truth的低光照，因此他们就收集了一些与低光照对应的清晰图像（可作为benchmark）。 相关工作： 去噪：全变分、小波域变换、稀疏编码、核范数最小化、BM3D（光滑、稀疏、低秩、自适应）。缺点：数据集一般都是合成的。一般认为BM3D在真实图片的表现结果比大部分其他算法好（Benchmarking denoising algorithms with real photographs）。多图效果不错，但本文想单图。 低光照图像增强：直方图均衡化、图像灰度校正（伽马校正）、暗通道反转、小波变换、Retinex model、光强映射估计。这些方法都认为退化图已经包含比较好的潜在信息，而没有考虑噪声和颜色扭曲的影响。 含噪数据集：RENOIR对应图片不是完美匹配、HDR+没有极低光照照片、DND也是白天获取的。本文收集了一些数据。 “看见黑暗”数据集 数据集是作者采集的，有5094张低曝光和高曝光的数据集。场景包含室内室外，都是在固定位置用三脚架拍的。采用app远程设置光圈、ISO等参数，室外光照在0.2勒克斯和5勒克斯之间，室内的更加黑暗。数据采集的原则是高曝光的只要保持视觉效果好即可，不一味追求移除全部噪声。他们称自己的数据集叫SID 方法 pipeline 部分方法流程图，其中L3指local,linear and learned filters。传统和L3都没有很好处理极低光照的情况。burst序列虽然可以满足一定的需求，但比较苛刻，而且需要“lucky imaging”。 本文用的是全卷积网络（FCN），而且不是处理普通的sRGB图片，而是用原始的传感器数据。 文章用到多尺度聚集网络（multi-scale context aggregation network (CAN)）和U-net（U-net: Convolutional networks for biomedical image segmentation），其中U-net是本文默认的网络。其他方法如残差并不适合这里，也许是因为色彩空间不一样。另外要尽量避免全连接层，因为完整的分辨率图可能有6000&#215;4000。 放大率决定了输出的亮度，这是输入的时候提供的，网络的最终输出直接就是sRGB空间。 训练 L1和Adam，剪裁成512，学习率1e-4到1e-5，共4000个epoch。 实验 质量和感知 与传统的比，传统的含噪严重，颜色扭曲。 他说BM3D在真实图片的去噪效果好，所以就用BM3D来比。不过BM3D需要手动输入一个预估的噪声等级，对结果非常有影响。 实验结果放到Amazon Mechanical Turk平台上对比，由10个工作者来完成。 控制变量实验 用CAN替换U-net、用sRGB替换原始信号，SSIM或L2替换L1等等。其中不用原始信号那个掉得最快，其余的差并不太多。 整体的实验效果显然这个算法无敌，看起来就和白天一样。有时候会稍微过平滑一点，不过看上去视觉效果都还可以。 讨论 低光照成像有很少的光子数量和低信噪比。本文设计了数据驱动的方法，能有效提高成像表现，还贡献了SID数据集。 未来的方向：没有处理亮度域的映射，可能会损失一些两端的信息。缺少动态物体。放大倍数也要人为来选，以后可以让它像自动ISO那样。 算法也不够实时，在两个数据集上需要0.38或0.66秒。 我个人觉得这个算法是很不错的benchmark，但是数据集实在太大了，而且RAM的消耗非常大。]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<h1 id="learning-to-see-in-the-dark">Learning to See in the Dark</h1>
<hr>
<p>ref：<a href="http://web.engr.illinois.edu/~cchen156/SID.html" rel="nofollow" data-token="ca78d6398980dcfc7fc81942fe752f50">http://web.engr.illinois.edu/~cchen156/SID.html</a></p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180630185255679?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hpdWRhd24=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" title=""></p>
<h2 id="介绍">介绍</h2>
<p>去噪、去模糊都有，不过在低光照下还是比较难的。高ISO可以提亮，但也会放大噪声。直接缩放或者拉伸直方图，也有一定效果，不过并不能弥补光子不足的缺陷。增加曝光时间，如果移动了会变模糊。这篇文章就是喜欢严重缺少亮度的低光照条件，最好曝光时间还短。传统的去噪方法不行，拍一系列暗图也不行，这些在极低光照条件下基本都会失效。</p>
<p>本文采用的是数据驱动的方法，设计了一个深度神经网络，能实现颜色转换、去马赛克、减少噪声和图像增强。这种端到端的设计能减少噪声的放大和误差的积累。这篇文章提到以往大家做低光照的研究都是用的合成图像，或者没有ground truth的低光照，因此他们就收集了一些与低光照对应的清晰图像（可作为benchmark）。</p>
<h2 id="相关工作">相关工作：</h2>
<p>去噪：全变分、小波域变换、稀疏编码、核范数最小化、BM3D（光滑、稀疏、低秩、自适应）。缺点：数据集一般都是<strong>合成</strong>的。一般认为BM3D在真实图片的表现结果比大部分其他算法好（Benchmarking denoising algorithms with real photographs）。多图效果不错，但本文想<strong>单图</strong>。</p>
<p>低光照图像增强：直方图均衡化、图像灰度校正（伽马校正）、暗通道反转、小波变换、Retinex model、光强映射估计。这些方法都认为退化图已经包含比较好的潜在信息，而没有考虑噪声和颜色扭曲的影响。</p>
<p>含噪数据集：RENOIR对应图片不是完美匹配、HDR+没有极低光照照片、DND也是白天获取的。本文收集了一些数据。</p>
<h2 id="看见黑暗数据集">“看见黑暗”数据集</h2>
<p>数据集是作者采集的，有5094张低曝光和高曝光的数据集。场景包含室内室外，都是在固定位置用三脚架拍的。采用app远程设置光圈、ISO等参数，室外光照在0.2勒克斯和5勒克斯之间，室内的更加黑暗。数据采集的原则是高曝光的只要保持视觉效果好即可，不一味追求移除全部噪声。他们称自己的数据集叫SID</p>
<h2 id="方法">方法</h2>
<h3 id="pipeline">pipeline</h3>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180630185335663?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hpdWRhd24=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" title=""></p>
<p>部分方法流程图，其中L3指local,linear and learned filters。传统和L3都没有很好处理极低光照的情况。burst序列虽然可以满足一定的需求，但比较苛刻，而且需要“lucky imaging”。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/201806301853432?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hpdWRhd24=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" title=""></p>
<p>本文用的是全卷积网络（FCN），而且不是处理普通的sRGB图片，而是用原始的传感器数据。</p>
<p>文章用到多尺度聚集网络（multi-scale context aggregation network (CAN)）和U-net（U-net: Convolutional networks for biomedical image segmentation），其中U-net是本文默认的网络。其他方法如残差并不适合这里，也许是因为色彩空间不一样。另外要尽量避免全连接层，因为完整的分辨率图可能有6000&#215;4000。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180630185352956?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hpdWRhd24=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" title=""></p>
<p>放大率决定了输出的亮度，这是输入的时候提供的，网络的最终输出直接就是sRGB空间。</p>
<h3 id="训练">训练</h3>
<p>L1和Adam，剪裁成512，学习率1e-4到1e-5，共4000个epoch。</p>
<h2 id="实验">实验</h2>
<h3 id="质量和感知">质量和感知</h3>
<p>与传统的比，传统的含噪严重，颜色扭曲。</p>
<p>他说BM3D在真实图片的去噪效果好，所以就用BM3D来比。不过BM3D需要手动输入一个预估的噪声等级，对结果非常有影响。</p>
<p>实验结果放到Amazon Mechanical Turk平台上对比，由10个工作者来完成。</p>
<h3 id="控制变量实验">控制变量实验</h3>
<p>用CAN替换U-net、用sRGB替换原始信号，SSIM或L2替换L1等等。其中不用原始信号那个掉得最快，其余的差并不太多。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180630185404181?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hpdWRhd24=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" title=""></p>
<p>整体的实验效果显然这个算法无敌，看起来就和白天一样。有时候会稍微过平滑一点，不过看上去视觉效果都还可以。</p>
<h2 id="讨论">讨论</h2>
<p>低光照成像有很少的光子数量和低信噪比。本文设计了数据驱动的方法，能有效提高成像表现，还贡献了SID数据集。</p>
<p>未来的方向：没有处理亮度域的映射，可能会损失一些两端的信息。缺少动态物体。放大倍数也要人为来选，以后可以让它像自动ISO那样。</p>
<p>算法也不够实时，在两个数据集上需要0.38或0.66秒。</p>
<p>我个人觉得这个算法是很不错的benchmark，但是数据集实在太大了，而且RAM的消耗非常大。</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
