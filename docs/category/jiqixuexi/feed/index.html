<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>机器学习 &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/category/jiqixuexi/feed" rel="self" type="application/rss+xml" />
	<link>http://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Sun, 07 Jul 2019 01:52:26 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>机器学习 &#8211; 有组织在!</title>
	<link>http://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Graph Convolutional Neural Networks for Web-Scale Recommender Systems（用于Web级推荐系统的图形卷积神经网络）</title>
		<link>https://uzzz.org/article/1588.html</link>
				<pubDate>Sun, 07 Jul 2019 01:52:26 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[AI程序员]]></category>
		<category><![CDATA[GNN]]></category>
		<category><![CDATA[人工智能应用开发]]></category>
		<category><![CDATA[推荐系统]]></category>
		<category><![CDATA[机器学习]]></category>
		<category><![CDATA[深度学习]]></category>
		<category><![CDATA[算法]]></category>
		<category><![CDATA[论文研读]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1588.html</guid>
				<description><![CDATA[Graph Convolutional Neural Networks for Web-Scale Recommender Systems 用于Web级推荐系统的图形卷积神经网络 ABSTRACT Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We deploy PinSage at Pinterest and train it on 7.5 billion exam-ples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<h4><a id="Graph_Convolutional_Neural_Networks_for_WebScale_Recommender_Systems_0"></a>Graph Convolutional Neural Networks for Web-Scale Recommender Systems</h4>
<h3><a id="Web_2"></a>用于Web级推荐系统的图形卷积神经网络</h3>
<h6><a id="ABSTRACT_4"></a>ABSTRACT</h6>
<p>Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge.</p>
<p>Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model.</p>
<p>We deploy PinSage at Pinterest and train it on 7.5 billion exam-ples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embed-dings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.<br /> 用于图形结构数据的深度神经网络的最新进展已经在推荐器系统基准上产生了最先进的性能。然而，使这些方法实用且可扩展到具有数十亿项目和数亿用户的网络规模推荐任务仍然是一项挑战。</p>
<p>在这里，我们描述了我们在Pinterest开发和部署的大规模深度推荐引擎。我们开发了一种数据有效的图形卷积网络（GCN）算法PinSage，它结合了有效的随机游走和图形卷积，以生成包含图形结构和节点特征信息的节点（即项目）的嵌入。与之前的GCN方法相比，我们开发了一种基于高效随机游走的新方法来构建卷积并设计一种新的训练策略，该策略依赖于更难和更难的训练示例来提高模型的鲁棒性和收敛性。</p>
<p>我们在Pinterest部署了PinSage，并在图表上培训了75亿个例子，其中30亿个节点代表引脚和电路板，以及180亿个边缘。根据离线指标，用户研究和A / B测试，PinSage可提供比可比较的深度学习和基于图形的替代方案更高质量的建议。据我们所知，这是迄今为止最大的深度图嵌入应用，并为基于图卷积结构的新一代Web级推荐系统铺平了道路。</p>
<h4><a id="1	INTRODUCTION_17"></a>1 INTRODUCTION</h4>
<p>Deep learning methods have an increasingly critical role in rec-ommender system applications, being used to learn useful low-dimensional embeddings of images, text, and even individual users [9, 12]. The representations learned using deep models can be used to complement, or even replace, traditional recommendation algo-rithms like collaborative filtering. and these learned representations have high utility because they can be re-used in various recom-mendation tasks. For example, item embeddings learned using a deep model can be used for item-item recommendation and also to recommended themed collections (e.g., playlists, or “feed” content).</p>
<p>Recent years have seen significant developments in this space— especially the development of new deep learning methods that are capable of learning on graph-structured data, which is fundamen-tal for recommendation applications (e.g., to exploit user-to-item interaction graphs as well as social graphs) [6, 19, 21, 24, 29, 30].</p>
<p>Most prominent among these recent advancements is the suc-cess of deep learning architectures known as Graph Convolutional Networks (GCNs) [19, 21, 24, 29]. The core idea behind GCNs is to learn how to iteratively aggregate feature information from lo-cal graph neighborhoods using neural networks (Figure 1). Here a single “convolution” operation transforms and aggregates feature information from a node’s one-hop graph neighborhood, and by stacking multiple such convolutions information can be propagated across far reaches of a graph. Unlike purely content-based deep models (e.g., recurrent neural networks [3]), GCNs leverage both content information as well as graph structure. GCN-based methods have set a new standard on countless recommender system bench-marks (see [19] for a survey). However, these gains on benchmark tasks have yet to be translated to gains in real-world production environments.</p>
<p>深度学习方法在调用系统应用程序中具有越来越重要的作用，用于学习图像，文本甚至个人用户的有用的低维嵌入[9,12]。使用深度模型学习的表示可用于补充甚至替代传统的推荐算法，如协同过滤。并且这些学习的表示具有很高的实用性，因为它们可以在各种推荐任务中重复使用。例如，使用深度模型学习的项目嵌入可以用于项目项目推荐以及推荐的主题集合（例如，播放列表或“馈送”内容）。</p>
<p>近年来，这一领域取得了重大进展 &#8211; 尤其是新的深度学习方法的开发，这些方法能够学习图形结构数据，这是推荐应用的基础（例如，利用用户到项目的交互图形以及社交图表）[6,19,21,24,29,30]。</p>
<p>这些最新进展中最突出的是深度学习架构的成功，称为图形卷积网络（GCN）[19,21,24,29]。 GCN背后的核心思想是学习如何使用神经网络从lo-cal图形邻域迭代地聚合特征信息（图1）。这里，单个“卷积”操作转换并聚合来自节点的单跳图邻域的特征信息，并且通过堆叠多个这样的卷积信息可以在图的远端传播。与纯粹基于内容的深层模型（例如，递归神经网络[3]）不同，GCN利用内容信息和图形结构。基于GCN的方法为无数的推荐系统基准设定了新的标准（参见[19]的一项调查）。但是，基准任务的这些收益尚未转化为实际生产环境中的收益。</p>
<p>The main challenge is to scale both the training as well as in-ference of GCN-based node embeddings to graphs with billions of nodes and tens of billions of edges. Scaling up GCNs is difficult because many of the core assumptions underlying their design are violated when working in a big data environment. For example, all existing GCN-based recommender systems require operating on the full graph Laplacian during training—an assumption that is infeasible when the underlying graph has billions of nodes and whose structure is constantly evolving.</p>
<p>Present work. Here we present a highly-scalable GCN framework that we have developed and deployed in production at Pinterest. Our framework, a random-walk-based GCN named PinSage, operates on a massive graph with 3 billion nodes and 18 billion edges—a graph that is 10, 000× larger than typical applications of GCNs. PinSage leverages several key insights to drastically improve the scalability of GCNs:<br /> 主要的挑战是将基于GCN的节点嵌入的训练和推理扩展到具有数十亿个节点和数百亿个边缘的图形。 扩展GCN很困难，因为在大数据环境中工作时，其设计的许多核心假设都会受到侵犯。 例如，所有现有的基于GCN的推荐系统都需要在训练期间对完整图拉普拉斯算子进行操作 &#8211; 当基础图具有数十亿个节点且其结构不断发展时，这种假设是不可行的。</p>
<p>目前的工作。 在这里，我们提出了一个高度可扩展的GCN框架，我们在Pinterest的生产中开发和部署了该框架。 我们的框架是一个名为PinSage的基于随机游走的GCN，它运行在一个包含30亿个节点和180亿个边缘的大型图形上 &#8211; 图形比GCN的典型应用程序大10,000倍。 PinSage利用几个关键见解来大幅提高GCN的可扩展性：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707091551510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707091641450.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 图1：使用深度2卷积的模型架构概述（最好以彩色查看）。 左：一个小例子输入</p>
<p>图形。 右：使用前一层表示计算节点A的嵌入hA（2）的2层神经网络，<br /> 节点A的hA（1）和其邻域N（A）（节点B，C，D）的hA（1）。 （然而，邻域的概念是通用的，并不是所有邻居都需要包括在内（第3.2节）。）底部：计算输入图的每个节点的嵌入的神经网络。 虽然神经网络在节点之间不同，但它们都共享相同的参数集（即，卷积（1）和卷积（2）函数的参数;算法1）。 具有相同阴影图案的框共享参数; γ表示重要性汇集函数; 薄矩形框表示密集连接的多层神经网络。</p>
<ul>
<li>
<p>•On-the-fly convolutions: Traditional GCN algorithms per-form graph convolutions by multiplying feature matrices by powers of the full graph Laplacian. In contrast, our PinSage algo-rithm performs efficient, localized convolutions by sampling the neighborhood around a node and dynamically constructing a computation graph from this sampled neighborhood. These dy-namically constructed computation graphs (Fig. 1) specify how to perform a localized convolution around a particular node, and alleviate the need to operate on the entire graph during training.</p>
</li>
<li>
<p>•Producer-consumer minibatch construction: We develop a producer-consumer architecture for constructing minibatches that ensures maximal GPU utilization during model training. A large-memory, CPU-bound producer efficiently samples node network neighborhoods and fetches the necessary features to define local convolutions, while a GPU-bound TensorFlow model consumes these pre-defined computation graphs to efficiently run stochastic gradient decent.<br /> •Efficient MapReduce inference: Given a fully-trained GCN model, we design an efficient MapReduce pipeline that can dis-tribute the trained model to generate embeddings for billions of nodes, while minimizing repeated computations.</p>
</li>
</ul>
<p>In addition to these fundamental advancements in scalability, we also introduce new training techniques and algorithmic innova-tions. These innovations improve the quality of the representations learned by PinSage, leading significant performance gains in down-stream recommender system tasks:</p>
<ul>
<li>Constructing convolutions via random walks: Taking full neighborhoods of nodes to perform convolutions (Fig. 1) would result in huge computation graphs, so we resort to sampling. However, random sampling is suboptimal, and we develop a new technique using short random walks to sample the computa-tion graph. An additional benefit is that each node now has an importance score, which we use in the pooling/aggregation step.</li>
<li>•Importance pooling: A core component of graph convolutions is the aggregation of feature information from local neighbor-hoods in the graph. We introduce a method to weigh the impor-tance of node features in this aggregation based upon random-walk similarity measures, leading to a 46% performance gain in offline evaluation metrics.</li>
<li>•Curriculum training: We design a curriculum training scheme, where the algorithm is fed harder-and-harder examples during training, resulting in a 12% performance gain.<br /> We have deployed PinSage for a variety of recommendation tasks at Pinterest, a popular content discovery and curation appli-cation where users interact with pins, which are visual bookmarks to online content (e.g., recipes they want to cook, or clothes they want to purchase). Users organize these pins into boards, which con-tain collections of similar pins. Altogether, Pinterest is the world’s largest user-curated graph of images, with over 2 billion unique pins collected into over 1 billion boards.</li>
</ul>
<p>Through extensive offline metrics, controlled user studies, and A/B tests, we show that our approach achieves state-of-the-art performance compared to other scalable deep content-based rec-ommendation algorithms, in both an item-item recommendation task (i.e., related-pin recommendation), as well as a “homefeed” recommendation task. In offline ranking metrics we improve over the best performing baseline by more than 40%, in head-to-head human evaluations our recommendations are preferred about 60% of the time, and the A/B tests show 30% to 100% improvements in user engagement across various settings.</p>
<p>To our knowledge, this is the largest-ever application of deep graph embeddings and paves the way for new generation of rec-ommendation systems based on graph convolutional architectures.</p>
<ul>
<li>
<p>动态卷积：传统的GCN算法通过将特征矩阵乘以完整图拉普拉斯算子的幂来进行每个图形卷积。相比之下，我们的PinSage算法通过对节点周围的邻域进行采样并从该采样邻域动态构建计算图来执行高效的局部卷积。这些动态构建的计算图（图1）指定了如何在特定节点周围执行局部卷积，并减少了在训练期间对整个图进行操作的需要。</p>
</li>
<li>
<p>生产者 &#8211; 消费者小批量建设：我们开发了一个生产者 &#8211; 消费者体系结构，用于构建微型计算机，确保在模型培训期间最大限度地利用GPU。一个大内存，受CPU限制的生产者有效地采样节点网络邻域并获取必要的特征来定义局部卷积，而受GPU约束的TensorFlow模型使用这些预定义的计算图来有效地运行随机梯度体面。</p>
</li>
<li>
<p>高效的MapReduce推理：给定一个完全训练的GCN模型，我们设计了一个有效的MapReduce管道，可以分解训练的模型，为数十亿个节点生成嵌入，同时最大限度地减少重复计算。</p>
</li>
</ul>
<p>除了可扩展性的这些基本进步之外，我们还引入了新的培训技术和算法创新。这些创新提高了PinSage所学习的表示质量，在下游推荐系统任务中带来了显着的性能提升：</p>
<ul>
<li>通过随机游走构建卷积：使用完整的节点邻域来执行卷积（图1）将导致巨大的计算图，因此我们求助于采样。然而，随机抽样不是最理想的，我们开发了一种使用短随机游走来抽样计算图的新技术。另一个好处是每个节点现在都有一个重要性分数，我们在池化/聚合步骤中使用它。</li>
<li>•重要性池：图卷的核心组件是图中本地邻居的特征信息的聚合。我们引入了一种方法来基于随机游走相似性度量来衡量此聚合中节点特征的重要性，从而使离线评估指标的性能提高46％。</li>
<li>课程培训：我们设计了一个课程培训方案，在培训过程中，算法得到了越来越难的实例，从而使性能提高了12％。<br /> 我们在Pinterest上部署了PinSage用于各种推荐任务，Pinterest是一种流行的内容发现和策展应用，用户可以在其中与引脚交互，引脚是在线内容的可视书签（例如，他们想要烹饪的食谱，或者他们想要购买的衣服） ）。用户将这些引脚组织成板，其中包含类似引脚的集合。总而言之，Pinterest是世界上最大的用户策划图像图形，超过20亿个独特的引脚被收集到超过10亿个电路板中。</li>
</ul>
<p>通过广泛的离线指标，受控用户研究和A / B测试，我们表明，与项目项目推荐任务中的其他可扩展的基于深度内容的推荐算法相比，我们的方法实现了最先进的性能（即相关引脚推荐），以及“主页”推荐任务。在离线排名指标中，我们在最佳绩效基线上的表现提高了40％以上，在人机对话评估中，我们的建议在60％的时间内是首选，A / B测试显示在30％到100％的情况下，用户参与各种设置。</p>
<p>据我们所知，这是有史以来最大的深度图嵌入应用，并为基于图卷积结构的新一代推荐系统铺平了道路。</p>
<h3><a id="2	RELATED_WORK_81"></a>2 RELATED WORK</h3>
<p>Our work builds upon a number of recent advancements in deep learning methods for graph-structured data.</p>
<p>The notion of neural networks for graph data was first outlined in Gori et al. (2005) [15] and further elaborated on in Scarselli et al. (2009) [27]. However, these initial approaches to deep learning on graphs required running expensive neural “message-passing” algorithms to convergence and were prohibitively expensive on large graphs. Some limitations were addressed by Gated Graph Sequence Neural Networks [22]—which employs modern recurrent neural architectures—but the approach remains computationally expensive and has mainly been used on graphs with &lt;10, 000 nodes.</p>
<p>More recently, there has been a surge of methods that rely on the notion of “graph convolutions” or Graph Convolutional Net-works (GCNs). This approach originated with the work of Bruna et al. (2013), which developed a version of graph convolutions based on spectral graph thery [7]. Following on this work, a number of authors proposed improvements, extensions, and approximations of these spectral convolutions [6, 10, 11, 13, 18, 21, 24, 29, 31], lead-ing to new state-of-the-art results on benchmarks such as node classification, link prediction, as well as recommender system tasks (e.g., the MovieLens benchmark [24]). These approaches have con-sistently outperformed techniques based upon matrix factorization or random walks (e.g., node2vec [17] and DeepWalk [26]), and their success has led to a surge of interest in applying GCN-based methods to applications ranging from recommender systems [24] to drug design [20, 31]. Hamilton et al. (2017b) [19] and Bronstein et al. (2017) [6] provide comprehensive surveys of recent advancements.</p>
<p>However, despite the successes of GCN algorithms, no previous works have managed to apply them to production-scale data with billions of nodes and edges—a limitation that is primarily due to the fact that traditional GCN methods require operating on the entire graph Laplacian during training. Here we fill this gap and show that GCNs can be scaled to operate in a production-scale recommender system setting involving billions of nodes/items. Our work also demonstrates the substantial impact that GCNs have on recommendation performance in a real-world environment.</p>
<p>In terms of algorithm design, our work is most closely related to Hamilton et al. (2017a)’s GraphSAGE algorithm [18] and the closely related follow-up work of Chen et al. (2018) [8]. GraphSAGE is an inductive variant of GCNs that we modify to avoid operating on the entire graph Laplacian. We fundamentally improve upon GraphSAGE by removing the limitation that the whole graph be stored in GPU memory, using low-latency random walks to sample<br /> 我们的工作建立在图形结构数据深度学习方法的最新进展之上。</p>
<p>Gori等人首次概述了图数据的神经网络概念。 （2005）[15]并在Scarselli等人的文章中进一步阐述。 （2009）[27]。然而，这些在图上进行深度学习的初始方法需要运行昂贵的神经“消息传递”算法来收敛，并且在大图上非常昂贵。门控图形序列神经网络[22]解决了一些局限性 &#8211; 它采用了现代的递归神经架构 &#8211; 但该方法计算成本仍然很高，主要用于节点数&lt;10,000的图形。</p>
<p>最近，出现了大量依赖“图形卷积”或图形卷积网络（GCN）概念的方法。这种方法起源于Bruna等人的工作。 （2013），基于光谱图[7]开发了一个图形卷积的版本。继这项工作之后，许多作者提出了这些光谱卷积的改进，扩展和近似[6,10,11,13,18,21,24,29,31]，导致了新的状态。 -art结果基于节点分类，链接预测以及推荐系统任务（例如，MovieLens基准测试[24]）等基准测试。这些方法始终优于基于矩阵分解或随机游走的技术（例如，node2vec [17]和DeepWalk [26]），并且它们的成功引起了人们对将基于GCN的方法应用于推荐器等应用的兴趣。系统[24]到药物设计[20,31]。汉密尔顿等人。 （2017b）[19]和Bronstein等。 （2017）[6]提供了最近进展的综合调查。</p>
<p>然而，尽管GCN算法取得了成功，但之前的工作没有设法将它们应用于具有数十亿节点和边缘的生产规模数据 &#8211; 这主要是因为传统的GCN方法需要在整个图形拉普拉斯算子上运行。训练。在这里，我们填补了这一空白，并表明GCN可以扩展到在涉及数十亿节点/项目的生产规模推荐系统设置中运行。我们的工作还证明了GCN对现实环境中的推荐性能产生的重大影响。</p>
<p>在算法设计方面，我们的工作与Hamilton等人的关系最为密切。 （2017a）的GraphSAGE算法[18]和陈等人的密切相关的后续工作。 （2018）[8]。 GraphSAGE是GCN的归纳变体，我们修改它以避免在整个图拉普拉斯算子上运行。我们通过消除整个图存储在GPU内存中的限制，从而使用低延迟随机游走来获取样本，从根本上改进了GraphSAGE</p>
<p>graph neighborhoods in a producer-consumer architecture. We also introduce a number of new training techniques to improve performance and a MapReduce inference pipeline to scale up to graphs with billions of nodes.</p>
<p>Lastly, also note that graph embedding methods like node2vec</p>
<p>[17]and DeepWalk [26] cannot be applied here. First, these are unsupervised methods. Second, they cannot include node feature information. Third, they directly learn embeddings of nodes and thus the number of model parameters is linear with the size of the graph, which is prohibitive for our setting.<br /> 生产者 &#8211; 消费者体系结构中的图形邻域。 我们还介绍了许多用于提高性能的新培训技术和MapReduce推理管道，以扩展到具有数十亿节点的图形。</p>
<p>最后，还要注意图形嵌入方法，如node2vec</p>
<p>[17]和DeepWalk [26]不能在这里应用。 首先，这些是无监督的方法。 其次，它们不能包含节点特征信息。 第三，它们直接学习节点的嵌入，因此模型参数的数量与图形的大小成线性关系，这对我们的设置来说是禁止的。</p>
<h3><a id="3_METHOD_114"></a>3 METHOD</h3>
<p>In this section, we describe the technical details of the PinSage archi-tecture and training, as well as a MapReduce pipeline to efficiently generate embeddings using a trained PinSage model.</p>
<p>The key computational workhorse of our approach is the notion of localized graph convolutions.1 To generate the embedding for a node (i.e., an item), we apply multiple convolutional modules that aggregate feature information (e.g., visual, textual features) from the node’s local graph neighborhood (Figure 1). Each module learns how to aggregate information from a small graph neighbor-hood, and by stacking multiple such modules, our approach can gain information about the local network topology. Importantly, parameters of these localized convolutional modules are shared across all nodes, making the parameter complexity of our approach independent of the input graph size.</p>
<p>在本节中，我们将介绍PinSage架构和培训的技术细节，以及使用经过训练的PinSage模型高效生成嵌入的MapReduce管道。</p>
<p>我们方法的关键计算主力是局部图卷积的概念.1为了生成节点（即项目）的嵌入，我们应用多个卷积模块来聚合来自节点的特征信息（例如，视觉，文本特征）。 局部图邻域（图1）。 每个模块都学习如何聚合来自小图邻居的信息，并通过堆叠多个这样的模块，我们的方法可以获得有关本地网络拓扑的信息。 重要的是，这些局部卷积模块的参数在所有节点之间共享，使得我们方法的参数复杂度与输入图形大小无关。</p>
<h5><a id="31	Problem_Setup_124"></a>3.1 Problem Setup</h5>
<p>Pinterest is a content discovery application where users interact with pins, which are visual bookmarks to online content (e.g., recipes they want to cook, or clothes they want to purchase). Users organize these pins into boards, which contain collections of pins that the user deems to be thematically related. Altogether, the Pinterest graph contains 2 billion pins, 1 billion boards, and over 18 billion edges (i.e., memberships of pins to their corresponding boards).</p>
<p>Our task is to generate high-quality embeddings or representa-tions of pins that can be used for recommendation (e.g., via nearest-neighbor lookup for related pin recommendation, or for use in a downstream re-ranking system). In order to learn these embed-dings, we model the Pinterest environment as a bipartite graph consisting of nodes in two disjoint sets, I (containing pins) and C (containing boards). Note, however, that our approach is also naturally generalizable, with I being viewed as a set of items and C as a set of user-defined contexts or collections.</p>
<p>In addition to the graph structure, we also assume that the pins/items u ∈ I are associated with real-valued attributes, xu ∈ Rd . In general, these attributes may specify metadata or content information about an item, and in the case of Pinterest, we have that pins are associated with both rich text and image features. Our goal is to leverage both these input attributes as well as the structure of the bipartite graph to generate high-quality embed-dings. These embeddings are then used for recommender system candidate generation via nearest neighbor lookup (i.e., given a pin, find related pins) or as features in machine learning systems for ranking the candidates.<br /> Pinterest是一种内容发现应用程序，其中用户与引脚交互，引脚是在线内容的可视书签（例如，他们想要烹饪的食谱，或者他们想要购买的衣服）。用户将这些引脚组织成电路板，其中包含用户认为与主题相关的引脚集合。总而言之，Pinterest图包含20亿个引脚，10亿个电路板和超过180亿个边沿（即，引脚到其相应电路板的成员资格）。</p>
<p>我们的任务是生成可用于推荐的高质量嵌入或引脚表示（例如，通过最近邻居查找以获得相关引脚推荐，或用于下游重新排序系统）。为了学习这些嵌入，我们将Pinterest环境建模为由两个不相交集合中的节点组成的二分图，I（包含引脚）和C（包含板）。但请注意，我们的方法也可以自然地推广，我将其视为一组项目，将C视为一组用户定义的上下文或集合。</p>
<p>除了图形结构之外，我们还假设引脚/项u∈I与实值属性xu∈Rd相关联。通常，这些属性可以指定关于项目的元数据或内容信息，并且在Pinterest的情况下，我们将这些引脚与富文本和图像特征相关联。我们的目标是利用这些输入属性以及二分图的结构来生成高质量的嵌入式数据。然后，这些嵌入用于通过最近邻居查找生成推荐系统候选（即，给定引脚，找到相关引脚）或用作机器学习系统中用于对候选进行排序的特征。</p>
<p>For notational convenience and generality, when we describe the PinSage algorithm, we simply refer to the node set of the full graph with V = I ∪ C and do not explicitly distinguish between pin and board nodes (unless strictly necessary), using the more general term “node” whenever possible.<br /> 为了符号方便性和通用性，当我们描述PinSage算法时，我们简单地引用完整图的节点集，其中V =I∪C并且没有明确区分引脚和板节点（除非严格必要），使用更一般 尽可能使用术语“节点”。</p>
<h5><a id="32	Model_Architecture_139"></a>3.2 Model Architecture</h5>
<p>We use localized convolutional modules to generate embeddings for nodes. We start with input node features and then learn neural networks that transform and aggregate features over the graph to compute the node embeddings (Figure 1).</p>
<p><strong>Forward propagation algorithm.</strong> We consider the task of gener-ating an embedding, zu for a node u, which depends on the node’s input features and the graph structure around this node.<br /> 我们使用局部卷积模块为节点生成嵌入。 我们从输入节点特征开始，然后学习神经网络，在图形上转换和聚合特征以计算节点嵌入（图1）。</p>
<p><strong>前向传播算法</strong> 我们考虑为节点u生成嵌入，zu的任务，这取决于节点的输入特征和该节点周围的图形结构。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707091950238.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>The core of our PinSage algorithm is a localized convolution operation, where we learn how to aggregate information from u’s neighborhood (Figure 1). This procedure is detailed in Algorithm 1 convolve. The basic idea is that we transform the representations zv , ∀v ∈ N(u) of u’s neighbors through a dense neural network and then apply a aggregator/pooling fuction (e.g., a element-wise mean or weighted sum, denoted as γ ) on the resulting set of vectors (Line 1). This aggregation step provides a vector representation, nu , of u’s local neighborhood, N(u). We then concatenate the ag-gregated neighborhood vector nu with u’s current representation hu and transform the concatenated vector through another dense neural network layer (Line 2). Empirically we observe significant performance gains when using concatenation operation instead of the average operation as in [21]. Additionally, the normalization in Line 3 makes training more stable, and it is more efficient to perform approximate nearest neighbor search for normalized embeddings (Section 3.5). The output of the algorithm is a representation of u that incorporates both information about itself and its local graph neighborhood.</p>
<p>Importance-based neighborhoods. An important innovation in our approach is how we define node neighborhoods N(u), i.e., how we select the set of neighbors to convolve over in Algorithm 1. Whereas previous GCN approaches simply examine k-hop graph neighborhoods, in PinSage we define importance-based neighbor-hoods, where the neighborhood of a node u is defined as the T nodes that exert the most influence on node u. Concretely, we simulate random walks starting from node u and compute the L1-normalized</p>
<p>visit count of nodes visited by the random walk [14].2 The neigh-borhood of u is then defined as the top T nodes with the highest normalized visit counts with respect to node u.</p>
<p>The advantages of this importance-based neighborhood defi-nition are two-fold. First, selecting a fixed number of nodes to aggregate from allows us to control the memory footprint of the algorithm during training [18]. Second, it allows Algorithm 1 to take into account the importance of neighbors when aggregating the vector representations of neighbors. In particular, we imple-ment γ in Algorithm 1 as a weighted-mean, with weights defined according to the L1 normalized visit counts. We refer to this new approach as importance pooling.</p>
<p>Stacking convolutions. Each time we apply the convolve opera-tion (Algorithm 1) we get a new representation for a node, and we can stack multiple such convolutions on top of each other in order to gain more information about the local graph structure around node u. In particular, we use multiple layers of convolutions, where the inputs to the convolutions at layer k depend on the representa-tions output from layer k − 1 (Figure 1) and where the initial (i.e., “layer 0”) representations are equal to the input node features. Note that the model parameters in Algorithm 1 (Q, q, W, and w) are shared across the nodes but differ between layers.<br /> 我们的PinSage算法的核心是局部卷积运算，我们学习如何从u的邻域聚合信息（图1）。此过程在算法1卷入中详细说明。基本思想是我们通过密集神经网络转换u邻居的表示zv，∀v∈N（u）然后应用聚合器/池化函数（例如，元素方式或加权和，表示为γ）在得到的矢量集上（第1行）。该聚合步骤提供了u的局部邻域N（u）的向量表示nu。然后，我们将ag-gregated邻域向量nu与u的当前表示hu连接，并将连接的向量转换为另一个密集的神经网络层（第2行）。根据经验，我们在使用串联操作而不是[21]中的平均操作时观察到显着的性能提升。此外，第3行中的归一化使训练更加稳定，并且对归一化嵌入执行近似最近邻搜索更有效（第3.5节）。算法的输出是u的表示，其包含关于其自身及其局部图邻域的信息。</p>
<p>基于重要性的社区。我们方法的一个重要创新是我们如何定义节点邻域N（u），即我们如何选择在算法1中进行卷积的邻居集合。而以前的GCN方法只是检查k-hop图形邻域，在PinSage中我们定义重要性基于邻居的邻居，其中节点u的邻域被定义为对节点u施加最大影响的T节点。具体地说，我们模拟从节点u开始的随机游走并计算L1标准化</p>
<p>访问随机游走所访问的节点数[14] .2然后将u的邻域定义为相对于节点u具有最高归一化访问计数的前T个节点。</p>
<p>这种基于重要性的邻域定义的优势是双重的。首先，选择要聚合的固定数量的节点允许我们在训练期间控制算法的内存占用[18]。其次，它允许算法1在聚合邻居的向量表示时考虑邻居的重要性。特别地，我们在算法1中实现γ作为加权平均值，其中权重根据L1归一化访问计数来定义。我们将这种新方法称为重要性池。</p>
<p>堆叠卷积。每次我们应用卷积运算（算法1）时，我们得到一个节点的新表示，并且我们可以将多个这样的卷积堆叠在彼此之上，以便获得关于节点u周围的局部图结构的更多信息。特别地，我们使用多层卷积，其中层k处的卷积的输入取决于层k-1（图1）的表示和初始（即“层0”）表示相等的表示。到输入节点功能。请注意，算法1（Q，q，W和w）中的模型参数在节点之间共享，但层之间不同。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092102872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709213627.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>We first describe our margin-based loss function in detail. Follow-ing this, we give an overview of several techniques we developed that lead to the computation efficiency and fast convergence rate of PinSage, allowing us to train on billion node graphs and billions training examples. And finally, we describe our curriculum-training scheme, which improves the overall quality of the recommendations.</p>
<p>我们首先详细描述基于保证金的损失函数。 接下来，我们概述了我们开发的几种技术，这些技术可以提高PinSage的计算效率和快速收敛速度，使我们能够训练数十亿个节点图和数十亿个训练样例。 最后，我们描述了我们的课程培训计划，该计划提高了建议的整体质量。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709224222.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Loss function. In order to train the parameters of the model, we use a max-margin-based loss function. The basic idea is that we want to maximize the inner product of positive examples, i.e., the embedding of the query item and the corresponding related item. At the same time we want to ensure that the inner product of negative examples—i.e., the inner product between the embedding of the query item and an unrelated item—is smaller than that of the positive sample by some pre-defined margin. The loss function for a<br /> 损失功能。 为了训练模型的参数，我们使用基于最大边际的损失函数。 基本思想是我们希望最大化正例的内积，即嵌入查询项和相应的相关项。 同时，我们希望确保否定示例的内积 &#8211; 即，查询项的嵌入与不相关项之间的内积 &#8211; 比正样本的内积小一些预定义的余量。 a的损失函数<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709234097.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Multi-GPU training with large minibatches. To make full use of multiple GPUs on a single machine for training, we run the for-ward and backward propagation in a multi-tower fashion. With multiple GPUs, we first divide each minibatch (Figure 1 bottom) into equal-sized portions. Each GPU takes one portion of the minibatch and performs the computations using the same set of parameters. Af-ter backward propagation, the gradients for each parameter across all GPUs are aggregated together, and a single step of synchronous SGD is performed. Due to the need to train on extremely large number of examples (on the scale of billions), we run our system with large batch sizes, ranging from 512 to 4096.</p>
<p>We use techniques similar to those proposed by Goyal et al. [16] to ensure fast convergence and maintain training and generalization accuracy when dealing with large batch sizes. We use a gradual warmup procedure that increases learning rate from small to a peak value in the first epoch according to the linear scaling rule. Afterwards the learning rate is decreased exponentially.</p>
<p>Producer-consumer minibatch construction. During training, the adjacency list and the feature matrix for billions of nodes are placed in CPU memory due to their large size. However, during the convolve step of PinSage, each GPU process needs access to the neighborhood and feature information of nodes in the neighbor-hood. Accessing the data in CPU memory from GPU is not efficient. To solve this problem, we use a re-indexing technique to create a sub-graph G′ = (V ′, E′) containing nodes and their neighborhood, which will be involved in the computation of the current minibatch. A small feature matrix containing only node features relevant to computation of the current minibatch is also extracted such that the order is consistent with the index of nodes in G′. The adjacency list of G′ and the small feature matrix are fed into GPUs at the start of each minibatch iteration, so that no communication between the GPU and CPU is needed during the convolve step, greatly improving GPU utilization.</p>
<p>The training procedure has alternating usage of CPUs and GPUs. The model computations are in GPUs, whereas extracting features, re-indexing, and negative sampling are computed on CPUs. In ad-dition to parallelizing GPU computation with multi-tower training, and CPU computation using OpenMP [25], we design a producer-consumer pattern to run GPU computation at the current iteration and CPU computation at the next iteration in parallel. This further reduces the training time by almost a half.</p>
<p>Sampling negative items. Negative sampling is used in our loss function (Equation 1) as an approximation of the normalization factor of edge likelihood [23]. To improve efficiency when training with large batch sizes, we sample a set of 500 negative items to be shared by all training examples in each minibatch. This drastically saves the number of embeddings that need to be computed during each training step, compared to running negative sampling for each node independently. Empirically, we do not observe a difference between the performance of the two sampling schemes.</p>
<p>In the simplest case, we could just uniformly sample negative examples from the entire set of items. However, ensuring that the inner product of the positive example (pair of items (q, i )) is larger than that of the q and each of the 500 negative items is too “easy” and does not provide fine enough “resolution” for the system to learn. In particular, our recommendation algorithm should be capable of finding 1,000 most relevant items to q among the catalog of over 2 billion items. In other words, our model should be able to distinguish/identify 1 item out of 2 million items. But with 500 random negative items, the model’s resolution is only 1 out of</p>
<p>500.Thus, if we sample 500 random negative items out of 2 billion items, the chance of any of these items being even slightly related to the query item is small. Therefore, with large probability the learning will not make good parameter updates and will not be able to differentiate slightly related items from the very related ones.</p>
<p>To solve the above problem, for each positive training example (i.e., item pair (q, i)), we add “hard” negative examples, i.e., items that are somewhat related to the query item q, but not as related as the positive item i. We call these “hard negative items”. They are generated by ranking items in a graph according to their Per-sonalized PageRank scores with respect to query item q [14]. Items ranked at 2000-5000 are randomly sampled as hard negative items. As illustrated in Figure 2, the hard negative examples are more similar to the query than random negative examples, and are thus challenging for the model to rank, forcing the model to learn to distinguish items at a finer granularity.<br /> 使用大型微型计算机进行多GPU培训。为了在一台机器上充分利用多个GPU进行培训，我们以多塔式方式进行前向和后向传播。对于多个GPU，我们首先将每个小批量（图1底部）划分为相等大小的部分。每个GPU获取一部分小批量并使用相同的参数集执行计算。在后向传播之后，所有GPU上的每个参数的梯度被聚合在一起，并且执行同步SGD的单个步骤。由于需要训练大量的例子（数十亿的规模），我们运行我们的系统，批量大，从512到4096。</p>
<p>我们使用类似于Goyal等人提出的技术。 [16]确保在处理大批量时快速收敛并保持培训和泛化准确性。我们使用渐进的预热程序，根据线性缩放规则，在第一个时期内将学习率从小值提高到峰值。之后学习率呈指数下降。</p>
<p>生产者 &#8211; 消费者小批量建设。在训练期间，由于数据量大，数十亿个节点的邻接列表和特征矩阵被放置在CPU内存中。但是，在PinSage的卷积步骤中，每个GPU进程都需要访问邻居和邻居中节点的特征信息。从GPU访问CPU内存中的数据效率不高。为了解决这个问题，我们使用重新索引技术来创建包含节点及其邻域的子图G’=（V’，E’），其将参与当前小批量的计算。还提取仅包含与当前小批量的计算相关的节点特征的小特征矩阵，使得该顺序与G’中的节点索引一致。 G’的邻接列表和小特征矩阵在每个小批量迭代开始时被馈送到GPU中，因此在卷积步骤期间不需要GPU和CPU之间的通信，从而大大提高了GPU利用率。</p>
<p>训练过程交替使用CPU和GPU。模型计算在GPU中，而在CPU上计算提取特征，重新索引和负抽样。除了使用多塔培训并行GPU计算和使用OpenMP进行CPU计算[25]之外，我们还设计了一个生产者 &#8211; 消费者模式，以便在当前迭代中运行GPU计算，并在下一次迭代中并行运行CPU计算。这进一步将训练时间减少了近一半。</p>
<p>采样负面项目。在我们的损失函数（等式1）中使用负采样作为边缘似然归一化因子的近似值[23]。为了提高大批量培训的效率，我们对每个小批量的所有培训示例共享500个负面项目。与每个节点独立运行负采样相比，这大大节省了每个训练步骤中需要计算的嵌入数量。根据经验，我们没有观察到两种抽样方案的性能差异。</p>
<p>在最简单的情况下，我们可以从整个项目集合中统一采样负面示例。但是，确保正例（项目对（q，i））的内积大于q的内积，并且500个负项中的每一个都太“容易”并且不能提供足够精细的“分辨率”。系统要学习。特别是，我们的推荐算法应该能够在超过20亿个项目的目录中找到1000个最相关的项目。换句话说，我们的模型应该能够区分/识别200万个项目中的1个项目。但是有500个随机负面项目，模型的分辨率只有1个</p>
<p>500.因此，如果我们从20亿个项目中抽取500个随机负面项目，那么这些项目中任何一个与查询项目稍微相关的可能性就很小。因此，很有可能学习不会进行良好的参数更新，并且无法区分略微相关的项目与非常相关的项目。</p>
<p>为了解决上述问题，对于每个积极的训练样例（即项目对（q，i）），我们添加“硬”否定示例，即与查询项目q有些相关的项目，但不是与积极的项目我我们将这些称为“硬性负面物品”。它们是通过根据查询项q [14]的Per-sonalized PageRank分数对图表中的项目进行排名而生成的。排名为2000-5000的项目被随机抽样为硬阴性项目。如图2所示，硬否定示例与查询比随机否定示例更相似，因此难以对模型进行排名，迫使模型学习以更精细的粒度区分项目。</p>
<p>Using hard negative items throughout the training procedure doubles the number of epochs needed for the training to con-verge. To help with convergence, we develop a curriculum training scheme [4]. In the first epoch of training, no hard negative items are used, so that the algorithm quickly finds an area in the parameter space where the loss is relatively small. We then add hard negative items in subsequent epochs, focusing the model to learn how to distinguish highly related pins from only slightly related ones. At epoch n of the training, we add n − 1 hard negative items to the set of negative items for each item.<br /> 在整个训练过程中使用硬阴性项目会使训练所需的时期数量增加一倍。 为了帮助融合，我们制定了课程培训计划[4]。 在第一个训练时期，没有使用硬负项，因此算法快速找到参数空间中损失相对较小的区域。 然后，我们在后续时期添加硬阴性项，重点关注模型，以了解如何区分高度相关的引脚和仅略微相关的引脚。 在训练的时期，我们将n &#8211; 1个硬阴性项添加到每个项目的负项目集中。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092622891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> Figure 2: Random negative examples and hard negative ex-amples. Notice that the hard negative example is signifi-cantly more similar to the query, than the random negative example, though not as similar as the positive example.<br /> 图2：随机负面例子和硬性负面例子。 请注意，与负面示例相比，硬负面示例显着更类似于查询，尽管与正面示例不太相似。</p>
<h5><a id="34	Node_Embeddings_via_MapReduce_223"></a>3.4 Node Embeddings via MapReduce</h5>
<p>After the model is trained, it is still challenging to directly apply the trained model to generate embeddings for all items, including those that were not seen during training. Naively computing embeddings for nodes using Algorithm 2 leads to repeated computations caused by the overlap between K-hop neighborhoods of nodes. As illus-trated in Figure 1, many nodes are repeatedly computed at multiple layers when generating the embeddings for different target nodes. To ensure efficient inference, we develop a MapReduce approach that runs model inference without repeated computations.</p>
<p>We observe that inference of node embeddings very nicely lends itself to MapReduce computational model. Figure 3 details the data flow on the bipartite pin-to-board Pinterest graph, where we assume the input (i.e., “layer-0”) nodes are pins/items (and the layer-1 nodes are boards/contexts). The MapReduce pipeline has two key parts:</p>
<p>(1)One MapReduce job is used to project all pins to a low-dimensional latent space, where the aggregation operation will be performed (Algorithm 1, Line 1).</p>
<p>(2)Another MapReduce job is then used to join the resulting pin representations with the ids of the boards they occur in, and the board embedding is computed by pooling the features of its (sampled) neighbors.</p>
<p>Note that our approach avoids redundant computations and that the latent vector for each node is computed only once. After the em-beddings of the boards are obtained, we use two more MapReduce jobs to compute the second-layer embeddings of pins, in a similar fashion as above, and this process can be iterated as necessary (up to K convolutional layers).3</p>
<p>在训练模型之后，直接应用训练模型来生成所有项目的嵌入仍然具有挑战性，包括在训练期间未看到的项目。使用算法2对节点进行初始计算嵌入导致由节点的K跳邻域之间的重叠引起的重复计算。如图1所示，当为不同的目标节点生成嵌入时，在多个层重复计算许多节点。为了确保有效的推理，我们开发了一种MapReduce方法，该方法运行模型推理而无需重复计算。</p>
<p>我们观察到节点嵌入的推断非常好地适用于MapReduce计算模型。图3详述了二分针对板Pinterest图的数据流，其中我们假设输入（即“第0层”）节点是引脚/项（并且第1层节点是板/上下文）。 MapReduce管道有两个关键部分：</p>
<p>（1）一个MapReduce作业用于将所有引脚投影到低维潜在空间，其中将执行聚合操作（算法1，第1行）。</p>
<p>（2）然后使用另一个MapReduce作业将得到的引脚表示与它们出现的板的ID相连，并且通过汇集其（采样的）邻居的特征来计算板嵌入。</p>
<p>请注意，我们的方法避免了冗余计算，并且每个节点的潜在向量仅计算一次。在获得电路板的em-bedding之后，我们使用另外两个MapReduce作业以与上面类似的方式计算引脚的第二层嵌入，并且可以根据需要迭代该过程（直到K个卷积层）。</p>
<h6><a id="35	Efficient_nearestneighbor_lookups_249"></a>3.5 Efficient nearest-neighbor lookups</h6>
<p>The embeddings generated by PinSage can be used for a wide range of downstream recommendation tasks, and in many settings we can directly use these embeddings to make recommendations by performing nearest-neighbor lookups in the learned embedding space. That is, given a query item q, the we can recommend items whose embeddings are the K-nearest neighbors of the query item’s embedding. Approximate KNN can be obtained efficiently via lo-cality sensitive hashing [2]. After the hash function is computed, retrieval of items can be implemented with a two-level retrieval pro-cess based on the Weak AND operator [5]. Given that the PinSage model is trained offline and all node embeddings are computed via MapReduce and saved in a database, the efficient nearest-neighbor lookup operation enables the system to serve recommendations in an online fashion,<br /> PinSage生成的嵌入可用于各种下游推荐任务，在许多设置中，我们可以通过在学习的嵌入空间中执行最近邻查找来直接使用这些嵌入来提出建议。 也就是说，给定查询项q，我们可以推荐其嵌入是查询项嵌入的K最近邻居的项。 通过物理敏感散列可以有效地获得近似KNN [2]。 在计算散列函数之后，可以使用基于Weak AND运算符的两级检索过程来实现项目的检索[5]。 鉴于PinSage模型是离线训练的，并且所有节点嵌入都是通过MapReduce计算并保存在数据库中，有效的最近邻查找操作使系统能够以在线方式提供建议，</p>
<h3><a id="4_EXPERIMENTS_253"></a>4 EXPERIMENTS</h3>
<p>To demonstrate the efficiency of PinSage and the quality of the embeddings it generates, we conduct a comprehensive suite of experiments on the entire Pinterest object graph, including offline experiments, production A/B tests as well as user studies.<br /> 为了证明PinSage的效率及其产生的嵌入质量，我们对整个Pinterest对象图进行了一整套实验，包括离线实验，生产A / B测试以及用户研究。</p>
<h6><a id="41	Experimental_Setup_258"></a>4.1 Experimental Setup</h6>
<p>We evaluate the embeddings generated by PinSage in two tasks: recommending related pins and recommending pins in a user’s home/news feed. To recommend related pins, we select the K near-est neighbors to the query pin in the embedding space. We evaluate performance on this related-pin recommendation task using both offline ranking measures as well as a controlled user study. For the homefeed recommendation task, we select the pins that are closest in the embedding space to one of the most recently pinned items by the user. We evaluate performance of a fully-deployed production system on this task using A/B tests to measure the overall impact on user engagement.</p>
<p>Training details and data preparation. We define the set, L, of positive training examples (Equation (1)) using historical user engagement data. In particular, we use historical user engagement data to identify pairs of pins (q, i), where a user interacted with pin</p>
<p>iimmediately after she interacted with pin q. We use all other pins as negative items (and sample them as described in Section 3.3). Overall, we use 1.2 billion pairs of positive training examples (in addition to 500 negative examples per batch and 6 hard negative examples per pin). Thus in total we use 7.5 billion training examples.<br /> Since PinSage can efficiently generate embeddings for unseen data, we only train on a subset of the Pinterest graph and then generate embeddings for the entire graph using the MapReduce pipeline described in Section 3.4. In particular, for training we use a randomly sampled subgraph of the entire graph, containing 20% of all boards (and all the pins touched by those boards) and 70% of the labeled examples. During hyperparameter tuning, a remaining 10% of the labeled examples are used. And, when testing, we run inference on the entire graph to compute embeddings for all 2 billion pins, and the remaining 20% of the labeled examples are used to test the recommendation performance of our PinSage in the offline evaluations. Note that training on a subset of the full graph drastically decreased training time, with a negligible impact on final performance. In total, the full datasets for training and evaluation are approximately 18TB in size with the full output embeddings being 4TB.<br /> 我们评估PinSage在两个任务中生成的嵌入：推荐相关引脚并在用户的主页/新闻源中推荐引脚。为了推荐相关引脚，我们选择嵌入空间中查询引脚的K近邻。我们使用离线排名度量和受控用户研究来评估此相关引脚推荐任务的性能。对于主页提交推荐任务，我们选择嵌入空间中最接近用户最近固定项目之一的引脚。我们使用A / B测试来评估完全部署的生产系统在此任务上的性能，以衡量对用户参与的总体影响。</p>
<p>培训细节和数据准备。我们使用历史用户参与数据定义积极训练样例（等式（1））的集合L.特别地，我们使用历史用户参与数据来识别用户与引脚交互的引脚对（q，i）</p>
<p>她与pin q交互后立刻。我们使用所有其他引脚作为负项（并按照第3.3节中的描述对它们进行采样）。总的来说，我们使用了12亿对正面训练示例（除了每批500个负面示例和每个针脚6个硬负面示例）。因此，我们总共使用了75亿个培训示例。<br /> 由于PinSage可以有效地为看不见的数据生成嵌入，因此我们只训练Pinterest图的子集，然后使用3.4节中描述的MapReduce管道为整个图生成嵌入。特别是，对于训练，我们使用整个图形的随机采样子图，包含20％的所有板（以及这些板触及的所有引脚）和70％的标记示例。在超参数调整期间，使用剩余的10％的标记示例。并且，在测试时，我们对整个图形进行推断，以计算所有20亿个引脚的嵌入，其余20％的标记示例用于测试我们的PinSage在离线评估中的推荐性能。请注意，对完整图表子集的培训大大减少了培训时间，对最终性能的影响可以忽略不计。总的来说，用于训练和评估的完整数据集大小约为18TB，完整输出嵌入为4TB。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092854376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Features used for learning. Each pin at Pinterest is associated with an image and a set of textual annotations (title, description). To generate feature representation xq for each pin q, we concatenate visual embeddings (4,096 dimensions), textual annotation embed-dings (256 dimensions), and the log degree of the node/pin in the graph. The visual embeddings are the 6-th fully connected layer of a classification network using the VGG-16 architecture [28]. Tex-tual annotation embeddings are trained using a Word2Vec-based model [23], where the context of an annotation consists of other annotations that are associated with each pin.</p>
<p>Baselines for comparison. We evaluate the performance of Pin-Sage against the following state-of-the-art content-based, graph-based and deep learning baselines that generate embeddings of pins:</p>
<p>(1)Visual embeddings (Visual): Uses nearest neighbors of deep visual embeddings for recommendations. The visual features are described above.</p>
<p>(2)Annotation embeddings (Annotation): Recommends based on nearest neighbors in terms of annotation embeddings. The annotation embeddings are described above.<br /> (3)Combined embeddings (Combined): Recommends based on concatenating visual and annotation embeddings, and using a 2-layer multi-layer perceptron to compute embeddings that capture both visual and annotation features.</p>
<p>(4)Graph-based method (Pixie): This random-walk-based method [14] uses biased random walks to generate ranking scores by simulating random walks starting at query pin q. Items with top K scores are retrieved as recommendations. While this approach does not generate pin embeddings, it is currently the state-of-the-art at Pinterest for certain recommendation tasks [14] and thus an informative baseline.</p>
<p>The visual and annotation embeddings are state-of-the-art deep learning content-based systems currently deployed at Pinterest to generate representations of pins. Note that we do not compare against other deep learning baselines from the literature simply due to the scale of our problem. We also do not consider non-deep learning approaches for generating item/content embeddings, since other works have already proven state-of-the-art performance of deep learning approaches for generating such embeddings [9, 12, 24].</p>
<p>用于学习的功能。 Pinterest中的每个引脚都与一个图像和一组文本注释（标题，描述）相关联。为了生成每个引脚q的特征表示xq，我们连接了视觉嵌入（4,096维），文本注释嵌入（256维）以及图中节点/引脚的对数度。视觉嵌入是使用VGG-16架构的分类网络的第6个完全连接层[28]。使用基于Word2Vec的模型[23]训练文本注释嵌入，其中注释的上下文包含与每个引脚相关联的其他注释。</p>
<p>比较基线。我们针对以下基于内容的，基于图形的深度学习基线评估了Pin-Sage的性能，这些基线生成了引脚的嵌入：</p>
<p>（1）视觉嵌入（Visual）：使用深度视觉嵌入的最近邻居作为推荐。视觉特征如上所述。</p>
<p>（2）注释嵌入（注释）：根据注释嵌入推荐基于最近邻居。注释嵌入如上所述。<br /> （3）组合嵌入（组合）：推荐基于连接视觉和注释嵌入，并使用2层多层感知器计算捕获视觉和注释特征的嵌入。</p>
<p>（4）基于图的方法（Pixie）：这种基于随机游走的方法[14]通过模拟从查询引脚q开始的随机游走来使用偏向随机游走来生成排名分数。具有最高K分数的项目被检索作为推荐。虽然这种方法不会产生引脚嵌入，但它目前是Pinterest中针对某些推荐任务[14]的最新技术，因此也是一个信息基线。</p>
<p>视觉和注释嵌入是目前在Pinterest部署的最先进的基于深度学习内容的系统，用于生成引脚的表示。请注意，由于问题的严重性，我们不会与文献中的其他深度学习基线进行比较。我们也不考虑用于生成项目/内容嵌入的非深度学习方法，因为其他工作已经证明了用于生成这种嵌入的深度学习方法的最先进性能[9,12,24]。</p>
<p>We also conduct ablation studies and consider several variants of PinSage when evaluating performance:</p>
<ul>
<li>
<p>max-pooling uses the element-wise max as a symmetric aggre-gation function (i.e., γ = max) without hard negative samples;</p>
</li>
<li>
<p>mean-pooling uses the element-wise mean as a symmetric aggregation function (i.e., γ = mean);</p>
</li>
<li>
<p>mean-pooling-xent is the same as mean-pooling but uses the cross-entropy loss introduced in [18].</p>
</li>
<li>
<p>mean-pooling-hard is the same as mean-pooling, except that it incorporates hard negative samples as detailed in Section 3.3.</p>
</li>
<li>
<p>PinSage uses all optimizations presented in this paper, includ-ing the use of importance pooling in the convolution step.</p>
</li>
</ul>
<p>The max-pooling and cross-entropy settings are extensions of the best-performing GCN model from Hamilton et al. [18]—other vari-ants (e.g., based on Kipf et al. [21]) performed significantly worse in development tests and are omitted for brevity.4 For all the above variants, we used K = 2, hidden dimension size m = 2048, and set the embedding dimension d to be 1024.</p>
<p>Computation resources. Training of PinSage is implemented in TensorFlow [1] and run on a single machine with 32 cores and 16 Tesla K80 GPUs. To ensure fast fetching of item’s visual and annotation features, we store them in main memory, together with the graph, using Linux HugePages to increase the size of virtual memory pages from 4KB to 2MB. The total amount of memory used in training is 500GB. Our MapReduce inference pipeline is run on a Hadoop2 cluster with 378 d2.8xlarge Amazon AWS nodes.</p>
<p>我们还进行消融研究，并在评估性能时考虑PinSage的几种变体：</p>
<ul>
<li>
<p>max-pooling使用逐元素max作为对称聚合函数（即γ= max）而没有硬阴性样本;</p>
</li>
<li>
<p>均值池使用元素均值作为对称聚合函数（即，γ=均值）;</p>
</li>
<li>
<p>mean-pooling-xent与均值池相同，但使用[18]中引入的交叉熵损失。</p>
</li>
<li>
<p>mean-pooling-hard与mean-pooling相同，除了它包含第3.3节中详述的硬阴性样本。</p>
</li>
<li>
<p>PinSage使用本文中介绍的所有优化，包括在卷积步骤中使用重要性池。</p>
</li>
</ul>
<p>最大池和交叉熵设置是Hamilton等人的最佳性能GCN模型的扩展。 [18]其他变量（例如，基于Kipf等人[21]）在开发测试中表现更差，为简洁省略.4对于所有上述变体，我们使用K = 2，隐藏尺寸大小为m = 2048，并将嵌入维度d设置为1024。</p>
<p>计算资源。 PinSage的培训在TensorFlow [1]中实施，并在具有32个核心和16个Tesla K80 GPU的单台机器上运行。为了确保快速获取项目的视觉和注释功能，我们将它们与图形一起存储在主存储器中，使用Linux HugePages将虚拟内存页面的大小从4KB增加到2MB。培训中使用的内存总量为500GB。我们的MapReduce推理管道在具有378个d2.8xlarge Amazon AWS节点的Hadoop2集群上运行。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092944123.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>表1：PinSage和基于内容的深度学习基线的命中率和MRR。 总体而言，PinSage在最佳基线上的命中率提高了150％，MRR提高了60％</p>
<h4><a id="42	Offline_Evaluation_333"></a>4.2 Offline Evaluation</h4>
<p>To evaluate performance on the related pin recommendation task, we define the notion of hit-rate. For each positive pair of pins (q, i) in the test set, we use q as a query pin and then compute its top<br /> Knearest neighbors NNq from a sample of 5 million test pins. We then define the hit-rate as the fraction of queries q where i was ranked among the top K of the test sample (i.e., where i ∈ NNq ). This metric directly measures the probability that recommendations made by the algorithm contain the items related to the query pin q. In our experiments K is set to be 500.</p>
<p>We also evaluate the methods using Mean Reciprocal Rank (MRR), which takes into account of the rank of the item j among recommended items for query item q:<br /> 为了评估相关引脚推荐任务的性能，我们定义了命中率的概念。 对于测试集中的每个正对引脚（q，i），我们使用q作为查询引脚，然后计算其顶部<br /> 来自500万个测试引脚样本的最近邻NNq。 然后，我们将命中率定义为查询q的分数，其中i在测试样本的前K中排名（即，其中i∈NNq）。 该度量直接测量算法所做推荐包含与查询引脚q相关的项目的概率。 在我们的实验中，K设定为500。</p>
<p>我们还使用均值倒数等级（MRR）来评估方法，其考虑了查询项目q的推荐项目中的项目j的等级：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093036909.png" alt="在这里插入图片描述"><br /> Due to the large pool of candidates (more than 2 billion), we use a scaled version of the MRR in Equation (2), where Ri, q is the rank of item i among recommended items for query q, and n is the total number of labeled item pairs. The scaling factor 100 ensures that, for example, the difference between rank at 1, 000 and rank at 2, 000 is still noticeable, instead of being very close to 0.</p>
<p>Table 1 compares the performance of the various approaches using the hit rate as well as the MRR.5 PinSage with our new importance-pooling aggregation and hard negative examples achieves the best performance at 67% hit-rate and 0.59 MRR, outperforming the top baseline by 40% absolute (150% relative) in terms of the hit rate and also 22% absolute (60% relative) in terms of MRR. We also observe that combining visual and textual information works much better than using either one alone (60% improvement of the combined approach over visual/annotation only).</p>
<p>Embedding similarity distribution. Another indication of the effectiveness of the learned embeddings is that the distances be-tween random pairs of item embeddings are widely distributed. If all items are at about the same distance (i.e., the distances are tightly clustered) then the embedding space does not have enough “resolu-tion” to distinguish between items of different relevance. Figure 4 plots the distribution of cosine similarities between pairs of items using annotation, visual, and PinSage embeddings. This distribution of cosine similarity between random pairs of items demonstrates the effectiveness of PinSage, which has the most spread out distribu-tion. In particular, the kurtosis of the cosine similarities of PinSage embeddings is 0.43, compared to 2.49 for annotation embeddings and 1.20 for visual embeddings.<br /> Another important advantage of having such a wide-spread in the embeddings is that it reduces the collision probability of the subsequent LSH algorithm, thus increasing the efficiency of serving the nearest neighbor pins during recommendation.<br /> 由于候选人数量大（超过20亿），我们在等式（2）中使用MRR的缩放版本，其中Ri，q是查询q的推荐项目中项目i的等级，n是总数标记的项目对的数量。缩放因子100确保例如，等级为1,000和等级为2,000的差异仍然是显着的，而不是非常接近于0。</p>
<p>表1比较了使用命中率的各种方法的性能以及MRR.5 PinSage与我们新的重要性汇总聚合和硬阴性示例在67％命中率和0.59 MRR下达到最佳性能，优于最高基线就命中率而言绝对值为40％（相对于150％），就MRR而言绝对值为22％（相对于60％）。我们还观察到，将视觉和文本信息结合起来比单独使用任何一个都要好得多（组合方法仅比视觉/注释提高60％）。</p>
<p>嵌入相似度分布。学习嵌入的有效性的另一个指示是随机对项目嵌入之间的距离被广泛分布。如果所有项目都处于大约相同的距离（即，距离紧密聚集），则嵌入空间没有足够的“分辨率”来区分不同相关的项目。图4绘制了使用注释，视觉和PinSage嵌入的项目对之间余弦相似度的分布。随机对项之间的余弦相似性的这种分布证明了PinSage的有效性，其具有最大的分布。特别是，PinSage嵌入的余弦相似度的峰度为0.43，而注释嵌入为2.49，视觉嵌入为1.20。<br /> 在嵌入中具有如此广泛的扩展的另一个重要优点是它降低了后续LSH算法的冲突概率，从而提高了在推荐期间服务最近邻居引脚的效率。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093124819.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 图4：视觉嵌入，注释嵌入和Pin-Sage嵌入的成对余弦相似度的概率密度。</p>
<h4><a id="43	User_Studies_359"></a>4.3 User Studies</h4>
<p>We also investigate the effectiveness of PinSage by performing head-to-head comparison between different learned representations. In the user study, a user is presented with an image of the query pin, together with two pins retrieved by two different recommendation algorithms. The user is then asked to choose which of the two candidate pins is more related to the query pin. Users are instructed to find various correlations between the recommended items and the query item, in aspects such as visual appearance, object category and personal identity. If both recommended items seem equally related, users have the option to choose “equal”. If no consensus is reached among 2/3 of users who rate the same question, we deem the result as inconclusive.</p>
<p>Table 2 shows the results of the head-to-head comparison be-tween PinSage and the 4 baselines. Among items for which the user has an opinion of which is more related, around 60% of the pre-ferred items are recommended by PinSage. Figure 5 gives examples of recommendations and illustrates strengths and weaknesses of the different methods. The image to the left represents the query item. Each row to the right corresponds to the top recommendations made by the visual embedding baseline, annotation embedding baseline, Pixie, and PinSage. Although visual embeddings gener-ally predict categories and visual similarity well, they occasionally make large mistakes in terms of image semantics. In this example, visual information confused plants with food, and tree logging with war photos, due to similar image style and appearance. The graph-based Pixie method, which uses the graph of pin-to-board relations, correctly understands that the category of query is “plants” and it recommends items in that general category. However, it does not find the most relevant items. Combining both visual/textual and graph information, PinSage is able to find relevant items that are both visually and topically similar to the query item.</p>
<p>我们还通过在不同的学习表示之间进行头对头比较来研究PinSage的有效性。在用户研究中，向用户呈现查询引脚的图像，以及由两个不同推荐算法检索的两个引脚。然后要求用户选择两个候选引脚中的哪一个与查询引脚更相关。指示用户在视觉外观，对象类别和个人身份等方面找到推荐项目和查询项目之间的各种相关性。如果两个推荐项似乎相同，则用户可以选择“相等”。如果对同一问题评分的2/3的用户未达成共识，我们认为结果不确定。</p>
<p>表2显示了PinSage与4个基线之间的头对头比较结果。在用户具有更多相关意见的项目中，PinSage推荐约60％的优先项目。图5给出了建议的示例，并说明了不同方法的优缺点。左侧的图像表示查询项。右侧的每一行对应于可视嵌入基线，注释嵌入基线，Pixie和PinSage所做的最高建议。虽然视觉嵌入通常可以很好地预测类别和视觉相似性，但它们偶尔会在图像语义方面犯大错误。在这个例子中，由于类似的图像样式和外观，视觉信息使植物与食物混淆，并且树木记录与战争照片。基于图形的Pixie方法使用引脚到板的关系图，正确地理解查询的类别是“植物”，并且它推荐该一般类别中的项目。但是，它没有找到最相关的项目。结合视觉/文本和图形信息，PinSage能够找到与查询项目在视觉上和主题上相似的相关项目。</p>
<p>In addition, we visualize the embedding space by randomly choosing 1000 items and compute the 2D t-SNE coordinates from the PinSage embedding, as shown in Figure 6.6 We observe that the proximity of the item embeddings corresponds well with the simi-larity of content, and that items of the same category are embedded into the same part of the space. Note that items that are visually different but have the same theme are also close to each other in the embedding space, as seen by the items depicting different fashion-related items on the bottom side of the plot.</p>
<p>此外，我们通过随机选择1000个项目来可视化嵌入空间，并从PinSage嵌入计算2D t-SNE坐标，如图6.6所示。我们观察到项目嵌入的接近度与内容的相似性很好地对应， 并且相同类别的项目嵌入到空间的相同部分中。 请注意，视觉上不同但具有相同主题的项目在嵌入空间中也彼此接近，如在绘图底部描绘不同时尚相关项目的项目所示。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093244394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="44	Production_AB_Test_375"></a>4.4 Production A/B Test</h4>
<p>Lastly, we also report on the production A/B test experiments, which compared the performance of PinSage to other deep learning content-based recommender systems at Pinterest on the task of homefeed recommendations. We evaluate the performance by ob-serving the lift in user engagement. The metric of interest is repin rate, which measures the percentage of homefeed recommendations that have been saved by the users. A user saving a pin to a board is a high-value action that signifies deep engagement of the user. It means that a given pin presented to a user at a given time was relevant enough for the user to save that pin to one of their boards so that they can retrieve it later.</p>
<p>We find that PinSage consistently recommends pins that are more likely to be re-pinned by the user than the alternative methods. Depending on the particular setting, we observe 10-30% improve-ments in repin rate over the Annotation and Visual embedding based recommendations.<br /> 最后，我们还报告了生产A / B测试实验，该实验比较了PinSage与Pinterest上其他基于深度学习内容的推荐系统在家庭饲料建议任务上的表现。 我们通过观察用户参与的提升来评估性能。 感兴趣的度量标准是repin rate，用于衡量用户保存的家庭馈送建议的百分比。 用户将引脚保存到电路板是一种高价值的操作，表示用户的深度参与。 这意味着在给定时间呈现给用户的给定引脚足够相关，用户可以将该引脚保存到其中一个板上，以便以后可以检索它。</p>
<p>我们发现PinSage始终推荐比其他方法更有可能被用户重新固定的引脚。 根据特定设置，我们观察到基于Annotation和Visual embedding建议的重复率提高了10-30％。</p>
<h5><a id="45	Training_and_Inference_Runtime_Analysis_383"></a>4.5 Training and Inference Runtime Analysis</h5>
<p>One advantage of GCNs is that they can be made inductive [19]: at the inference (i.e., embedding generation) step, we are able to compute embeddings for items that were not in the training set. This allows us to train on a subgraph to obtain model parameters, and then make embed nodes that have not been observed during training. Also note that it is easy to compute embeddings of new nodes that get added into the graph over time. This means that recommendations can be made on the full (and constantly grow-ing) graph. Experiments on development data demonstrated that training on a subgraph containing 300 million items could achieve the best performance in terms of hit-rate (i.e., further increases in the training set size did not seem to help), reducing the runtime by a factor of 6 compared to training on the full graph.<br /> Table 3 shows the the effect of batch size of the minibatch SGD on the runtime of PinSage training procedure, using the mean-pooling-hard variant. For varying batch sizes, the table shows: (1) the computation time, in milliseconds, for each minibatch, when varying batch size; (2) the number of iterations needed for the model to converge; and (3) the total estimated time for the training proce-dure. Experiments show that a batch size of 2048 makes training most efficient.</p>
<p>When training the PinSage variant with importance pooling, another trade-off comes from choosing the size of neighborhood T . Table 3 shows the runtime and performance of PinSage when</p>
<p>T= 10, 20 and 50. We observe a diminishing return as T increases, and find that a two-layer GCN with neighborhood size 50 can best capture the neighborhood information of nodes, while still being computationally efficient.<br /> After training completes, due to the highly efficient MapReduce inference pipeline, the whole inference procedure to generate em-beddings for 3 billion items can finish in less than 24 hours.</p>
<p>GCN的一个优点是可以使它们具有归纳性[19]：在推理（即嵌入生成）步骤中，我们能够计算不在训练集中的项目的嵌入。这允许我们在子图上训练以获得模型参数，然后制作在训练期间未观察到的嵌入节点。另请注意，计算新节点的嵌入很容易随着时间的推移而添加到图形中。这意味着可以在完整（并且不断增长）的图表上进行推荐。对开发数据的实验表明，对包含3亿个项目的子图的训练可以在命中率方面达到最佳性能（即，训练集大小的进一步增加似乎没有帮助），将运行时间缩短了6倍与完整图表上的培训相比。<br /> 表3显示了使用mean-pooling-hard变体，miniatch SGD的批量大小对PinSage训练过程的运行时间的影响。对于不同的批量大小，该表显示：（1）当批量大小变化时，每个小批量的计算时间（以毫秒为单位）; （2）模型收敛所需的迭代次数; （3）培训程序的总预计时间。实验表明，2048的批量大小使培训效率最高。</p>
<p>在训练具有重要性汇集的PinSage变体时，另一个权衡取决于选择邻域T的大小。表3显示了PinSage的运行时和性能</p>
<p>T = 10,20和50.随着T的增加，我们观察到收益递减，并发现邻域大小为50的双层GCN可以最好地捕获节点的邻域信息，同时仍然具有计算效率。<br /> 培训完成后，由于高效的MapReduce推理管道，生成30亿件物品的整体推理程序可以在不到24小时内完成。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093352552.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 图5：不同算法推荐的Pinterest引脚示例。 左边的图像是查询引脚。 使用Visual em-beddings，Annotation embeddings，基于图形的Pixie和PinSage计算右侧的推荐项目。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093445155.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="Figure_6_tSNE_plot_of_item_embeddings_in_2_dimensions_405"></a>Figure 6: t-SNE plot of item embeddings in 2 dimensions.</h4>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709351932.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h6><a id="Table_3_Runtime_comparisons_for_different_batch_sizes_409"></a>Table 3: Runtime comparisons for different batch sizes.</h6>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093557957.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="Table_4_Performance_tradeoffs_for_importance_pooling_412"></a>Table 4: Performance tradeoffs for importance pooling.</h4>
<h4><a id="5	CONCLUSION_415"></a>5 CONCLUSION</h4>
<p>We proposed PinSage, a random-walk graph convolutional network (GCN). PinSage is a highly-scalable GCN algorithm capable of learn-ing embeddings for nodes in web-scale graphs containing billions of objects. In addition to new techniques that ensure scalability, we introduced the use of importance pooling and curriculum training that drastically improved embedding performance. We deployed PinSage at Pinterest and comprehensively evaluated the quality of the learned embeddings on a number of recommendation tasks, with offline metrics, user studies and A/B tests all demonstrating a substantial improvement in recommendation performance. Our work demonstrates the impact that graph convolutional methods can have in a production recommender system, and we believe that PinSage can be further extended in the future to tackle other graph representation learning problems at large scale, including knowledge graph reasoning and graph clustering.<br /> 我们提出了PinSage，一种随机游走图卷积网络（GCN）。 PinSage是一种高度可扩展的GCN算法，能够在包含数十亿个对象的Web级图形中学习节点的嵌入。 除了确保可扩展性的新技术之外，我们还引入了重要性池和课程训练，大大提高了嵌入性能。 我们在Pinterest部署了PinSage，并在一系列推荐任务中全面评估了学习嵌入的质量，离线指标，用户研究和A / B测试都证明了推荐性能的实质性改进。 我们的工作展示了图卷积方法在生产推荐系统中可能产生的影响，我们相信PinSage可以在未来进一步扩展，以解决大规模的其他图表表示学习问题，包括知识图推理和图聚类。</p>
<h2><a id="Acknowledgments_419"></a>Acknowledgments</h2>
<p>The authors acknowledge Raymond Hsu, Andrei Curelea and Ali Altaf for performing various A/B tests in production system, Jerry</p>
<p>Zitao Liu for providing data used by Pixie[14], and Vitaliy Kulikov for help in nearest neighbor query of the item embeddings.</p>
<h2><a id="REFERENCES_429"></a>REFERENCES</h2>
<p>[1]M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. 2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 (2016).</p>
<p>[2]A. Andoni and P. Indyk. 2006. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In FOCS.<br /> [3]T. Bansal, D. Belanger, and A. McCallum. 2016. Ask the GRU: Multi-task learning for deep text recommendations. In RecSys. ACM.<br /> [4]Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009. Curriculum learning. In ICML.<br /> [5]A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. 2003. Efficient query evaluation using a two-level retrieval process. In CIKM.<br /> [6]M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. 2017. Geometric deep learning: Going beyond euclidean data. IEEE Signal Processing Magazine 34, 4 (2017).</p>
<p>[7]J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. 2014. Spectral networks and locally connected networks on graphs. In ICLR.<br /> [8]J. Chen, T. Ma, and C. Xiao. 2018. FastGCN: Fast Learning with Graph Convolu-tional Networks via Importance Sampling. ICLR (2018).<br /> [9]P. Covington, J. Adams, and E. Sargin. 2016. Deep neural networks for youtube recommendations. In RecSys. ACM.<br /> [10]H. Dai, B. Dai, and L. Song. 2016. Discriminative Embeddings of Latent Variable Models for Structured Data. In ICML.<br /> [11]M. Defferrard, X. Bresson, and P. Vandergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral filtering. In NIPS.<br /> [12]A. Van den Oord, S. Dieleman, and B. Schrauwen. 2013. Deep content-based music recommendation. In NIPS.<br /> [13]D. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. 2015. Convolutional networks on graphs for learning molecular fingerprints. In NIPS.</p>
<p>[14]C. Eksombatchai, P. Jindal, J. Z. Liu, Y. Liu, R. Sharma, C. Sugnet, M. Ulrich, and</p>
<p>J.Leskovec. 2018. Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time. WWW (2018).<br /> [15]M. Gori, G. Monfardini, and F. Scarselli. 2005. A new model for learning in graph domains. In IEEE International Joint Conference on Neural Networks.</p>
<p>[16]P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch,</p>
<p>Y.Jia, and K. He. 2017. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. arXiv preprint arXiv:1706.02677 (2017).<br /> [17]A. Grover and J. Leskovec. 2016. node2vec: Scalable feature learning for networks. In KDD.<br /> [18]W. L. Hamilton, R. Ying, and J. Leskovec. 2017. Inductive Representation Learning on Large Graphs. In NIPS.<br /> [19]W. L. Hamilton, R. Ying, and J. Leskovec. 2017. Representation Learning on Graphs: Methods and Applications. IEEE Data Engineering Bulletin (2017).</p>
<p>[20]S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley. 2016. Molecular graph convolutions: moving beyond fingerprints. CAMD 30, 8.<br /> [21]T. N. Kipf and M. Welling. 2017. Semi-supervised classification with graph convolutional networks. In ICLR.<br /> [22]Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. 2015. Gated graph sequence neural networks. In ICLR.<br /> [23]T. Mikolov, I Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS.<br /> [24]F. Monti, M. M. Bronstein, and X. Bresson. 2017. Geometric matrix completion with recurrent multi-graph neural networks. In NIPS.<br /> [25]OpenMP Architecture Review Board. 2015. OpenMP Application Program Inter-face Version 4.5. (2015).<br /> [26]B. Perozzi, R. Al-Rfou, and S. Skiena. 2014. DeepWalk: Online learning of social representations. In KDD.<br /> [27]F. Scarselli, M. Gori, A.C. Tsoi, M. Hagenbuchner, and G. Monfardini. 2009. The graph neural network model. IEEE Transactions on Neural Networks 20, 1 (2009), 61–80.</p>
<p>[28]K. Simonyan and A. Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).<br /> [29]R. van den Berg, T. N. Kipf, and M. Welling. 2017. Graph Convolutional Matrix Completion. arXiv preprint arXiv:1706.02263 (2017).<br /> [30]J. You, R. Ying, X. Ren, W. L. Hamilton, and J. Leskovec. 2018. GraphRNN: Generating Realistic Graphs using Deep Auto-regressive Models. ICML (2018).<br /> [31]M. Zitnik, M. Agrawal, and J. Leskovec. 2018. Modeling polypharmacy side effects with graph convolutional networks. Bioinformatics (2018).</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>Graph Convolutional Neural Networks for Web-Scale Recommender Systems（用于Web级推荐系统的图形卷积神经网络）</title>
		<link>https://uzzz.org/article/1656.html</link>
				<pubDate>Sun, 07 Jul 2019 01:52:26 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[AI程序员]]></category>
		<category><![CDATA[GNN]]></category>
		<category><![CDATA[人工智能应用开发]]></category>
		<category><![CDATA[推荐系统]]></category>
		<category><![CDATA[机器学习]]></category>
		<category><![CDATA[深度学习]]></category>
		<category><![CDATA[算法]]></category>
		<category><![CDATA[论文研读]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1656.html</guid>
				<description><![CDATA[Graph Convolutional Neural Networks for Web-Scale Recommender Systems 用于Web级推荐系统的图形卷积神经网络 ABSTRACT Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We deploy PinSage at Pinterest and train it on 7.5 billion exam-ples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<h4><a id="Graph_Convolutional_Neural_Networks_for_WebScale_Recommender_Systems_0"></a>Graph Convolutional Neural Networks for Web-Scale Recommender Systems</h4>
<h3><a id="Web_2"></a>用于Web级推荐系统的图形卷积神经网络</h3>
<h6><a id="ABSTRACT_4"></a>ABSTRACT</h6>
<p>Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge.</p>
<p>Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model.</p>
<p>We deploy PinSage at Pinterest and train it on 7.5 billion exam-ples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embed-dings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.<br /> 用于图形结构数据的深度神经网络的最新进展已经在推荐器系统基准上产生了最先进的性能。然而，使这些方法实用且可扩展到具有数十亿项目和数亿用户的网络规模推荐任务仍然是一项挑战。</p>
<p>在这里，我们描述了我们在Pinterest开发和部署的大规模深度推荐引擎。我们开发了一种数据有效的图形卷积网络（GCN）算法PinSage，它结合了有效的随机游走和图形卷积，以生成包含图形结构和节点特征信息的节点（即项目）的嵌入。与之前的GCN方法相比，我们开发了一种基于高效随机游走的新方法来构建卷积并设计一种新的训练策略，该策略依赖于更难和更难的训练示例来提高模型的鲁棒性和收敛性。</p>
<p>我们在Pinterest部署了PinSage，并在图表上培训了75亿个例子，其中30亿个节点代表引脚和电路板，以及180亿个边缘。根据离线指标，用户研究和A / B测试，PinSage可提供比可比较的深度学习和基于图形的替代方案更高质量的建议。据我们所知，这是迄今为止最大的深度图嵌入应用，并为基于图卷积结构的新一代Web级推荐系统铺平了道路。</p>
<h4><a id="1	INTRODUCTION_17"></a>1 INTRODUCTION</h4>
<p>Deep learning methods have an increasingly critical role in rec-ommender system applications, being used to learn useful low-dimensional embeddings of images, text, and even individual users [9, 12]. The representations learned using deep models can be used to complement, or even replace, traditional recommendation algo-rithms like collaborative filtering. and these learned representations have high utility because they can be re-used in various recom-mendation tasks. For example, item embeddings learned using a deep model can be used for item-item recommendation and also to recommended themed collections (e.g., playlists, or “feed” content).</p>
<p>Recent years have seen significant developments in this space— especially the development of new deep learning methods that are capable of learning on graph-structured data, which is fundamen-tal for recommendation applications (e.g., to exploit user-to-item interaction graphs as well as social graphs) [6, 19, 21, 24, 29, 30].</p>
<p>Most prominent among these recent advancements is the suc-cess of deep learning architectures known as Graph Convolutional Networks (GCNs) [19, 21, 24, 29]. The core idea behind GCNs is to learn how to iteratively aggregate feature information from lo-cal graph neighborhoods using neural networks (Figure 1). Here a single “convolution” operation transforms and aggregates feature information from a node’s one-hop graph neighborhood, and by stacking multiple such convolutions information can be propagated across far reaches of a graph. Unlike purely content-based deep models (e.g., recurrent neural networks [3]), GCNs leverage both content information as well as graph structure. GCN-based methods have set a new standard on countless recommender system bench-marks (see [19] for a survey). However, these gains on benchmark tasks have yet to be translated to gains in real-world production environments.</p>
<p>深度学习方法在调用系统应用程序中具有越来越重要的作用，用于学习图像，文本甚至个人用户的有用的低维嵌入[9,12]。使用深度模型学习的表示可用于补充甚至替代传统的推荐算法，如协同过滤。并且这些学习的表示具有很高的实用性，因为它们可以在各种推荐任务中重复使用。例如，使用深度模型学习的项目嵌入可以用于项目项目推荐以及推荐的主题集合（例如，播放列表或“馈送”内容）。</p>
<p>近年来，这一领域取得了重大进展 &#8211; 尤其是新的深度学习方法的开发，这些方法能够学习图形结构数据，这是推荐应用的基础（例如，利用用户到项目的交互图形以及社交图表）[6,19,21,24,29,30]。</p>
<p>这些最新进展中最突出的是深度学习架构的成功，称为图形卷积网络（GCN）[19,21,24,29]。 GCN背后的核心思想是学习如何使用神经网络从lo-cal图形邻域迭代地聚合特征信息（图1）。这里，单个“卷积”操作转换并聚合来自节点的单跳图邻域的特征信息，并且通过堆叠多个这样的卷积信息可以在图的远端传播。与纯粹基于内容的深层模型（例如，递归神经网络[3]）不同，GCN利用内容信息和图形结构。基于GCN的方法为无数的推荐系统基准设定了新的标准（参见[19]的一项调查）。但是，基准任务的这些收益尚未转化为实际生产环境中的收益。</p>
<p>The main challenge is to scale both the training as well as in-ference of GCN-based node embeddings to graphs with billions of nodes and tens of billions of edges. Scaling up GCNs is difficult because many of the core assumptions underlying their design are violated when working in a big data environment. For example, all existing GCN-based recommender systems require operating on the full graph Laplacian during training—an assumption that is infeasible when the underlying graph has billions of nodes and whose structure is constantly evolving.</p>
<p>Present work. Here we present a highly-scalable GCN framework that we have developed and deployed in production at Pinterest. Our framework, a random-walk-based GCN named PinSage, operates on a massive graph with 3 billion nodes and 18 billion edges—a graph that is 10, 000× larger than typical applications of GCNs. PinSage leverages several key insights to drastically improve the scalability of GCNs:<br /> 主要的挑战是将基于GCN的节点嵌入的训练和推理扩展到具有数十亿个节点和数百亿个边缘的图形。 扩展GCN很困难，因为在大数据环境中工作时，其设计的许多核心假设都会受到侵犯。 例如，所有现有的基于GCN的推荐系统都需要在训练期间对完整图拉普拉斯算子进行操作 &#8211; 当基础图具有数十亿个节点且其结构不断发展时，这种假设是不可行的。</p>
<p>目前的工作。 在这里，我们提出了一个高度可扩展的GCN框架，我们在Pinterest的生产中开发和部署了该框架。 我们的框架是一个名为PinSage的基于随机游走的GCN，它运行在一个包含30亿个节点和180亿个边缘的大型图形上 &#8211; 图形比GCN的典型应用程序大10,000倍。 PinSage利用几个关键见解来大幅提高GCN的可扩展性：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707091551510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707091641450.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 图1：使用深度2卷积的模型架构概述（最好以彩色查看）。 左：一个小例子输入</p>
<p>图形。 右：使用前一层表示计算节点A的嵌入hA（2）的2层神经网络，<br /> 节点A的hA（1）和其邻域N（A）（节点B，C，D）的hA（1）。 （然而，邻域的概念是通用的，并不是所有邻居都需要包括在内（第3.2节）。）底部：计算输入图的每个节点的嵌入的神经网络。 虽然神经网络在节点之间不同，但它们都共享相同的参数集（即，卷积（1）和卷积（2）函数的参数;算法1）。 具有相同阴影图案的框共享参数; γ表示重要性汇集函数; 薄矩形框表示密集连接的多层神经网络。</p>
<ul>
<li>
<p>•On-the-fly convolutions: Traditional GCN algorithms per-form graph convolutions by multiplying feature matrices by powers of the full graph Laplacian. In contrast, our PinSage algo-rithm performs efficient, localized convolutions by sampling the neighborhood around a node and dynamically constructing a computation graph from this sampled neighborhood. These dy-namically constructed computation graphs (Fig. 1) specify how to perform a localized convolution around a particular node, and alleviate the need to operate on the entire graph during training.</p>
</li>
<li>
<p>•Producer-consumer minibatch construction: We develop a producer-consumer architecture for constructing minibatches that ensures maximal GPU utilization during model training. A large-memory, CPU-bound producer efficiently samples node network neighborhoods and fetches the necessary features to define local convolutions, while a GPU-bound TensorFlow model consumes these pre-defined computation graphs to efficiently run stochastic gradient decent.<br /> •Efficient MapReduce inference: Given a fully-trained GCN model, we design an efficient MapReduce pipeline that can dis-tribute the trained model to generate embeddings for billions of nodes, while minimizing repeated computations.</p>
</li>
</ul>
<p>In addition to these fundamental advancements in scalability, we also introduce new training techniques and algorithmic innova-tions. These innovations improve the quality of the representations learned by PinSage, leading significant performance gains in down-stream recommender system tasks:</p>
<ul>
<li>Constructing convolutions via random walks: Taking full neighborhoods of nodes to perform convolutions (Fig. 1) would result in huge computation graphs, so we resort to sampling. However, random sampling is suboptimal, and we develop a new technique using short random walks to sample the computa-tion graph. An additional benefit is that each node now has an importance score, which we use in the pooling/aggregation step.</li>
<li>•Importance pooling: A core component of graph convolutions is the aggregation of feature information from local neighbor-hoods in the graph. We introduce a method to weigh the impor-tance of node features in this aggregation based upon random-walk similarity measures, leading to a 46% performance gain in offline evaluation metrics.</li>
<li>•Curriculum training: We design a curriculum training scheme, where the algorithm is fed harder-and-harder examples during training, resulting in a 12% performance gain.<br /> We have deployed PinSage for a variety of recommendation tasks at Pinterest, a popular content discovery and curation appli-cation where users interact with pins, which are visual bookmarks to online content (e.g., recipes they want to cook, or clothes they want to purchase). Users organize these pins into boards, which con-tain collections of similar pins. Altogether, Pinterest is the world’s largest user-curated graph of images, with over 2 billion unique pins collected into over 1 billion boards.</li>
</ul>
<p>Through extensive offline metrics, controlled user studies, and A/B tests, we show that our approach achieves state-of-the-art performance compared to other scalable deep content-based rec-ommendation algorithms, in both an item-item recommendation task (i.e., related-pin recommendation), as well as a “homefeed” recommendation task. In offline ranking metrics we improve over the best performing baseline by more than 40%, in head-to-head human evaluations our recommendations are preferred about 60% of the time, and the A/B tests show 30% to 100% improvements in user engagement across various settings.</p>
<p>To our knowledge, this is the largest-ever application of deep graph embeddings and paves the way for new generation of rec-ommendation systems based on graph convolutional architectures.</p>
<ul>
<li>
<p>动态卷积：传统的GCN算法通过将特征矩阵乘以完整图拉普拉斯算子的幂来进行每个图形卷积。相比之下，我们的PinSage算法通过对节点周围的邻域进行采样并从该采样邻域动态构建计算图来执行高效的局部卷积。这些动态构建的计算图（图1）指定了如何在特定节点周围执行局部卷积，并减少了在训练期间对整个图进行操作的需要。</p>
</li>
<li>
<p>生产者 &#8211; 消费者小批量建设：我们开发了一个生产者 &#8211; 消费者体系结构，用于构建微型计算机，确保在模型培训期间最大限度地利用GPU。一个大内存，受CPU限制的生产者有效地采样节点网络邻域并获取必要的特征来定义局部卷积，而受GPU约束的TensorFlow模型使用这些预定义的计算图来有效地运行随机梯度体面。</p>
</li>
<li>
<p>高效的MapReduce推理：给定一个完全训练的GCN模型，我们设计了一个有效的MapReduce管道，可以分解训练的模型，为数十亿个节点生成嵌入，同时最大限度地减少重复计算。</p>
</li>
</ul>
<p>除了可扩展性的这些基本进步之外，我们还引入了新的培训技术和算法创新。这些创新提高了PinSage所学习的表示质量，在下游推荐系统任务中带来了显着的性能提升：</p>
<ul>
<li>通过随机游走构建卷积：使用完整的节点邻域来执行卷积（图1）将导致巨大的计算图，因此我们求助于采样。然而，随机抽样不是最理想的，我们开发了一种使用短随机游走来抽样计算图的新技术。另一个好处是每个节点现在都有一个重要性分数，我们在池化/聚合步骤中使用它。</li>
<li>•重要性池：图卷的核心组件是图中本地邻居的特征信息的聚合。我们引入了一种方法来基于随机游走相似性度量来衡量此聚合中节点特征的重要性，从而使离线评估指标的性能提高46％。</li>
<li>课程培训：我们设计了一个课程培训方案，在培训过程中，算法得到了越来越难的实例，从而使性能提高了12％。<br /> 我们在Pinterest上部署了PinSage用于各种推荐任务，Pinterest是一种流行的内容发现和策展应用，用户可以在其中与引脚交互，引脚是在线内容的可视书签（例如，他们想要烹饪的食谱，或者他们想要购买的衣服） ）。用户将这些引脚组织成板，其中包含类似引脚的集合。总而言之，Pinterest是世界上最大的用户策划图像图形，超过20亿个独特的引脚被收集到超过10亿个电路板中。</li>
</ul>
<p>通过广泛的离线指标，受控用户研究和A / B测试，我们表明，与项目项目推荐任务中的其他可扩展的基于深度内容的推荐算法相比，我们的方法实现了最先进的性能（即相关引脚推荐），以及“主页”推荐任务。在离线排名指标中，我们在最佳绩效基线上的表现提高了40％以上，在人机对话评估中，我们的建议在60％的时间内是首选，A / B测试显示在30％到100％的情况下，用户参与各种设置。</p>
<p>据我们所知，这是有史以来最大的深度图嵌入应用，并为基于图卷积结构的新一代推荐系统铺平了道路。</p>
<h3><a id="2	RELATED_WORK_81"></a>2 RELATED WORK</h3>
<p>Our work builds upon a number of recent advancements in deep learning methods for graph-structured data.</p>
<p>The notion of neural networks for graph data was first outlined in Gori et al. (2005) [15] and further elaborated on in Scarselli et al. (2009) [27]. However, these initial approaches to deep learning on graphs required running expensive neural “message-passing” algorithms to convergence and were prohibitively expensive on large graphs. Some limitations were addressed by Gated Graph Sequence Neural Networks [22]—which employs modern recurrent neural architectures—but the approach remains computationally expensive and has mainly been used on graphs with &lt;10, 000 nodes.</p>
<p>More recently, there has been a surge of methods that rely on the notion of “graph convolutions” or Graph Convolutional Net-works (GCNs). This approach originated with the work of Bruna et al. (2013), which developed a version of graph convolutions based on spectral graph thery [7]. Following on this work, a number of authors proposed improvements, extensions, and approximations of these spectral convolutions [6, 10, 11, 13, 18, 21, 24, 29, 31], lead-ing to new state-of-the-art results on benchmarks such as node classification, link prediction, as well as recommender system tasks (e.g., the MovieLens benchmark [24]). These approaches have con-sistently outperformed techniques based upon matrix factorization or random walks (e.g., node2vec [17] and DeepWalk [26]), and their success has led to a surge of interest in applying GCN-based methods to applications ranging from recommender systems [24] to drug design [20, 31]. Hamilton et al. (2017b) [19] and Bronstein et al. (2017) [6] provide comprehensive surveys of recent advancements.</p>
<p>However, despite the successes of GCN algorithms, no previous works have managed to apply them to production-scale data with billions of nodes and edges—a limitation that is primarily due to the fact that traditional GCN methods require operating on the entire graph Laplacian during training. Here we fill this gap and show that GCNs can be scaled to operate in a production-scale recommender system setting involving billions of nodes/items. Our work also demonstrates the substantial impact that GCNs have on recommendation performance in a real-world environment.</p>
<p>In terms of algorithm design, our work is most closely related to Hamilton et al. (2017a)’s GraphSAGE algorithm [18] and the closely related follow-up work of Chen et al. (2018) [8]. GraphSAGE is an inductive variant of GCNs that we modify to avoid operating on the entire graph Laplacian. We fundamentally improve upon GraphSAGE by removing the limitation that the whole graph be stored in GPU memory, using low-latency random walks to sample<br /> 我们的工作建立在图形结构数据深度学习方法的最新进展之上。</p>
<p>Gori等人首次概述了图数据的神经网络概念。 （2005）[15]并在Scarselli等人的文章中进一步阐述。 （2009）[27]。然而，这些在图上进行深度学习的初始方法需要运行昂贵的神经“消息传递”算法来收敛，并且在大图上非常昂贵。门控图形序列神经网络[22]解决了一些局限性 &#8211; 它采用了现代的递归神经架构 &#8211; 但该方法计算成本仍然很高，主要用于节点数&lt;10,000的图形。</p>
<p>最近，出现了大量依赖“图形卷积”或图形卷积网络（GCN）概念的方法。这种方法起源于Bruna等人的工作。 （2013），基于光谱图[7]开发了一个图形卷积的版本。继这项工作之后，许多作者提出了这些光谱卷积的改进，扩展和近似[6,10,11,13,18,21,24,29,31]，导致了新的状态。 -art结果基于节点分类，链接预测以及推荐系统任务（例如，MovieLens基准测试[24]）等基准测试。这些方法始终优于基于矩阵分解或随机游走的技术（例如，node2vec [17]和DeepWalk [26]），并且它们的成功引起了人们对将基于GCN的方法应用于推荐器等应用的兴趣。系统[24]到药物设计[20,31]。汉密尔顿等人。 （2017b）[19]和Bronstein等。 （2017）[6]提供了最近进展的综合调查。</p>
<p>然而，尽管GCN算法取得了成功，但之前的工作没有设法将它们应用于具有数十亿节点和边缘的生产规模数据 &#8211; 这主要是因为传统的GCN方法需要在整个图形拉普拉斯算子上运行。训练。在这里，我们填补了这一空白，并表明GCN可以扩展到在涉及数十亿节点/项目的生产规模推荐系统设置中运行。我们的工作还证明了GCN对现实环境中的推荐性能产生的重大影响。</p>
<p>在算法设计方面，我们的工作与Hamilton等人的关系最为密切。 （2017a）的GraphSAGE算法[18]和陈等人的密切相关的后续工作。 （2018）[8]。 GraphSAGE是GCN的归纳变体，我们修改它以避免在整个图拉普拉斯算子上运行。我们通过消除整个图存储在GPU内存中的限制，从而使用低延迟随机游走来获取样本，从根本上改进了GraphSAGE</p>
<p>graph neighborhoods in a producer-consumer architecture. We also introduce a number of new training techniques to improve performance and a MapReduce inference pipeline to scale up to graphs with billions of nodes.</p>
<p>Lastly, also note that graph embedding methods like node2vec</p>
<p>[17]and DeepWalk [26] cannot be applied here. First, these are unsupervised methods. Second, they cannot include node feature information. Third, they directly learn embeddings of nodes and thus the number of model parameters is linear with the size of the graph, which is prohibitive for our setting.<br /> 生产者 &#8211; 消费者体系结构中的图形邻域。 我们还介绍了许多用于提高性能的新培训技术和MapReduce推理管道，以扩展到具有数十亿节点的图形。</p>
<p>最后，还要注意图形嵌入方法，如node2vec</p>
<p>[17]和DeepWalk [26]不能在这里应用。 首先，这些是无监督的方法。 其次，它们不能包含节点特征信息。 第三，它们直接学习节点的嵌入，因此模型参数的数量与图形的大小成线性关系，这对我们的设置来说是禁止的。</p>
<h3><a id="3_METHOD_114"></a>3 METHOD</h3>
<p>In this section, we describe the technical details of the PinSage archi-tecture and training, as well as a MapReduce pipeline to efficiently generate embeddings using a trained PinSage model.</p>
<p>The key computational workhorse of our approach is the notion of localized graph convolutions.1 To generate the embedding for a node (i.e., an item), we apply multiple convolutional modules that aggregate feature information (e.g., visual, textual features) from the node’s local graph neighborhood (Figure 1). Each module learns how to aggregate information from a small graph neighbor-hood, and by stacking multiple such modules, our approach can gain information about the local network topology. Importantly, parameters of these localized convolutional modules are shared across all nodes, making the parameter complexity of our approach independent of the input graph size.</p>
<p>在本节中，我们将介绍PinSage架构和培训的技术细节，以及使用经过训练的PinSage模型高效生成嵌入的MapReduce管道。</p>
<p>我们方法的关键计算主力是局部图卷积的概念.1为了生成节点（即项目）的嵌入，我们应用多个卷积模块来聚合来自节点的特征信息（例如，视觉，文本特征）。 局部图邻域（图1）。 每个模块都学习如何聚合来自小图邻居的信息，并通过堆叠多个这样的模块，我们的方法可以获得有关本地网络拓扑的信息。 重要的是，这些局部卷积模块的参数在所有节点之间共享，使得我们方法的参数复杂度与输入图形大小无关。</p>
<h5><a id="31	Problem_Setup_124"></a>3.1 Problem Setup</h5>
<p>Pinterest is a content discovery application where users interact with pins, which are visual bookmarks to online content (e.g., recipes they want to cook, or clothes they want to purchase). Users organize these pins into boards, which contain collections of pins that the user deems to be thematically related. Altogether, the Pinterest graph contains 2 billion pins, 1 billion boards, and over 18 billion edges (i.e., memberships of pins to their corresponding boards).</p>
<p>Our task is to generate high-quality embeddings or representa-tions of pins that can be used for recommendation (e.g., via nearest-neighbor lookup for related pin recommendation, or for use in a downstream re-ranking system). In order to learn these embed-dings, we model the Pinterest environment as a bipartite graph consisting of nodes in two disjoint sets, I (containing pins) and C (containing boards). Note, however, that our approach is also naturally generalizable, with I being viewed as a set of items and C as a set of user-defined contexts or collections.</p>
<p>In addition to the graph structure, we also assume that the pins/items u ∈ I are associated with real-valued attributes, xu ∈ Rd . In general, these attributes may specify metadata or content information about an item, and in the case of Pinterest, we have that pins are associated with both rich text and image features. Our goal is to leverage both these input attributes as well as the structure of the bipartite graph to generate high-quality embed-dings. These embeddings are then used for recommender system candidate generation via nearest neighbor lookup (i.e., given a pin, find related pins) or as features in machine learning systems for ranking the candidates.<br /> Pinterest是一种内容发现应用程序，其中用户与引脚交互，引脚是在线内容的可视书签（例如，他们想要烹饪的食谱，或者他们想要购买的衣服）。用户将这些引脚组织成电路板，其中包含用户认为与主题相关的引脚集合。总而言之，Pinterest图包含20亿个引脚，10亿个电路板和超过180亿个边沿（即，引脚到其相应电路板的成员资格）。</p>
<p>我们的任务是生成可用于推荐的高质量嵌入或引脚表示（例如，通过最近邻居查找以获得相关引脚推荐，或用于下游重新排序系统）。为了学习这些嵌入，我们将Pinterest环境建模为由两个不相交集合中的节点组成的二分图，I（包含引脚）和C（包含板）。但请注意，我们的方法也可以自然地推广，我将其视为一组项目，将C视为一组用户定义的上下文或集合。</p>
<p>除了图形结构之外，我们还假设引脚/项u∈I与实值属性xu∈Rd相关联。通常，这些属性可以指定关于项目的元数据或内容信息，并且在Pinterest的情况下，我们将这些引脚与富文本和图像特征相关联。我们的目标是利用这些输入属性以及二分图的结构来生成高质量的嵌入式数据。然后，这些嵌入用于通过最近邻居查找生成推荐系统候选（即，给定引脚，找到相关引脚）或用作机器学习系统中用于对候选进行排序的特征。</p>
<p>For notational convenience and generality, when we describe the PinSage algorithm, we simply refer to the node set of the full graph with V = I ∪ C and do not explicitly distinguish between pin and board nodes (unless strictly necessary), using the more general term “node” whenever possible.<br /> 为了符号方便性和通用性，当我们描述PinSage算法时，我们简单地引用完整图的节点集，其中V =I∪C并且没有明确区分引脚和板节点（除非严格必要），使用更一般 尽可能使用术语“节点”。</p>
<h5><a id="32	Model_Architecture_139"></a>3.2 Model Architecture</h5>
<p>We use localized convolutional modules to generate embeddings for nodes. We start with input node features and then learn neural networks that transform and aggregate features over the graph to compute the node embeddings (Figure 1).</p>
<p><strong>Forward propagation algorithm.</strong> We consider the task of gener-ating an embedding, zu for a node u, which depends on the node’s input features and the graph structure around this node.<br /> 我们使用局部卷积模块为节点生成嵌入。 我们从输入节点特征开始，然后学习神经网络，在图形上转换和聚合特征以计算节点嵌入（图1）。</p>
<p><strong>前向传播算法</strong> 我们考虑为节点u生成嵌入，zu的任务，这取决于节点的输入特征和该节点周围的图形结构。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707091950238.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>The core of our PinSage algorithm is a localized convolution operation, where we learn how to aggregate information from u’s neighborhood (Figure 1). This procedure is detailed in Algorithm 1 convolve. The basic idea is that we transform the representations zv , ∀v ∈ N(u) of u’s neighbors through a dense neural network and then apply a aggregator/pooling fuction (e.g., a element-wise mean or weighted sum, denoted as γ ) on the resulting set of vectors (Line 1). This aggregation step provides a vector representation, nu , of u’s local neighborhood, N(u). We then concatenate the ag-gregated neighborhood vector nu with u’s current representation hu and transform the concatenated vector through another dense neural network layer (Line 2). Empirically we observe significant performance gains when using concatenation operation instead of the average operation as in [21]. Additionally, the normalization in Line 3 makes training more stable, and it is more efficient to perform approximate nearest neighbor search for normalized embeddings (Section 3.5). The output of the algorithm is a representation of u that incorporates both information about itself and its local graph neighborhood.</p>
<p>Importance-based neighborhoods. An important innovation in our approach is how we define node neighborhoods N(u), i.e., how we select the set of neighbors to convolve over in Algorithm 1. Whereas previous GCN approaches simply examine k-hop graph neighborhoods, in PinSage we define importance-based neighbor-hoods, where the neighborhood of a node u is defined as the T nodes that exert the most influence on node u. Concretely, we simulate random walks starting from node u and compute the L1-normalized</p>
<p>visit count of nodes visited by the random walk [14].2 The neigh-borhood of u is then defined as the top T nodes with the highest normalized visit counts with respect to node u.</p>
<p>The advantages of this importance-based neighborhood defi-nition are two-fold. First, selecting a fixed number of nodes to aggregate from allows us to control the memory footprint of the algorithm during training [18]. Second, it allows Algorithm 1 to take into account the importance of neighbors when aggregating the vector representations of neighbors. In particular, we imple-ment γ in Algorithm 1 as a weighted-mean, with weights defined according to the L1 normalized visit counts. We refer to this new approach as importance pooling.</p>
<p>Stacking convolutions. Each time we apply the convolve opera-tion (Algorithm 1) we get a new representation for a node, and we can stack multiple such convolutions on top of each other in order to gain more information about the local graph structure around node u. In particular, we use multiple layers of convolutions, where the inputs to the convolutions at layer k depend on the representa-tions output from layer k − 1 (Figure 1) and where the initial (i.e., “layer 0”) representations are equal to the input node features. Note that the model parameters in Algorithm 1 (Q, q, W, and w) are shared across the nodes but differ between layers.<br /> 我们的PinSage算法的核心是局部卷积运算，我们学习如何从u的邻域聚合信息（图1）。此过程在算法1卷入中详细说明。基本思想是我们通过密集神经网络转换u邻居的表示zv，∀v∈N（u）然后应用聚合器/池化函数（例如，元素方式或加权和，表示为γ）在得到的矢量集上（第1行）。该聚合步骤提供了u的局部邻域N（u）的向量表示nu。然后，我们将ag-gregated邻域向量nu与u的当前表示hu连接，并将连接的向量转换为另一个密集的神经网络层（第2行）。根据经验，我们在使用串联操作而不是[21]中的平均操作时观察到显着的性能提升。此外，第3行中的归一化使训练更加稳定，并且对归一化嵌入执行近似最近邻搜索更有效（第3.5节）。算法的输出是u的表示，其包含关于其自身及其局部图邻域的信息。</p>
<p>基于重要性的社区。我们方法的一个重要创新是我们如何定义节点邻域N（u），即我们如何选择在算法1中进行卷积的邻居集合。而以前的GCN方法只是检查k-hop图形邻域，在PinSage中我们定义重要性基于邻居的邻居，其中节点u的邻域被定义为对节点u施加最大影响的T节点。具体地说，我们模拟从节点u开始的随机游走并计算L1标准化</p>
<p>访问随机游走所访问的节点数[14] .2然后将u的邻域定义为相对于节点u具有最高归一化访问计数的前T个节点。</p>
<p>这种基于重要性的邻域定义的优势是双重的。首先，选择要聚合的固定数量的节点允许我们在训练期间控制算法的内存占用[18]。其次，它允许算法1在聚合邻居的向量表示时考虑邻居的重要性。特别地，我们在算法1中实现γ作为加权平均值，其中权重根据L1归一化访问计数来定义。我们将这种新方法称为重要性池。</p>
<p>堆叠卷积。每次我们应用卷积运算（算法1）时，我们得到一个节点的新表示，并且我们可以将多个这样的卷积堆叠在彼此之上，以便获得关于节点u周围的局部图结构的更多信息。特别地，我们使用多层卷积，其中层k处的卷积的输入取决于层k-1（图1）的表示和初始（即“层0”）表示相等的表示。到输入节点功能。请注意，算法1（Q，q，W和w）中的模型参数在节点之间共享，但层之间不同。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092102872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709213627.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>We first describe our margin-based loss function in detail. Follow-ing this, we give an overview of several techniques we developed that lead to the computation efficiency and fast convergence rate of PinSage, allowing us to train on billion node graphs and billions training examples. And finally, we describe our curriculum-training scheme, which improves the overall quality of the recommendations.</p>
<p>我们首先详细描述基于保证金的损失函数。 接下来，我们概述了我们开发的几种技术，这些技术可以提高PinSage的计算效率和快速收敛速度，使我们能够训练数十亿个节点图和数十亿个训练样例。 最后，我们描述了我们的课程培训计划，该计划提高了建议的整体质量。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709224222.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Loss function. In order to train the parameters of the model, we use a max-margin-based loss function. The basic idea is that we want to maximize the inner product of positive examples, i.e., the embedding of the query item and the corresponding related item. At the same time we want to ensure that the inner product of negative examples—i.e., the inner product between the embedding of the query item and an unrelated item—is smaller than that of the positive sample by some pre-defined margin. The loss function for a<br /> 损失功能。 为了训练模型的参数，我们使用基于最大边际的损失函数。 基本思想是我们希望最大化正例的内积，即嵌入查询项和相应的相关项。 同时，我们希望确保否定示例的内积 &#8211; 即，查询项的嵌入与不相关项之间的内积 &#8211; 比正样本的内积小一些预定义的余量。 a的损失函数<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709234097.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Multi-GPU training with large minibatches. To make full use of multiple GPUs on a single machine for training, we run the for-ward and backward propagation in a multi-tower fashion. With multiple GPUs, we first divide each minibatch (Figure 1 bottom) into equal-sized portions. Each GPU takes one portion of the minibatch and performs the computations using the same set of parameters. Af-ter backward propagation, the gradients for each parameter across all GPUs are aggregated together, and a single step of synchronous SGD is performed. Due to the need to train on extremely large number of examples (on the scale of billions), we run our system with large batch sizes, ranging from 512 to 4096.</p>
<p>We use techniques similar to those proposed by Goyal et al. [16] to ensure fast convergence and maintain training and generalization accuracy when dealing with large batch sizes. We use a gradual warmup procedure that increases learning rate from small to a peak value in the first epoch according to the linear scaling rule. Afterwards the learning rate is decreased exponentially.</p>
<p>Producer-consumer minibatch construction. During training, the adjacency list and the feature matrix for billions of nodes are placed in CPU memory due to their large size. However, during the convolve step of PinSage, each GPU process needs access to the neighborhood and feature information of nodes in the neighbor-hood. Accessing the data in CPU memory from GPU is not efficient. To solve this problem, we use a re-indexing technique to create a sub-graph G′ = (V ′, E′) containing nodes and their neighborhood, which will be involved in the computation of the current minibatch. A small feature matrix containing only node features relevant to computation of the current minibatch is also extracted such that the order is consistent with the index of nodes in G′. The adjacency list of G′ and the small feature matrix are fed into GPUs at the start of each minibatch iteration, so that no communication between the GPU and CPU is needed during the convolve step, greatly improving GPU utilization.</p>
<p>The training procedure has alternating usage of CPUs and GPUs. The model computations are in GPUs, whereas extracting features, re-indexing, and negative sampling are computed on CPUs. In ad-dition to parallelizing GPU computation with multi-tower training, and CPU computation using OpenMP [25], we design a producer-consumer pattern to run GPU computation at the current iteration and CPU computation at the next iteration in parallel. This further reduces the training time by almost a half.</p>
<p>Sampling negative items. Negative sampling is used in our loss function (Equation 1) as an approximation of the normalization factor of edge likelihood [23]. To improve efficiency when training with large batch sizes, we sample a set of 500 negative items to be shared by all training examples in each minibatch. This drastically saves the number of embeddings that need to be computed during each training step, compared to running negative sampling for each node independently. Empirically, we do not observe a difference between the performance of the two sampling schemes.</p>
<p>In the simplest case, we could just uniformly sample negative examples from the entire set of items. However, ensuring that the inner product of the positive example (pair of items (q, i )) is larger than that of the q and each of the 500 negative items is too “easy” and does not provide fine enough “resolution” for the system to learn. In particular, our recommendation algorithm should be capable of finding 1,000 most relevant items to q among the catalog of over 2 billion items. In other words, our model should be able to distinguish/identify 1 item out of 2 million items. But with 500 random negative items, the model’s resolution is only 1 out of</p>
<p>500.Thus, if we sample 500 random negative items out of 2 billion items, the chance of any of these items being even slightly related to the query item is small. Therefore, with large probability the learning will not make good parameter updates and will not be able to differentiate slightly related items from the very related ones.</p>
<p>To solve the above problem, for each positive training example (i.e., item pair (q, i)), we add “hard” negative examples, i.e., items that are somewhat related to the query item q, but not as related as the positive item i. We call these “hard negative items”. They are generated by ranking items in a graph according to their Per-sonalized PageRank scores with respect to query item q [14]. Items ranked at 2000-5000 are randomly sampled as hard negative items. As illustrated in Figure 2, the hard negative examples are more similar to the query than random negative examples, and are thus challenging for the model to rank, forcing the model to learn to distinguish items at a finer granularity.<br /> 使用大型微型计算机进行多GPU培训。为了在一台机器上充分利用多个GPU进行培训，我们以多塔式方式进行前向和后向传播。对于多个GPU，我们首先将每个小批量（图1底部）划分为相等大小的部分。每个GPU获取一部分小批量并使用相同的参数集执行计算。在后向传播之后，所有GPU上的每个参数的梯度被聚合在一起，并且执行同步SGD的单个步骤。由于需要训练大量的例子（数十亿的规模），我们运行我们的系统，批量大，从512到4096。</p>
<p>我们使用类似于Goyal等人提出的技术。 [16]确保在处理大批量时快速收敛并保持培训和泛化准确性。我们使用渐进的预热程序，根据线性缩放规则，在第一个时期内将学习率从小值提高到峰值。之后学习率呈指数下降。</p>
<p>生产者 &#8211; 消费者小批量建设。在训练期间，由于数据量大，数十亿个节点的邻接列表和特征矩阵被放置在CPU内存中。但是，在PinSage的卷积步骤中，每个GPU进程都需要访问邻居和邻居中节点的特征信息。从GPU访问CPU内存中的数据效率不高。为了解决这个问题，我们使用重新索引技术来创建包含节点及其邻域的子图G’=（V’，E’），其将参与当前小批量的计算。还提取仅包含与当前小批量的计算相关的节点特征的小特征矩阵，使得该顺序与G’中的节点索引一致。 G’的邻接列表和小特征矩阵在每个小批量迭代开始时被馈送到GPU中，因此在卷积步骤期间不需要GPU和CPU之间的通信，从而大大提高了GPU利用率。</p>
<p>训练过程交替使用CPU和GPU。模型计算在GPU中，而在CPU上计算提取特征，重新索引和负抽样。除了使用多塔培训并行GPU计算和使用OpenMP进行CPU计算[25]之外，我们还设计了一个生产者 &#8211; 消费者模式，以便在当前迭代中运行GPU计算，并在下一次迭代中并行运行CPU计算。这进一步将训练时间减少了近一半。</p>
<p>采样负面项目。在我们的损失函数（等式1）中使用负采样作为边缘似然归一化因子的近似值[23]。为了提高大批量培训的效率，我们对每个小批量的所有培训示例共享500个负面项目。与每个节点独立运行负采样相比，这大大节省了每个训练步骤中需要计算的嵌入数量。根据经验，我们没有观察到两种抽样方案的性能差异。</p>
<p>在最简单的情况下，我们可以从整个项目集合中统一采样负面示例。但是，确保正例（项目对（q，i））的内积大于q的内积，并且500个负项中的每一个都太“容易”并且不能提供足够精细的“分辨率”。系统要学习。特别是，我们的推荐算法应该能够在超过20亿个项目的目录中找到1000个最相关的项目。换句话说，我们的模型应该能够区分/识别200万个项目中的1个项目。但是有500个随机负面项目，模型的分辨率只有1个</p>
<p>500.因此，如果我们从20亿个项目中抽取500个随机负面项目，那么这些项目中任何一个与查询项目稍微相关的可能性就很小。因此，很有可能学习不会进行良好的参数更新，并且无法区分略微相关的项目与非常相关的项目。</p>
<p>为了解决上述问题，对于每个积极的训练样例（即项目对（q，i）），我们添加“硬”否定示例，即与查询项目q有些相关的项目，但不是与积极的项目我我们将这些称为“硬性负面物品”。它们是通过根据查询项q [14]的Per-sonalized PageRank分数对图表中的项目进行排名而生成的。排名为2000-5000的项目被随机抽样为硬阴性项目。如图2所示，硬否定示例与查询比随机否定示例更相似，因此难以对模型进行排名，迫使模型学习以更精细的粒度区分项目。</p>
<p>Using hard negative items throughout the training procedure doubles the number of epochs needed for the training to con-verge. To help with convergence, we develop a curriculum training scheme [4]. In the first epoch of training, no hard negative items are used, so that the algorithm quickly finds an area in the parameter space where the loss is relatively small. We then add hard negative items in subsequent epochs, focusing the model to learn how to distinguish highly related pins from only slightly related ones. At epoch n of the training, we add n − 1 hard negative items to the set of negative items for each item.<br /> 在整个训练过程中使用硬阴性项目会使训练所需的时期数量增加一倍。 为了帮助融合，我们制定了课程培训计划[4]。 在第一个训练时期，没有使用硬负项，因此算法快速找到参数空间中损失相对较小的区域。 然后，我们在后续时期添加硬阴性项，重点关注模型，以了解如何区分高度相关的引脚和仅略微相关的引脚。 在训练的时期，我们将n &#8211; 1个硬阴性项添加到每个项目的负项目集中。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092622891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> Figure 2: Random negative examples and hard negative ex-amples. Notice that the hard negative example is signifi-cantly more similar to the query, than the random negative example, though not as similar as the positive example.<br /> 图2：随机负面例子和硬性负面例子。 请注意，与负面示例相比，硬负面示例显着更类似于查询，尽管与正面示例不太相似。</p>
<h5><a id="34	Node_Embeddings_via_MapReduce_223"></a>3.4 Node Embeddings via MapReduce</h5>
<p>After the model is trained, it is still challenging to directly apply the trained model to generate embeddings for all items, including those that were not seen during training. Naively computing embeddings for nodes using Algorithm 2 leads to repeated computations caused by the overlap between K-hop neighborhoods of nodes. As illus-trated in Figure 1, many nodes are repeatedly computed at multiple layers when generating the embeddings for different target nodes. To ensure efficient inference, we develop a MapReduce approach that runs model inference without repeated computations.</p>
<p>We observe that inference of node embeddings very nicely lends itself to MapReduce computational model. Figure 3 details the data flow on the bipartite pin-to-board Pinterest graph, where we assume the input (i.e., “layer-0”) nodes are pins/items (and the layer-1 nodes are boards/contexts). The MapReduce pipeline has two key parts:</p>
<p>(1)One MapReduce job is used to project all pins to a low-dimensional latent space, where the aggregation operation will be performed (Algorithm 1, Line 1).</p>
<p>(2)Another MapReduce job is then used to join the resulting pin representations with the ids of the boards they occur in, and the board embedding is computed by pooling the features of its (sampled) neighbors.</p>
<p>Note that our approach avoids redundant computations and that the latent vector for each node is computed only once. After the em-beddings of the boards are obtained, we use two more MapReduce jobs to compute the second-layer embeddings of pins, in a similar fashion as above, and this process can be iterated as necessary (up to K convolutional layers).3</p>
<p>在训练模型之后，直接应用训练模型来生成所有项目的嵌入仍然具有挑战性，包括在训练期间未看到的项目。使用算法2对节点进行初始计算嵌入导致由节点的K跳邻域之间的重叠引起的重复计算。如图1所示，当为不同的目标节点生成嵌入时，在多个层重复计算许多节点。为了确保有效的推理，我们开发了一种MapReduce方法，该方法运行模型推理而无需重复计算。</p>
<p>我们观察到节点嵌入的推断非常好地适用于MapReduce计算模型。图3详述了二分针对板Pinterest图的数据流，其中我们假设输入（即“第0层”）节点是引脚/项（并且第1层节点是板/上下文）。 MapReduce管道有两个关键部分：</p>
<p>（1）一个MapReduce作业用于将所有引脚投影到低维潜在空间，其中将执行聚合操作（算法1，第1行）。</p>
<p>（2）然后使用另一个MapReduce作业将得到的引脚表示与它们出现的板的ID相连，并且通过汇集其（采样的）邻居的特征来计算板嵌入。</p>
<p>请注意，我们的方法避免了冗余计算，并且每个节点的潜在向量仅计算一次。在获得电路板的em-bedding之后，我们使用另外两个MapReduce作业以与上面类似的方式计算引脚的第二层嵌入，并且可以根据需要迭代该过程（直到K个卷积层）。</p>
<h6><a id="35	Efficient_nearestneighbor_lookups_249"></a>3.5 Efficient nearest-neighbor lookups</h6>
<p>The embeddings generated by PinSage can be used for a wide range of downstream recommendation tasks, and in many settings we can directly use these embeddings to make recommendations by performing nearest-neighbor lookups in the learned embedding space. That is, given a query item q, the we can recommend items whose embeddings are the K-nearest neighbors of the query item’s embedding. Approximate KNN can be obtained efficiently via lo-cality sensitive hashing [2]. After the hash function is computed, retrieval of items can be implemented with a two-level retrieval pro-cess based on the Weak AND operator [5]. Given that the PinSage model is trained offline and all node embeddings are computed via MapReduce and saved in a database, the efficient nearest-neighbor lookup operation enables the system to serve recommendations in an online fashion,<br /> PinSage生成的嵌入可用于各种下游推荐任务，在许多设置中，我们可以通过在学习的嵌入空间中执行最近邻查找来直接使用这些嵌入来提出建议。 也就是说，给定查询项q，我们可以推荐其嵌入是查询项嵌入的K最近邻居的项。 通过物理敏感散列可以有效地获得近似KNN [2]。 在计算散列函数之后，可以使用基于Weak AND运算符的两级检索过程来实现项目的检索[5]。 鉴于PinSage模型是离线训练的，并且所有节点嵌入都是通过MapReduce计算并保存在数据库中，有效的最近邻查找操作使系统能够以在线方式提供建议，</p>
<h3><a id="4_EXPERIMENTS_253"></a>4 EXPERIMENTS</h3>
<p>To demonstrate the efficiency of PinSage and the quality of the embeddings it generates, we conduct a comprehensive suite of experiments on the entire Pinterest object graph, including offline experiments, production A/B tests as well as user studies.<br /> 为了证明PinSage的效率及其产生的嵌入质量，我们对整个Pinterest对象图进行了一整套实验，包括离线实验，生产A / B测试以及用户研究。</p>
<h6><a id="41	Experimental_Setup_258"></a>4.1 Experimental Setup</h6>
<p>We evaluate the embeddings generated by PinSage in two tasks: recommending related pins and recommending pins in a user’s home/news feed. To recommend related pins, we select the K near-est neighbors to the query pin in the embedding space. We evaluate performance on this related-pin recommendation task using both offline ranking measures as well as a controlled user study. For the homefeed recommendation task, we select the pins that are closest in the embedding space to one of the most recently pinned items by the user. We evaluate performance of a fully-deployed production system on this task using A/B tests to measure the overall impact on user engagement.</p>
<p>Training details and data preparation. We define the set, L, of positive training examples (Equation (1)) using historical user engagement data. In particular, we use historical user engagement data to identify pairs of pins (q, i), where a user interacted with pin</p>
<p>iimmediately after she interacted with pin q. We use all other pins as negative items (and sample them as described in Section 3.3). Overall, we use 1.2 billion pairs of positive training examples (in addition to 500 negative examples per batch and 6 hard negative examples per pin). Thus in total we use 7.5 billion training examples.<br /> Since PinSage can efficiently generate embeddings for unseen data, we only train on a subset of the Pinterest graph and then generate embeddings for the entire graph using the MapReduce pipeline described in Section 3.4. In particular, for training we use a randomly sampled subgraph of the entire graph, containing 20% of all boards (and all the pins touched by those boards) and 70% of the labeled examples. During hyperparameter tuning, a remaining 10% of the labeled examples are used. And, when testing, we run inference on the entire graph to compute embeddings for all 2 billion pins, and the remaining 20% of the labeled examples are used to test the recommendation performance of our PinSage in the offline evaluations. Note that training on a subset of the full graph drastically decreased training time, with a negligible impact on final performance. In total, the full datasets for training and evaluation are approximately 18TB in size with the full output embeddings being 4TB.<br /> 我们评估PinSage在两个任务中生成的嵌入：推荐相关引脚并在用户的主页/新闻源中推荐引脚。为了推荐相关引脚，我们选择嵌入空间中查询引脚的K近邻。我们使用离线排名度量和受控用户研究来评估此相关引脚推荐任务的性能。对于主页提交推荐任务，我们选择嵌入空间中最接近用户最近固定项目之一的引脚。我们使用A / B测试来评估完全部署的生产系统在此任务上的性能，以衡量对用户参与的总体影响。</p>
<p>培训细节和数据准备。我们使用历史用户参与数据定义积极训练样例（等式（1））的集合L.特别地，我们使用历史用户参与数据来识别用户与引脚交互的引脚对（q，i）</p>
<p>她与pin q交互后立刻。我们使用所有其他引脚作为负项（并按照第3.3节中的描述对它们进行采样）。总的来说，我们使用了12亿对正面训练示例（除了每批500个负面示例和每个针脚6个硬负面示例）。因此，我们总共使用了75亿个培训示例。<br /> 由于PinSage可以有效地为看不见的数据生成嵌入，因此我们只训练Pinterest图的子集，然后使用3.4节中描述的MapReduce管道为整个图生成嵌入。特别是，对于训练，我们使用整个图形的随机采样子图，包含20％的所有板（以及这些板触及的所有引脚）和70％的标记示例。在超参数调整期间，使用剩余的10％的标记示例。并且，在测试时，我们对整个图形进行推断，以计算所有20亿个引脚的嵌入，其余20％的标记示例用于测试我们的PinSage在离线评估中的推荐性能。请注意，对完整图表子集的培训大大减少了培训时间，对最终性能的影响可以忽略不计。总的来说，用于训练和评估的完整数据集大小约为18TB，完整输出嵌入为4TB。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092854376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Features used for learning. Each pin at Pinterest is associated with an image and a set of textual annotations (title, description). To generate feature representation xq for each pin q, we concatenate visual embeddings (4,096 dimensions), textual annotation embed-dings (256 dimensions), and the log degree of the node/pin in the graph. The visual embeddings are the 6-th fully connected layer of a classification network using the VGG-16 architecture [28]. Tex-tual annotation embeddings are trained using a Word2Vec-based model [23], where the context of an annotation consists of other annotations that are associated with each pin.</p>
<p>Baselines for comparison. We evaluate the performance of Pin-Sage against the following state-of-the-art content-based, graph-based and deep learning baselines that generate embeddings of pins:</p>
<p>(1)Visual embeddings (Visual): Uses nearest neighbors of deep visual embeddings for recommendations. The visual features are described above.</p>
<p>(2)Annotation embeddings (Annotation): Recommends based on nearest neighbors in terms of annotation embeddings. The annotation embeddings are described above.<br /> (3)Combined embeddings (Combined): Recommends based on concatenating visual and annotation embeddings, and using a 2-layer multi-layer perceptron to compute embeddings that capture both visual and annotation features.</p>
<p>(4)Graph-based method (Pixie): This random-walk-based method [14] uses biased random walks to generate ranking scores by simulating random walks starting at query pin q. Items with top K scores are retrieved as recommendations. While this approach does not generate pin embeddings, it is currently the state-of-the-art at Pinterest for certain recommendation tasks [14] and thus an informative baseline.</p>
<p>The visual and annotation embeddings are state-of-the-art deep learning content-based systems currently deployed at Pinterest to generate representations of pins. Note that we do not compare against other deep learning baselines from the literature simply due to the scale of our problem. We also do not consider non-deep learning approaches for generating item/content embeddings, since other works have already proven state-of-the-art performance of deep learning approaches for generating such embeddings [9, 12, 24].</p>
<p>用于学习的功能。 Pinterest中的每个引脚都与一个图像和一组文本注释（标题，描述）相关联。为了生成每个引脚q的特征表示xq，我们连接了视觉嵌入（4,096维），文本注释嵌入（256维）以及图中节点/引脚的对数度。视觉嵌入是使用VGG-16架构的分类网络的第6个完全连接层[28]。使用基于Word2Vec的模型[23]训练文本注释嵌入，其中注释的上下文包含与每个引脚相关联的其他注释。</p>
<p>比较基线。我们针对以下基于内容的，基于图形的深度学习基线评估了Pin-Sage的性能，这些基线生成了引脚的嵌入：</p>
<p>（1）视觉嵌入（Visual）：使用深度视觉嵌入的最近邻居作为推荐。视觉特征如上所述。</p>
<p>（2）注释嵌入（注释）：根据注释嵌入推荐基于最近邻居。注释嵌入如上所述。<br /> （3）组合嵌入（组合）：推荐基于连接视觉和注释嵌入，并使用2层多层感知器计算捕获视觉和注释特征的嵌入。</p>
<p>（4）基于图的方法（Pixie）：这种基于随机游走的方法[14]通过模拟从查询引脚q开始的随机游走来使用偏向随机游走来生成排名分数。具有最高K分数的项目被检索作为推荐。虽然这种方法不会产生引脚嵌入，但它目前是Pinterest中针对某些推荐任务[14]的最新技术，因此也是一个信息基线。</p>
<p>视觉和注释嵌入是目前在Pinterest部署的最先进的基于深度学习内容的系统，用于生成引脚的表示。请注意，由于问题的严重性，我们不会与文献中的其他深度学习基线进行比较。我们也不考虑用于生成项目/内容嵌入的非深度学习方法，因为其他工作已经证明了用于生成这种嵌入的深度学习方法的最先进性能[9,12,24]。</p>
<p>We also conduct ablation studies and consider several variants of PinSage when evaluating performance:</p>
<ul>
<li>
<p>max-pooling uses the element-wise max as a symmetric aggre-gation function (i.e., γ = max) without hard negative samples;</p>
</li>
<li>
<p>mean-pooling uses the element-wise mean as a symmetric aggregation function (i.e., γ = mean);</p>
</li>
<li>
<p>mean-pooling-xent is the same as mean-pooling but uses the cross-entropy loss introduced in [18].</p>
</li>
<li>
<p>mean-pooling-hard is the same as mean-pooling, except that it incorporates hard negative samples as detailed in Section 3.3.</p>
</li>
<li>
<p>PinSage uses all optimizations presented in this paper, includ-ing the use of importance pooling in the convolution step.</p>
</li>
</ul>
<p>The max-pooling and cross-entropy settings are extensions of the best-performing GCN model from Hamilton et al. [18]—other vari-ants (e.g., based on Kipf et al. [21]) performed significantly worse in development tests and are omitted for brevity.4 For all the above variants, we used K = 2, hidden dimension size m = 2048, and set the embedding dimension d to be 1024.</p>
<p>Computation resources. Training of PinSage is implemented in TensorFlow [1] and run on a single machine with 32 cores and 16 Tesla K80 GPUs. To ensure fast fetching of item’s visual and annotation features, we store them in main memory, together with the graph, using Linux HugePages to increase the size of virtual memory pages from 4KB to 2MB. The total amount of memory used in training is 500GB. Our MapReduce inference pipeline is run on a Hadoop2 cluster with 378 d2.8xlarge Amazon AWS nodes.</p>
<p>我们还进行消融研究，并在评估性能时考虑PinSage的几种变体：</p>
<ul>
<li>
<p>max-pooling使用逐元素max作为对称聚合函数（即γ= max）而没有硬阴性样本;</p>
</li>
<li>
<p>均值池使用元素均值作为对称聚合函数（即，γ=均值）;</p>
</li>
<li>
<p>mean-pooling-xent与均值池相同，但使用[18]中引入的交叉熵损失。</p>
</li>
<li>
<p>mean-pooling-hard与mean-pooling相同，除了它包含第3.3节中详述的硬阴性样本。</p>
</li>
<li>
<p>PinSage使用本文中介绍的所有优化，包括在卷积步骤中使用重要性池。</p>
</li>
</ul>
<p>最大池和交叉熵设置是Hamilton等人的最佳性能GCN模型的扩展。 [18]其他变量（例如，基于Kipf等人[21]）在开发测试中表现更差，为简洁省略.4对于所有上述变体，我们使用K = 2，隐藏尺寸大小为m = 2048，并将嵌入维度d设置为1024。</p>
<p>计算资源。 PinSage的培训在TensorFlow [1]中实施，并在具有32个核心和16个Tesla K80 GPU的单台机器上运行。为了确保快速获取项目的视觉和注释功能，我们将它们与图形一起存储在主存储器中，使用Linux HugePages将虚拟内存页面的大小从4KB增加到2MB。培训中使用的内存总量为500GB。我们的MapReduce推理管道在具有378个d2.8xlarge Amazon AWS节点的Hadoop2集群上运行。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092944123.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>表1：PinSage和基于内容的深度学习基线的命中率和MRR。 总体而言，PinSage在最佳基线上的命中率提高了150％，MRR提高了60％</p>
<h4><a id="42	Offline_Evaluation_333"></a>4.2 Offline Evaluation</h4>
<p>To evaluate performance on the related pin recommendation task, we define the notion of hit-rate. For each positive pair of pins (q, i) in the test set, we use q as a query pin and then compute its top<br /> Knearest neighbors NNq from a sample of 5 million test pins. We then define the hit-rate as the fraction of queries q where i was ranked among the top K of the test sample (i.e., where i ∈ NNq ). This metric directly measures the probability that recommendations made by the algorithm contain the items related to the query pin q. In our experiments K is set to be 500.</p>
<p>We also evaluate the methods using Mean Reciprocal Rank (MRR), which takes into account of the rank of the item j among recommended items for query item q:<br /> 为了评估相关引脚推荐任务的性能，我们定义了命中率的概念。 对于测试集中的每个正对引脚（q，i），我们使用q作为查询引脚，然后计算其顶部<br /> 来自500万个测试引脚样本的最近邻NNq。 然后，我们将命中率定义为查询q的分数，其中i在测试样本的前K中排名（即，其中i∈NNq）。 该度量直接测量算法所做推荐包含与查询引脚q相关的项目的概率。 在我们的实验中，K设定为500。</p>
<p>我们还使用均值倒数等级（MRR）来评估方法，其考虑了查询项目q的推荐项目中的项目j的等级：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093036909.png" alt="在这里插入图片描述"><br /> Due to the large pool of candidates (more than 2 billion), we use a scaled version of the MRR in Equation (2), where Ri, q is the rank of item i among recommended items for query q, and n is the total number of labeled item pairs. The scaling factor 100 ensures that, for example, the difference between rank at 1, 000 and rank at 2, 000 is still noticeable, instead of being very close to 0.</p>
<p>Table 1 compares the performance of the various approaches using the hit rate as well as the MRR.5 PinSage with our new importance-pooling aggregation and hard negative examples achieves the best performance at 67% hit-rate and 0.59 MRR, outperforming the top baseline by 40% absolute (150% relative) in terms of the hit rate and also 22% absolute (60% relative) in terms of MRR. We also observe that combining visual and textual information works much better than using either one alone (60% improvement of the combined approach over visual/annotation only).</p>
<p>Embedding similarity distribution. Another indication of the effectiveness of the learned embeddings is that the distances be-tween random pairs of item embeddings are widely distributed. If all items are at about the same distance (i.e., the distances are tightly clustered) then the embedding space does not have enough “resolu-tion” to distinguish between items of different relevance. Figure 4 plots the distribution of cosine similarities between pairs of items using annotation, visual, and PinSage embeddings. This distribution of cosine similarity between random pairs of items demonstrates the effectiveness of PinSage, which has the most spread out distribu-tion. In particular, the kurtosis of the cosine similarities of PinSage embeddings is 0.43, compared to 2.49 for annotation embeddings and 1.20 for visual embeddings.<br /> Another important advantage of having such a wide-spread in the embeddings is that it reduces the collision probability of the subsequent LSH algorithm, thus increasing the efficiency of serving the nearest neighbor pins during recommendation.<br /> 由于候选人数量大（超过20亿），我们在等式（2）中使用MRR的缩放版本，其中Ri，q是查询q的推荐项目中项目i的等级，n是总数标记的项目对的数量。缩放因子100确保例如，等级为1,000和等级为2,000的差异仍然是显着的，而不是非常接近于0。</p>
<p>表1比较了使用命中率的各种方法的性能以及MRR.5 PinSage与我们新的重要性汇总聚合和硬阴性示例在67％命中率和0.59 MRR下达到最佳性能，优于最高基线就命中率而言绝对值为40％（相对于150％），就MRR而言绝对值为22％（相对于60％）。我们还观察到，将视觉和文本信息结合起来比单独使用任何一个都要好得多（组合方法仅比视觉/注释提高60％）。</p>
<p>嵌入相似度分布。学习嵌入的有效性的另一个指示是随机对项目嵌入之间的距离被广泛分布。如果所有项目都处于大约相同的距离（即，距离紧密聚集），则嵌入空间没有足够的“分辨率”来区分不同相关的项目。图4绘制了使用注释，视觉和PinSage嵌入的项目对之间余弦相似度的分布。随机对项之间的余弦相似性的这种分布证明了PinSage的有效性，其具有最大的分布。特别是，PinSage嵌入的余弦相似度的峰度为0.43，而注释嵌入为2.49，视觉嵌入为1.20。<br /> 在嵌入中具有如此广泛的扩展的另一个重要优点是它降低了后续LSH算法的冲突概率，从而提高了在推荐期间服务最近邻居引脚的效率。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093124819.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 图4：视觉嵌入，注释嵌入和Pin-Sage嵌入的成对余弦相似度的概率密度。</p>
<h4><a id="43	User_Studies_359"></a>4.3 User Studies</h4>
<p>We also investigate the effectiveness of PinSage by performing head-to-head comparison between different learned representations. In the user study, a user is presented with an image of the query pin, together with two pins retrieved by two different recommendation algorithms. The user is then asked to choose which of the two candidate pins is more related to the query pin. Users are instructed to find various correlations between the recommended items and the query item, in aspects such as visual appearance, object category and personal identity. If both recommended items seem equally related, users have the option to choose “equal”. If no consensus is reached among 2/3 of users who rate the same question, we deem the result as inconclusive.</p>
<p>Table 2 shows the results of the head-to-head comparison be-tween PinSage and the 4 baselines. Among items for which the user has an opinion of which is more related, around 60% of the pre-ferred items are recommended by PinSage. Figure 5 gives examples of recommendations and illustrates strengths and weaknesses of the different methods. The image to the left represents the query item. Each row to the right corresponds to the top recommendations made by the visual embedding baseline, annotation embedding baseline, Pixie, and PinSage. Although visual embeddings gener-ally predict categories and visual similarity well, they occasionally make large mistakes in terms of image semantics. In this example, visual information confused plants with food, and tree logging with war photos, due to similar image style and appearance. The graph-based Pixie method, which uses the graph of pin-to-board relations, correctly understands that the category of query is “plants” and it recommends items in that general category. However, it does not find the most relevant items. Combining both visual/textual and graph information, PinSage is able to find relevant items that are both visually and topically similar to the query item.</p>
<p>我们还通过在不同的学习表示之间进行头对头比较来研究PinSage的有效性。在用户研究中，向用户呈现查询引脚的图像，以及由两个不同推荐算法检索的两个引脚。然后要求用户选择两个候选引脚中的哪一个与查询引脚更相关。指示用户在视觉外观，对象类别和个人身份等方面找到推荐项目和查询项目之间的各种相关性。如果两个推荐项似乎相同，则用户可以选择“相等”。如果对同一问题评分的2/3的用户未达成共识，我们认为结果不确定。</p>
<p>表2显示了PinSage与4个基线之间的头对头比较结果。在用户具有更多相关意见的项目中，PinSage推荐约60％的优先项目。图5给出了建议的示例，并说明了不同方法的优缺点。左侧的图像表示查询项。右侧的每一行对应于可视嵌入基线，注释嵌入基线，Pixie和PinSage所做的最高建议。虽然视觉嵌入通常可以很好地预测类别和视觉相似性，但它们偶尔会在图像语义方面犯大错误。在这个例子中，由于类似的图像样式和外观，视觉信息使植物与食物混淆，并且树木记录与战争照片。基于图形的Pixie方法使用引脚到板的关系图，正确地理解查询的类别是“植物”，并且它推荐该一般类别中的项目。但是，它没有找到最相关的项目。结合视觉/文本和图形信息，PinSage能够找到与查询项目在视觉上和主题上相似的相关项目。</p>
<p>In addition, we visualize the embedding space by randomly choosing 1000 items and compute the 2D t-SNE coordinates from the PinSage embedding, as shown in Figure 6.6 We observe that the proximity of the item embeddings corresponds well with the simi-larity of content, and that items of the same category are embedded into the same part of the space. Note that items that are visually different but have the same theme are also close to each other in the embedding space, as seen by the items depicting different fashion-related items on the bottom side of the plot.</p>
<p>此外，我们通过随机选择1000个项目来可视化嵌入空间，并从PinSage嵌入计算2D t-SNE坐标，如图6.6所示。我们观察到项目嵌入的接近度与内容的相似性很好地对应， 并且相同类别的项目嵌入到空间的相同部分中。 请注意，视觉上不同但具有相同主题的项目在嵌入空间中也彼此接近，如在绘图底部描绘不同时尚相关项目的项目所示。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093244394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="44	Production_AB_Test_375"></a>4.4 Production A/B Test</h4>
<p>Lastly, we also report on the production A/B test experiments, which compared the performance of PinSage to other deep learning content-based recommender systems at Pinterest on the task of homefeed recommendations. We evaluate the performance by ob-serving the lift in user engagement. The metric of interest is repin rate, which measures the percentage of homefeed recommendations that have been saved by the users. A user saving a pin to a board is a high-value action that signifies deep engagement of the user. It means that a given pin presented to a user at a given time was relevant enough for the user to save that pin to one of their boards so that they can retrieve it later.</p>
<p>We find that PinSage consistently recommends pins that are more likely to be re-pinned by the user than the alternative methods. Depending on the particular setting, we observe 10-30% improve-ments in repin rate over the Annotation and Visual embedding based recommendations.<br /> 最后，我们还报告了生产A / B测试实验，该实验比较了PinSage与Pinterest上其他基于深度学习内容的推荐系统在家庭饲料建议任务上的表现。 我们通过观察用户参与的提升来评估性能。 感兴趣的度量标准是repin rate，用于衡量用户保存的家庭馈送建议的百分比。 用户将引脚保存到电路板是一种高价值的操作，表示用户的深度参与。 这意味着在给定时间呈现给用户的给定引脚足够相关，用户可以将该引脚保存到其中一个板上，以便以后可以检索它。</p>
<p>我们发现PinSage始终推荐比其他方法更有可能被用户重新固定的引脚。 根据特定设置，我们观察到基于Annotation和Visual embedding建议的重复率提高了10-30％。</p>
<h5><a id="45	Training_and_Inference_Runtime_Analysis_383"></a>4.5 Training and Inference Runtime Analysis</h5>
<p>One advantage of GCNs is that they can be made inductive [19]: at the inference (i.e., embedding generation) step, we are able to compute embeddings for items that were not in the training set. This allows us to train on a subgraph to obtain model parameters, and then make embed nodes that have not been observed during training. Also note that it is easy to compute embeddings of new nodes that get added into the graph over time. This means that recommendations can be made on the full (and constantly grow-ing) graph. Experiments on development data demonstrated that training on a subgraph containing 300 million items could achieve the best performance in terms of hit-rate (i.e., further increases in the training set size did not seem to help), reducing the runtime by a factor of 6 compared to training on the full graph.<br /> Table 3 shows the the effect of batch size of the minibatch SGD on the runtime of PinSage training procedure, using the mean-pooling-hard variant. For varying batch sizes, the table shows: (1) the computation time, in milliseconds, for each minibatch, when varying batch size; (2) the number of iterations needed for the model to converge; and (3) the total estimated time for the training proce-dure. Experiments show that a batch size of 2048 makes training most efficient.</p>
<p>When training the PinSage variant with importance pooling, another trade-off comes from choosing the size of neighborhood T . Table 3 shows the runtime and performance of PinSage when</p>
<p>T= 10, 20 and 50. We observe a diminishing return as T increases, and find that a two-layer GCN with neighborhood size 50 can best capture the neighborhood information of nodes, while still being computationally efficient.<br /> After training completes, due to the highly efficient MapReduce inference pipeline, the whole inference procedure to generate em-beddings for 3 billion items can finish in less than 24 hours.</p>
<p>GCN的一个优点是可以使它们具有归纳性[19]：在推理（即嵌入生成）步骤中，我们能够计算不在训练集中的项目的嵌入。这允许我们在子图上训练以获得模型参数，然后制作在训练期间未观察到的嵌入节点。另请注意，计算新节点的嵌入很容易随着时间的推移而添加到图形中。这意味着可以在完整（并且不断增长）的图表上进行推荐。对开发数据的实验表明，对包含3亿个项目的子图的训练可以在命中率方面达到最佳性能（即，训练集大小的进一步增加似乎没有帮助），将运行时间缩短了6倍与完整图表上的培训相比。<br /> 表3显示了使用mean-pooling-hard变体，miniatch SGD的批量大小对PinSage训练过程的运行时间的影响。对于不同的批量大小，该表显示：（1）当批量大小变化时，每个小批量的计算时间（以毫秒为单位）; （2）模型收敛所需的迭代次数; （3）培训程序的总预计时间。实验表明，2048的批量大小使培训效率最高。</p>
<p>在训练具有重要性汇集的PinSage变体时，另一个权衡取决于选择邻域T的大小。表3显示了PinSage的运行时和性能</p>
<p>T = 10,20和50.随着T的增加，我们观察到收益递减，并发现邻域大小为50的双层GCN可以最好地捕获节点的邻域信息，同时仍然具有计算效率。<br /> 培训完成后，由于高效的MapReduce推理管道，生成30亿件物品的整体推理程序可以在不到24小时内完成。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093352552.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 图5：不同算法推荐的Pinterest引脚示例。 左边的图像是查询引脚。 使用Visual em-beddings，Annotation embeddings，基于图形的Pixie和PinSage计算右侧的推荐项目。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093445155.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="Figure_6_tSNE_plot_of_item_embeddings_in_2_dimensions_405"></a>Figure 6: t-SNE plot of item embeddings in 2 dimensions.</h4>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709351932.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h6><a id="Table_3_Runtime_comparisons_for_different_batch_sizes_409"></a>Table 3: Runtime comparisons for different batch sizes.</h6>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093557957.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="Table_4_Performance_tradeoffs_for_importance_pooling_412"></a>Table 4: Performance tradeoffs for importance pooling.</h4>
<h4><a id="5	CONCLUSION_415"></a>5 CONCLUSION</h4>
<p>We proposed PinSage, a random-walk graph convolutional network (GCN). PinSage is a highly-scalable GCN algorithm capable of learn-ing embeddings for nodes in web-scale graphs containing billions of objects. In addition to new techniques that ensure scalability, we introduced the use of importance pooling and curriculum training that drastically improved embedding performance. We deployed PinSage at Pinterest and comprehensively evaluated the quality of the learned embeddings on a number of recommendation tasks, with offline metrics, user studies and A/B tests all demonstrating a substantial improvement in recommendation performance. Our work demonstrates the impact that graph convolutional methods can have in a production recommender system, and we believe that PinSage can be further extended in the future to tackle other graph representation learning problems at large scale, including knowledge graph reasoning and graph clustering.<br /> 我们提出了PinSage，一种随机游走图卷积网络（GCN）。 PinSage是一种高度可扩展的GCN算法，能够在包含数十亿个对象的Web级图形中学习节点的嵌入。 除了确保可扩展性的新技术之外，我们还引入了重要性池和课程训练，大大提高了嵌入性能。 我们在Pinterest部署了PinSage，并在一系列推荐任务中全面评估了学习嵌入的质量，离线指标，用户研究和A / B测试都证明了推荐性能的实质性改进。 我们的工作展示了图卷积方法在生产推荐系统中可能产生的影响，我们相信PinSage可以在未来进一步扩展，以解决大规模的其他图表表示学习问题，包括知识图推理和图聚类。</p>
<h2><a id="Acknowledgments_419"></a>Acknowledgments</h2>
<p>The authors acknowledge Raymond Hsu, Andrei Curelea and Ali Altaf for performing various A/B tests in production system, Jerry</p>
<p>Zitao Liu for providing data used by Pixie[14], and Vitaliy Kulikov for help in nearest neighbor query of the item embeddings.</p>
<h2><a id="REFERENCES_429"></a>REFERENCES</h2>
<p>[1]M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. 2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 (2016).</p>
<p>[2]A. Andoni and P. Indyk. 2006. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In FOCS.<br /> [3]T. Bansal, D. Belanger, and A. McCallum. 2016. Ask the GRU: Multi-task learning for deep text recommendations. In RecSys. ACM.<br /> [4]Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009. Curriculum learning. In ICML.<br /> [5]A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. 2003. Efficient query evaluation using a two-level retrieval process. In CIKM.<br /> [6]M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. 2017. Geometric deep learning: Going beyond euclidean data. IEEE Signal Processing Magazine 34, 4 (2017).</p>
<p>[7]J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. 2014. Spectral networks and locally connected networks on graphs. In ICLR.<br /> [8]J. Chen, T. Ma, and C. Xiao. 2018. FastGCN: Fast Learning with Graph Convolu-tional Networks via Importance Sampling. ICLR (2018).<br /> [9]P. Covington, J. Adams, and E. Sargin. 2016. Deep neural networks for youtube recommendations. In RecSys. ACM.<br /> [10]H. Dai, B. Dai, and L. Song. 2016. Discriminative Embeddings of Latent Variable Models for Structured Data. In ICML.<br /> [11]M. Defferrard, X. Bresson, and P. Vandergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral filtering. In NIPS.<br /> [12]A. Van den Oord, S. Dieleman, and B. Schrauwen. 2013. Deep content-based music recommendation. In NIPS.<br /> [13]D. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. 2015. Convolutional networks on graphs for learning molecular fingerprints. In NIPS.</p>
<p>[14]C. Eksombatchai, P. Jindal, J. Z. Liu, Y. Liu, R. Sharma, C. Sugnet, M. Ulrich, and</p>
<p>J.Leskovec. 2018. Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time. WWW (2018).<br /> [15]M. Gori, G. Monfardini, and F. Scarselli. 2005. A new model for learning in graph domains. In IEEE International Joint Conference on Neural Networks.</p>
<p>[16]P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch,</p>
<p>Y.Jia, and K. He. 2017. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. arXiv preprint arXiv:1706.02677 (2017).<br /> [17]A. Grover and J. Leskovec. 2016. node2vec: Scalable feature learning for networks. In KDD.<br /> [18]W. L. Hamilton, R. Ying, and J. Leskovec. 2017. Inductive Representation Learning on Large Graphs. In NIPS.<br /> [19]W. L. Hamilton, R. Ying, and J. Leskovec. 2017. Representation Learning on Graphs: Methods and Applications. IEEE Data Engineering Bulletin (2017).</p>
<p>[20]S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley. 2016. Molecular graph convolutions: moving beyond fingerprints. CAMD 30, 8.<br /> [21]T. N. Kipf and M. Welling. 2017. Semi-supervised classification with graph convolutional networks. In ICLR.<br /> [22]Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. 2015. Gated graph sequence neural networks. In ICLR.<br /> [23]T. Mikolov, I Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS.<br /> [24]F. Monti, M. M. Bronstein, and X. Bresson. 2017. Geometric matrix completion with recurrent multi-graph neural networks. In NIPS.<br /> [25]OpenMP Architecture Review Board. 2015. OpenMP Application Program Inter-face Version 4.5. (2015).<br /> [26]B. Perozzi, R. Al-Rfou, and S. Skiena. 2014. DeepWalk: Online learning of social representations. In KDD.<br /> [27]F. Scarselli, M. Gori, A.C. Tsoi, M. Hagenbuchner, and G. Monfardini. 2009. The graph neural network model. IEEE Transactions on Neural Networks 20, 1 (2009), 61–80.</p>
<p>[28]K. Simonyan and A. Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).<br /> [29]R. van den Berg, T. N. Kipf, and M. Welling. 2017. Graph Convolutional Matrix Completion. arXiv preprint arXiv:1706.02263 (2017).<br /> [30]J. You, R. Ying, X. Ren, W. L. Hamilton, and J. Leskovec. 2018. GraphRNN: Generating Realistic Graphs using Deep Auto-regressive Models. ICML (2018).<br /> [31]M. Zitnik, M. Agrawal, and J. Leskovec. 2018. Modeling polypharmacy side effects with graph convolutional networks. Bioinformatics (2018).</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>使用darknet识别点选验证码详细过程（附带源码）</title>
		<link>https://uzzz.org/article/1407.html</link>
				<pubDate>Thu, 21 Mar 2019 15:06:33 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Python]]></category>
		<category><![CDATA[机器学习]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1407.html</guid>
				<description><![CDATA[项目源码：https://github.com/nickliqian/darknet_captcha darknet_captcha 项目基于darknet开发了一系列的快速启动脚本，旨在让图像识别新手或者开发人员能够快速的启动一个目标检测（定位）的项目。 如果有没有讲清楚的地方，欢迎提issue和PR，希望能和大家共同完善！ 本项目分为两个部分： 提供两个目标检测（单分类和多分类点选验证码）的例子，你可以通过例子熟悉定位yolo3定位网络的使用方式 基于darknet提供一系列API，用于使用自己的数据进行目标检测模型的训练，并提供web server的代码 目录 项目结构 开始一个例子：单类型目标检测 第二个例子：多类型目标检测 训练自己的数据 Web服务 API文档 其他问题 使用阿里云OSS加速下载 GPU云推荐 CPU和GPU识别速度对比 报错解决办法 TODO 项目结构 项目分为darknet、extent、app三部分 darknet: 这部分是darknet项目源码，没有作任何改动。 extent: 扩展部分，包含生成配置、生成样本、训练、识别demo、api程序。 app: 每一个新的识别需求都以app区分，其中包含配置文件、样本和标签文件等。 开始一个例子：单类型目标检测 以点选验证码为例 darknet实际上给我们提供了一系列的深度学习算法，我们要做的就是使用比较简单的步骤来调用darknet训练我们的识别模型。 推荐使用的操作系统是ubuntu，遇到的坑会少很多。 如果使用windowns系统，需要先安装cygwin，便于编译darknet。（参考我的博客：安装cygwin） 下面的步骤都已经通过ubuntu16.04测试。 1.下载项目 git clone https://github.com/nickliqian/darknet_captcha.git 2.编译darknet 进入darknet_captcha目录，下载darknet项目，覆盖darknet目录： cd darknet_captcha git clone https://github.com/pjreddie/darknet.git 进入darknet目录，修改darknet/Makefile配置文件 cd darknet vim Makefile 如果使用GPU训练则下面的GPU=1 使用CPU训练则下面的GPU=0 GPU=1 CUDNN=0 OPENCV=0 OPENMP=0 DEBUG=0 然后使用make编译darknet： make 不建议使用CPU进行训练，因为使用CPU不管是训练还是预测，耗时都非常久。 如果你需要租用临时且价格低的GPU主机进行测试，后面介绍了一些推荐的GPU云服务。 如果在编译过程中会出错，可以在darknet的issue找一下解决办法，也可以发邮件找我要旧版本的darknet。 3.安装python3环境 使用pip执行下面的语句，并确保你的系统上已经安装了tk： pip install -r requirement.txt sudo apt-get install python3-tk 4.创建一个应用 进入根目录，运行下面的程序生成一个应用的基本配置： cd darknet_captcha python3 extend/create_app_config.py my_captcha 1 这里的类别默认生成classes_1，你可以修改类别名称； 打开app/my_captcha/my_captcha.names修改classes_1为主机想要的名称即可。 如何查看create_app_config.py的命令行参数解释？ 直接运行python create_app_config.py便可以在控制台查看，下面的程序也是如此。 如果你对darknet相关配置有一定的了解，可以直接打开文件修改参数的值，这里我们保持原样即可。 5.生成样本 生成样本使用另外一个项目 nickliqian/generate_click_captcha 这里我已经集成进去了，执行下面的命令生成样本和对应标签到指定应用中yolo规定的目录： python3 extend/generate_click_captcha.py my_captcha 运行python generate_click_captcha.py查看参数解释。 6.划分训练集和验证集 运行下面的程序，划分训练集和验证集，同时将标签的值转换为yolo认识的格式： python3 extend/output_label.py my_captcha 1 这里填写的种类需要与上面一致。 运行python output_label.py查看参数解释。 7.开始训练 到这里，我们要准备的东西还差一样，我们需要下载darknet提供的预训练模型放在darknet_captcha目录下： wget https://pjreddie.com/media/files/darknet53.conv.74 在darknet_captcha目录下，执行下面的命令开始训练： ./darknet/darknet detector train app/my_captcha/my_captcha.data app/my_captcha/my_captcha_train.yolov3.cfg darknet53.conv.74 训练过程中模型会每一百次迭代储存一次，储存在app/my_captcha/backup/下，可以进行查看。 8.识别效果 使用GTX 1060训练大概1.5小时，训练迭代到1000次，会有比较明显的效果。 我们找一张验证集的图片使用不同进度下的模型进行识别测试，执行下面的语句开始识别： python3 extend/rec.py my_captcha 100 这里的100是选择app/my_captcha/images_data/JPEGImages目录下的第一百张图片进行识别。 运行python rec.py查看参数解释。 迭代300次： 迭代800次： 迭代1000次： 迭代1200次： 9.图片切割 这部分比较简单，网上有很多示例代码，可以调用darknet_interface.cut_and_save方法把定位到的字符切割下来。 10.分类器 到分类这一步就比较容易了，可以使用darknet自带的分类器，也可以使用cnn_captcha一个使用卷积神经网络识别验证码的项目。 11.总结 我们识别点选验证码的大致流程如下： 搜集样本 打标签（标注坐标和字符） 训练定位器 检测位置，切割图片 训练分类器 使用定位器+分类器识别点选验证码上字符的位置和字符类别 第二个例子：多类型目标检测 步骤和上面基本上一致，直接把命令列出来： # 生成配置文件 python3 extend/create_app_config.py dummy_captcha 2 # 生成图片 python3 extend/generate_click_captcha.py dummy_captcha 500 True # 输出标签到txt python3 extend/output_label.py dummy_captcha 2 # 开始训练w ./darknet/darknet detector train app/dummy_captcha/dummy_captcha.data app/dummy_captcha/dummy_captcha_train.yolov3.cfg darknet53.conv.74 # 识别测试 python3 extend/rec.py dummy_captcha 100 训练自己的数据]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<p>项目源码：<a href="https://github.com/nickliqian/darknet_captcha" rel="nofollow" data-token="bbe2c7667b3860ca3d312bfc79eac8c5">https://github.com/nickliqian/darknet_captcha</a></p>
<h1><a id="darknet_captcha_1"></a>darknet_captcha</h1>
<p>项目基于darknet开发了一系列的快速启动脚本，旨在让图像识别新手或者开发人员能够快速的启动一个目标检测（定位）的项目。<br /> 如果有没有讲清楚的地方，欢迎提issue和PR，希望能和大家共同完善！</p>
<p>本项目分为两个部分：</p>
<ol>
<li>提供两个目标检测（<strong>单分类和多分类点选验证码</strong>）的例子，你可以通过例子熟悉定位yolo3定位网络的使用方式</li>
<li>基于darknet提供一系列API，用于使用<strong>自己的数据</strong>进行目标检测模型的训练，并提供web server的代码<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230022127.jpg" alt="在这里插入图片描述"><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230043287.jpg" alt="在这里插入图片描述"><br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230056211.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTE5ODQwNg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230103266.jpg" alt="在这里插入图片描述"></li>
</ol>
<h1><a id="_11"></a>目录</h1>
<ul>
<li><a href="#%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84" rel="nofollow" data-token="e3cbb68166d7c73612de3c0f2d8b45c6">项目结构</a></li>
<li><a href="#%E5%BC%80%E5%A7%8B%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90%EF%BC%9A%E5%8D%95%E7%B1%BB%E5%9E%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B" rel="nofollow" data-token="560cba2a0ed3ef673a7f8f820d8d4a1e">开始一个例子：单类型目标检测</a></li>
<li><a href="#%E7%AC%AC%E4%BA%8C%E4%B8%AA%E4%BE%8B%E5%AD%90%EF%BC%9A%E5%A4%9A%E7%B1%BB%E5%9E%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B" rel="nofollow" data-token="5dd41eef749a59fc22928cd1b18d037b">第二个例子：多类型目标检测</a></li>
<li><a href="#%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE" rel="nofollow" data-token="840083843945e74ca8091f34e29a5a50">训练自己的数据</a></li>
<li><a href="#web%E6%9C%8D%E5%8A%A1" rel="nofollow" data-token="f21cb879da123717190c98419efeebf1">Web服务</a></li>
<li><a href="#API%E6%96%87%E6%A1%A3" rel="nofollow" data-token="f87e5f2dde5b4e7171b2527339b00bac">API文档</a></li>
<li><a href="#%E5%85%B6%E4%BB%96%E9%97%AE%E9%A2%98" rel="nofollow" data-token="34df0cdf9eb93c33be13c1355c511280">其他问题</a>
<ul>
<li><a href="#%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91OSS%E5%8A%A0%E9%80%9F%E4%B8%8B%E8%BD%BD" rel="nofollow" data-token="661678172549864fcbc2e6f6281463c6">使用阿里云OSS加速下载</a></li>
<li><a href="#GPU%E4%BA%91%E6%8E%A8%E8%8D%90" rel="nofollow" data-token="4c5b24d02e2778a6aed6853623fd5195">GPU云推荐</a></li>
<li><a href="#CPU%E5%92%8CGPU%E8%AF%86%E5%88%AB%E9%80%9F%E5%BA%A6%E5%AF%B9%E6%AF%94" rel="nofollow" data-token="9ff9cbe779fc908b6dfe29bd8421dc09">CPU和GPU识别速度对比</a></li>
</ul>
</li>
<li><a href="#%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95" rel="nofollow" data-token="b8b4502a5899c83abe3e0bbf6b10e0ec">报错解决办法</a></li>
<li><a href="#TODO" rel="nofollow" data-token="da9cead761e3db3ac27337578bd89ec3">TODO</a></li>
</ul>
<h1><a id="_25"></a>项目结构</h1>
<p>项目分为<code>darknet、extent、app</code>三部分</p>
<ol>
<li>darknet: 这部分是<a href="https://github.com/pjreddie/darknet" rel="nofollow" data-token="993f4a46bae6e583288b4ebc86111b92">darknet</a>项目源码，没有作任何改动。</li>
<li>extent: 扩展部分，包含<strong>生成配置</strong>、<strong>生成样本</strong>、<strong>训练</strong>、<strong>识别demo</strong>、<strong>api程序</strong>。</li>
<li>app: 每一个新的识别需求都以app区分，其中包含配置文件、样本和标签文件等。</li>
</ol>
<h1><a id="_31"></a>开始一个例子：单类型目标检测</h1>
<p><strong>以点选验证码为例</strong><br /> darknet实际上给我们提供了一系列的深度学习算法，我们要做的就是使用比较简单的步骤来调用darknet训练我们的识别模型。</p>
<ul>
<li>推荐使用的操作系统是<code>ubuntu</code>，遇到的坑会少很多。</li>
<li>如果使用windowns系统，需要先安装<code>cygwin</code>，便于编译darknet。（参考我的博客：<a href="https://blog.csdn.net/weixin_39198406/article/details/83020632" rel="nofollow" data-token="d3dffad050d9d40d9ff00266a708da7e">安装cygwin</a>）</li>
</ul>
<p>下面的步骤都已经通过<code>ubuntu16.04</code>测试。</p>
<h4><a id="1_38"></a>1.下载项目</h4>
<pre><code>git clone https://github.com/nickliqian/darknet_captcha.git
</code></pre>
<h4><a id="2darknet_42"></a>2.编译darknet</h4>
<p>进入<code>darknet_captcha</code>目录，下载<code>darknet</code>项目，覆盖<code>darknet</code>目录：</p>
<pre><code>cd darknet_captcha
git clone https://github.com/pjreddie/darknet.git
</code></pre>
<p>进入<code>darknet</code>目录，修改<code>darknet/Makefile</code>配置文件</p>
<pre><code>cd darknet
vim Makefile
</code></pre>
<ul>
<li>如果使用GPU训练则下面的GPU=1</li>
<li>使用CPU训练则下面的GPU=0</li>
</ul>
<pre><code>GPU=1
CUDNN=0
OPENCV=0
OPENMP=0
DEBUG=0
</code></pre>
<p>然后使用<code>make</code>编译<code>darknet</code>：</p>
<pre><code>make
</code></pre>
<blockquote>
<p>不建议使用CPU进行训练，因为使用CPU不管是训练还是预测，耗时都非常久。<br /> 如果你需要租用临时且价格低的GPU主机进行测试，后面介绍了一些推荐的GPU云服务。<br /> 如果在编译过程中会出错，可以在darknet的issue找一下解决办法，也可以发邮件找我要旧版本的darknet。</p>
</blockquote>
<h4><a id="3python3_70"></a>3.安装python3环境</h4>
<p>使用pip执行下面的语句，并确保你的系统上已经安装了tk：</p>
<pre><code>pip install -r requirement.txt
sudo apt-get install python3-tk
</code></pre>
<h4><a id="4_77"></a>4.创建一个应用</h4>
<p>进入根目录，运行下面的程序生成一个应用的基本配置：</p>
<pre><code>cd darknet_captcha
python3 extend/create_app_config.py my_captcha 1
</code></pre>
<p>这里的类别默认生成<code>classes_1</code>，你可以修改类别名称；<br /> 打开<code>app/my_captcha/my_captcha.names</code>修改<code>classes_1</code>为主机想要的名称即可。</p>
<p>如何查看<code>create_app_config.py</code>的命令行参数解释？<br /> 直接运行<code>python create_app_config.py</code>便可以在控制台查看，下面的程序也是如此。</p>
<blockquote>
<p>如果你对darknet相关配置有一定的了解，可以直接打开文件修改参数的值，这里我们保持原样即可。</p>
</blockquote>
<h4><a id="5_91"></a>5.生成样本</h4>
<p>生成样本使用另外一个项目 <a href="https://github.com/nickliqian/generate_click_captcha" rel="nofollow" data-token="7b93780a4f9d60f2d43ef3b928cc2f56">nickliqian/generate_click_captcha</a><br /> 这里我已经集成进去了，执行下面的命令生成样本和对应标签到指定应用中<code>yolo</code>规定的目录：</p>
<pre><code>python3 extend/generate_click_captcha.py my_captcha
</code></pre>
<p>运行<code>python generate_click_captcha.py</code>查看参数解释。</p>
<h4><a id="6_99"></a>6.划分训练集和验证集</h4>
<p>运行下面的程序，划分训练集和验证集，同时将标签的值转换为<code>yolo</code>认识的格式：</p>
<pre><code>python3 extend/output_label.py my_captcha 1
</code></pre>
<p>这里填写的种类需要与上面一致。<br /> 运行<code>python output_label.py</code>查看参数解释。</p>
<h4><a id="7_107"></a>7.开始训练</h4>
<p>到这里，我们要准备的东西还差一样，我们需要下载darknet提供的预训练模型放在<code>darknet_captcha</code>目录下：</p>
<pre><code>wget https://pjreddie.com/media/files/darknet53.conv.74
</code></pre>
<p>在<code>darknet_captcha</code>目录下，执行下面的命令开始训练：</p>
<pre><code>./darknet/darknet detector train app/my_captcha/my_captcha.data app/my_captcha/my_captcha_train.yolov3.cfg darknet53.conv.74
</code></pre>
<p>训练过程中模型会每一百次迭代储存一次，储存在<code>app/my_captcha/backup/</code>下，可以进行查看。</p>
<h4><a id="8_118"></a>8.识别效果</h4>
<p>使用<code>GTX 1060</code>训练大概1.5小时，训练迭代到1000次，会有比较明显的效果。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230224316.jpg" alt="在这里插入图片描述"><br /> 我们找一张验证集的图片使用不同进度下的模型进行识别测试，执行下面的语句开始识别：</p>
<pre><code>python3 extend/rec.py my_captcha 100
</code></pre>
<p>这里的100是选择<code>app/my_captcha/images_data/JPEGImages</code>目录下的第一百张图片进行识别。<br /> 运行<code>python rec.py</code>查看参数解释。</p>
<p>迭代300次：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230246699.jpg" alt="在这里插入图片描述"><br /> 迭代800次：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230254808.jpg" alt="在这里插入图片描述"><br /> 迭代1000次：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230303211.jpg" alt="在这里插入图片描述"><br /> 迭代1200次：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230312599.jpg" alt="在这里插入图片描述"></p>
<h4><a id="9_137"></a>9.图片切割</h4>
<p>这部分比较简单，网上有很多示例代码，可以调用<code>darknet_interface.cut_and_save</code>方法把定位到的字符切割下来。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230421943.png" alt="在这里插入图片描述"></p>
<h4><a id="10_141"></a>10.分类器</h4>
<p>到分类这一步就比较容易了，可以使用darknet自带的分类器，也可以使用<a href="https://github.com/nickliqian/cnn_captcha" rel="nofollow" data-token="1beeb7f0521c4b0077accb44f90b0df3">cnn_captcha</a>一个使用卷积神经网络识别验证码的项目。</p>
<h4><a id="11_143"></a>11.总结</h4>
<p>我们识别点选验证码的大致流程如下：</p>
<ol>
<li>搜集样本</li>
<li>打标签（标注坐标和字符）</li>
<li>训练定位器</li>
<li>检测位置，切割图片</li>
<li>训练分类器</li>
<li>使用定位器+分类器识别点选验证码上字符的位置和字符类别</li>
</ol>
<h2><a id="_152"></a>第二个例子：多类型目标检测</h2>
<p>步骤和上面基本上一致，直接把命令列出来：</p>
<pre><code># 生成配置文件
python3 extend/create_app_config.py dummy_captcha 2
# 生成图片
python3 extend/generate_click_captcha.py dummy_captcha 500 True
# 输出标签到txt
python3 extend/output_label.py dummy_captcha 2
# 开始训练w
./darknet/darknet detector train app/dummy_captcha/dummy_captcha.data app/dummy_captcha/dummy_captcha_train.yolov3.cfg darknet53.conv.74
# 识别测试
python3 extend/rec.py dummy_captcha 100
</code></pre>
<h2><a id="_167"></a>训练自己的数据</h2>
<p>下面的过程教你如何训练自己数据。<br /> 假定我们要创建一个识别路上的车和人的应用，因此类别数量为2。<br /> 假定你现在有一些原始图片，首先你需要给这些图片打上标签，推荐使用<a href="https://github.com/tzutalin/labelImg" rel="nofollow" data-token="1e1d91f57353c3b2de80c67156a204c0">labelImg</a>进行打标工作。<br /> 使用教程可以自行谷歌，软件界面大致如下：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230438799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTE5ODQwNg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>给图片中的人和车分别打上person和car的标签，会生成xml标签文件。<br /> 接下来，我们创建一个应用，应用名称是<code>car</code>，类别为<code>2</code>类，同时生成一些配置文件：</p>
<pre><code>python3 extend/create_app_config.py car 2
</code></pre>
<p>然后把你的原始图片放到指定的路径<code>app/car/JPEGImages</code>，把xml标签文件放在<code>app/car/Annotations</code><br /> yolo训练的时候需要图片中目标的相对坐标，所以这里需要把xml的坐标计算为相对坐标的形式。<br /> 同时car.data中需要分别定义训练集和验证集的样本路径，这里会划分出训练集和验证集，同时生成两个txt文件记录其路径。</p>
<pre><code>python3 extend/output_label.py car 2
</code></pre>
<p>要提到的是，这里可以打开car.names，把里面的class_1和class_2分别修改为car和person，这里识别结果就会输出car和person。<br /> 然后就可以开始训练了：</p>
<pre><code>./darknet/darknet detector train app/car/car.data app/car/car_train.yolov3.cfg darknet53.conv.74
</code></pre>
<p>识别测试和上面也没有上面区别：</p>
<pre><code># 识别测试
python3 extend/rec.py car 100
</code></pre>
<h2><a id="web_195"></a>web服务</h2>
<p>启动web服务：</p>
<pre><code>python3 extend/web_server.py
</code></pre>
<p>启动前需要按需修改配置参数：</p>
<pre><code># 生成识别对象，需要配置参数
app_name = "car"  # 应用名称
config_file = "app/{}/{}_train.yolov3.cfg".format(app_name, app_name)  # 配置文件路径
model_file = "app/{}/backup/{}_train.backup".format(app_name, app_name)  # 模型路径
data_config_file = "app/{}/{}.data".format(app_name, app_name)  # 数据配置文件路径
dr = DarknetRecognize(
    config_file=config_file,
    model_file=model_file,
    data_config_file=data_config_file
)
save_path = "api_images"  # 保存图片的路径
</code></pre>
<p>使用下面的脚本<code>request_api.py</code>进行web服务的识别测试（注意修改图片路径）:</p>
<pre><code>python3 extend/request_api.py
</code></pre>
<p>返回响应，响应包含目标类别和中心点位置：</p>
<pre><code>接口响应: {
  "speed_time(ms)": 16469, 
  "time": "15472704635706885", 
  "value": [
    [
      "word", 
      0.9995613694190979, 
      [
        214.47508239746094, 
        105.97418212890625, 
        24.86412811279297, 
        33.40662384033203
      ]
    ],
    ...
}
</code></pre>
<h2><a id="API_237"></a>API文档</h2>
<p>暂无</p>
<h2><a id="_240"></a>其他问题</h2>
<h3><a id="OSS_241"></a>使用阿里云OSS加速下载</h3>
<p>如果你使用国外云主机进行训练，训练好的模型的下载速度确实是一个问题。<br /> 这里推荐使用阿里云oss，在云主机上把文件上传上去，然后使用oss下载下来。<br /> 配置秘钥：</p>
<pre><code># 从环境变量获取密钥
AccessKeyId = os.getenv("AccessKeyId")
AccessKeySecret = os.getenv("AccessKeySecret")
BucketName = os.getenv("BucketName")
</code></pre>
<p>上传图片：</p>
<pre><code>python3 extend/upload2oss.py app/my_captcha/images_data/JPEGImages/1_15463317590530567.jpg
python3 extend/upload2oss.py text.jpg
</code></pre>
<h3><a id="GPU_257"></a>GPU云推荐</h3>
<p>使用租用 vectordash GPU云主机，ssh连接集成了Nvidia深度学习环境的ubuntu16.04系统<br /> 包含以下工具或框架：</p>
<pre><code>CUDA 9.0, cuDNN, Tensorflow, PyTorch, Caffe, Keras
</code></pre>
<p>vectordash提供了一个客户端，具备远程连接、上传和下载文件、管理多个云主机等。<br /> 下面是几种显卡的租用价格：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019032123045338.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTE5ODQwNg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 创建实例后，面板会提供一个秘钥，输入秘钥后，就可以使用客户端操作了：</p>
<pre><code># 安装客户端
pip install vectordash --upgrade
# 登录
vectordash login
# 列出主机
vectordash list
# ssh登录
vectordash ssh &lt;instance_id&gt;
# 打开jupyter
vectordash jupyter &lt;instance_id&gt;
# 上传文件
vectordash push &lt;instance_id&gt; &lt;from_path&gt; &lt;to_path&gt;
# 下载文件
vectordash pull &lt;instance_id&gt; &lt;from_path&gt; &lt;to_path&gt;
</code></pre>
<p>由于vectordash主机在国外，所以上传和下载都很慢，建议临时租用一台阿里云竞价突发型实例（约7分钱一小时）作为中转使用。</p>
<h3><a id="CPUGPU_285"></a>CPU和GPU识别速度对比</h3>
<p>GTX 1060, 识别耗时1s</p>
<pre><code>[load model] speed time: 4.691879987716675s
[detect image - i] speed time: 1.002530813217163s
</code></pre>
<p>CPU, 识别耗时13s</p>
<pre><code>[load model] speed time: 3.313053846359253s
[detect image - i] speed time: 13.256595849990845s
</code></pre>
<h2><a id="_297"></a>报错解决办法</h2>
<ol>
<li>UnicodeEncodeError: ‘ascii’ codec can’t encode character ‘\U0001f621’ in posit<br /> <a href="https://blog.csdn.net/u011415481/article/details/80794567" rel="nofollow" data-token="39d2177d12495f5471ded8c7eb62aa46">参考链接</a></li>
<li>pip install, locale.Error: unsupported locale setting<br /> <a href="https://blog.csdn.net/qq_33232071/article/details/51108062" rel="nofollow" data-token="99aa19d8b859c366914ec7034be979f2">参考链接</a></li>
</ol>
<h2><a id="TODO_303"></a>TODO</h2>
<ol>
<li>支持多类别检测的识别和训练 <strong>Done</strong></li>
<li>WebServer API调用 <strong>Done</strong></li>
<li>分类器</li>
</ol></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>【论文阅读笔记】Learning to see in the dark</title>
		<link>https://uzzz.org/article/1395.html</link>
				<pubDate>Thu, 31 May 2018 03:15:59 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[机器学习]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1395.html</guid>
				<description><![CDATA[&#160; &#160; &#160;本文是CVPR2018论文，主要提出一种通过FCN方法将在黑暗环境中进行的拍摄还原的方法，实现让机器让机器“看破”黑暗。本文的主要创新点为： &#160; &#160; &#160; 1.提出了一个新的照片数据集，包含原始的short-exposure low-light图像，并附有long-exposure reference图像作为Groud truth，以往类似的研究使用的都是人工合成的图像； &#160; &#160; &#160; &#160;2.与以往方法使用相机拍摄出的sRGB图像进行复原不同，本文使用的是原始的传感器数据。 &#160; &#160; &#160; &#160;3.提出了一种端到端的学习方法，通过训练一个全卷积网络FCN来直接处理快速成像系统中的低亮度图像。结构如图： &#160; &#160; &#160; 本文最后提出了该模型待改进的几个地方： &#160; &#160; &#160; 1.数据集中目前不包含人和运动物体； &#160; &#160; &#160; 2.模型中的放大率amplification ratio是人工选择的，如果能根据图像自动选择，效果会更好。 &#160; &#160; &#160; 3.可以进行进一步的运行时优化，目前处理一幅照片的时间不能满足实时处理的时限要求。 &#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;. 下面的内容转载自：https://blog.csdn.net/linchunmian/article/details/80291921，个人认为是对本文比较好的一篇翻译： 整理下最近一篇论文的学习笔记。这是由UIUC的陈晨和Intel Labs的陈启峰、许佳、Vladlen Koltun 合作提出的一种在黑暗中也能快速、清晰的成像系统，让机器“看破”黑暗。以下是论文的主要部分。 摘要 在暗光条件下，受到低信噪比和低亮度的影响，图片的质量会受到很大的影响。此外，低曝光率的照片会出现很多噪声，而长曝光时间会让照片变得模糊、不真实。目前，很多关于去噪、去模糊、图像增强等技术的研究已被相继提出，但是在一些极端条件下，这些技术的作用就很有限了。为了发展基于学习的低亮度图像处理技术，本文提出了一种在黑暗中也能快速、清晰的成像系统，效果令人非常惊讶。此外，我们引入了一个数据集，包含有原始的低曝光率、低亮度图片，同时还有对应的长曝光率图像。利用该数据集，提出了一种端到端训练模式的全卷积网络结构，用于处理低亮度图像。该网络直接使用原始传感器数据，并替代了大量的传统图像处理流程。最终，实验结果表明这种网络结构在新数据集上能够表现出出色的性能，并在未来工作中有很大前途。 简介 任何的图像成像系统都存在噪声，但这很大地影响在弱光条件下图像的质量。高ISO 可以用于增加亮度，但它同时也会放大噪音。诸如缩放或直方图拉伸等图像后处理可以缓解这种噪声影响，但这并不能从根本上解决低信噪比 (SNR) 问题。在物理学上，这可以解释为在弱光条件下增加SNR，包括开放光圈，延长曝光时间以及使用闪光灯等，但这些也都有其自身的缺陷。例如，曝光时间的延长可能会引起相机抖动或物体运动模糊。 众所周知，暗光条件下的快速成像系统一直都是计算摄影界的一大挑战，也是一直以来开放性的研究领域。目前，许多关于图像去噪，去模糊和低光图像增强等技术相继提出，但这些技术通常假设这些在昏暗环境下捕获到的图像带有中等程度的噪音。相反，我们更感兴趣的是在极端低光条件下，如光照严重受限 (例如月光) 和短时间曝光 (理想情况下是视频率) 等条件下的图像成像系统。在这种情况下，传统相机的处理方式显然已不适用，图像必须根据原始的传感器数据来重建。 为此，本文提出了一种新的图像处理技术：通过一种数据驱动的方法来解决极端低光条件下快速成像系统的挑战。具体来说，我们训练深度神经网络来学习低光照条件下原始数据的图像处理技术，包括颜色转换，去马赛克，降噪和图像增强等。我们通过端对端的训练方式来避免放大噪声，还能表征这种环境下传统相机处理的累积误差。 据我们所知，现有用于处理低光图像的方法，在合成数据或真实的低光图像上测试都缺乏事实根据。此外，用于处理不同真实环境下的低光图像数据集也相当匮乏。因此，我们收集了一个在低光条件下快速曝光的原始图像数据集。每个低光图像都有对应的长曝光时间的高质量图像用于参考。在新的数据集上我们的方法表现出不出色的结果：将低光图像放大300倍，成功减少了图像中的噪音并正确实现了颜色转换。我们系统地分析方法中的关键要素并讨论未来的研究方向。 下图1展示了我们的设置。我们可以看到，在很高的ISO 8,000条件下，尽管使用全帧的索尼高光灵敏度相机，但相机仍会产生全黑的图像。在ISO 409,600条件下，图像仍会产生朦胧，嘈杂，颜色扭曲等现象。换而言之，即使是当前最先进的图像去噪技术也无法消除这种噪音，也无法解决颜色偏差问题。而我们提出的全卷积网络结构能够有效地克服这些问题。 图1卷积网络下的极端低光成像。黑暗的室内环境：:相机的照度 &#60;0.1 lux。Sony α7S II传感器曝光1/30秒。左图：ISO 8,000相机产生的图像。中间图：ISO 409,600相机产生的图像，图像受到噪声和颜色偏差的影响。右图：由我们的全卷积网络生生的图像。 数据集 (SID) 我们收集了一个新的数据集，用于原始低光图像的训练和基准测试。See-in-the-Dark(SID) 数据集包含5094张原始的短曝光图像，每张都有相应的长曝光时间的参考图像。值得注意的是，多张短曝光的图像可以对应于相同的长曝光时间的参考图像。例如，我们收集了短时间曝光图像用于评估去燥方法。序列中的每张图像都可视为一张独特的低光图像，这样包含真实世界伪像的图片能够更有利于模型的训练和培训测试。SID 数据集中长时间曝光的参考图像是424。 此外，我们的数据集包含了室内和室外图像。室外图像通常是在月光或街道照明条件下拍摄。在室外场景下，相机的亮度一般在0.2 lux 和5 lux 之间。室内图像通常更暗。在室内场景中的相机亮度一般在0.03 lux 和0.3 lux 之间。输入图像的曝光时间设置为1/30和1/10秒。相应的参考图像 (真实图像) 的曝光时间通常会延长100到300倍：即10至30秒。各数据集的具体情况如下表1中所示。 表1. SID 数据集包含5094个原始的短曝光率图像，每张图像都有一个长曝光的参考图像。图像由顶部和底部两台相机收集得到。表中的指标参数分别是(从左到右)：输入与参考图像之间的曝光时间率，滤波器阵列，输入图像的曝光时间以及在每种条件下的图像数量。 下图2显示了数据集中一部分的参考图像。在每种条件下，我们随机选择大约20％的图像是组成测试集，另外选定10％的数据用于模型验证。 图2 SID 数据库的实例。前两行是SID 数据集中室外的图像，底部两行是室内的图像。长曝光时间的参考图像 (地面实况) 显示在前面。短曝光的输入图像(基本上是黑色) 显示在背部。室外场景下相机的亮度一般在0.2到5 lux，而室内的相机亮度在0.03和0.3 lux 之间。 数据采集时，相机固定在三脚架上。我们用无反光镜相机来避免由于镜面拍打引起的振动。在每个场景中，相机设置 (如光圈，ISO，焦距和焦距) 进行了调整，以最大限度地提高参考图像(长曝光时间)的质量。此外，利用远程的智能手机 app 将曝光时间缩短一倍缩小后的曝光时间为100至300。该相机专门用于参考图像 (长曝光时间) 的拍摄，而没有触及短曝光的图像。我们收集了一系列短曝光的图像用于方法的比较和评估，以突出我们方法的优势。 虽然，数据集中的参考图像仍可能存在一些噪音，但感知质量足够高。我们目的是为了在光线不足的条件下产生在感知良好的图像，而不是彻底删除图像中所有噪音或最大化图像对比度。因此，这种参考图像的存在不会影响我们的方法评估。 方法 从成像传感器中得到原始数据后，传统图像处理过程会应用一系列模块，例如白平衡、去马赛克、去噪、增加图像锐度、 γ 矫正等等。而这些图像处理模块只在某些特定相机中才有。一些研究提出使用局部线性、可学习的L3 过滤器来模拟现代成像系统中复杂的非线性流程，但是这些方法都无法成功解决在低光条件中快速成像的问题，也无法解决极低的SNR 问题。此外，通过智能手机相机拍摄的照片，利用bursting imaging成像方法，结合多张图像也可以生成效果较好的图像，但是这种方法的复杂程度较高。 因此，我们提出了的端到端的学习方法，即训练一个全卷积网络FCN 来直接处理快速成像系统中的低亮度图像。纯粹的FCN 结构可以有效地代表许多图像处理算法。受此启发，我们调查并研究这种方法在极端低光条件下成像系统的应用。相比于传统图像处理方法使用的sRGB 图像，在这里我们使用原始传感器数据。下图3展示了我们所提出的方法结构。 图3 我们提出的图像处理方法 对于 Bayer 数组，我们将输入打包为四个通道并在每个通道上将空间分辨率降低一半。对于X-Trans 数组(图中未显示出)，原始数据以6×6排列块组成;我们通过交换相邻通道元素的方法将36个通道的数组打包成9个通道。此外，我们消除黑色像素并按照期望的倍数缩放数据(例如，x100或x300)。将处理后数据作为 FCN 模型的输入，输出是一个带12通道的图像，其空间分辨率只有输入的一半。 我们将两个标准的 FCN 结构作为我们模型的核心架构：用于快速图像处理的多尺度上下文聚合网络 (CAN) 和U-net 网络。影响我们模型选择的另一个因素是内存消耗：在 GPU 中，我们选择的模型结构可以处理全分辨率的图像(例如，在4240×2832或6000×4000分辨率)。同时，我们避免使用完全连接结构及模型集成方式。我们的默认架构是 U-net。 放大比率决定了模型的亮度输出。在我们的方法中，放大比率设置在外部指定并作为输入提供给模型，这类似于相机中的 ISO 设置。下图4显示了不同放大比率的影响。用户可以通过设置不同的放大率来调整输出图像的亮度。在测试时间，我们的方法能够抑制盲点噪声并实现颜色转换，并在sRGB 空间网络直接处理图像，得到网络的输出。 图4 SID 数据集中放大系数对室内图像 (Sony x100子集) 的影响。类似于摄像机中的ISO设置，这里的放大系数是作为外部输入提供给我们的模型。更高的放大倍数可以产生更明亮的图像。我们在我们的方法中引入了不同的放大因子，并展示了模型的输出图像。 模型训练 我们使用 L1 损失和 Adam 优化器，从零开始训练我们的网络。在训练期间，网络输入是原始的短曝光图像，在 sRGB 空间中的真实数据是相应的长曝光时间图像(由一个原始图像处理库 libraw 处理过得参考图像)。我们为每台相机训练一个网络，并将原始图像和参考图像之间曝光时间的倍数差作为我们的放大因子(例如，x100，x250，或x300)。在每次训练迭代中，我们随机裁剪一个512×512的补丁用于训练并利用翻转、旋转等操作来随机增强数据。初始学习率设定为0.0001，在2000次迭代后学习率降为0.00001，训练一共进行4000次迭代。 实验结果分析 首先，与传统方法的对比，我们提出的方法具有放大的功能。如下图5,6,7所示，传统的图像处理方法在极端低光条件下容易受到严重的噪声影响，导致生成图像颜色失真。我们提出的方法能够有效地抑制图像噪声，生成色彩均衡、逼真的图像。此外，由于在数据集中对齐数据序列，我们的方法也更优于 post-hocdenoising、brust denoising 等图像去燥方法 图5 (a) 由富士胶片 X-T2 相机拍摄的夜间图像，ISO 800，光圈f / 7.1，曝光时间1/30s，相机亮度约为1 lux。(b) 传统的图像处理方法不能有效处理原始数据中的噪声和颜色偏差。(c) 基于相同的数据，我们方法处理的结果。 图6 将 SID 数据集训练好的模型应用于用 iPhone 6s智能手机拍摄的原始低光图像。(a) iPhone 6s 在夜间所拍摄的原始图像，ISO 400，光圈f/2.2，曝光时间0. 05s。经传统的图像处理方法处理后的图像及缩放到相匹配的亮度的参考图像。(b)我们提出的方法处理后的结果 图7 Sony]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<p>&nbsp; &nbsp; &nbsp;本文是CVPR2018论文，主要提出一种通过FCN方法将在黑暗环境中进行的拍摄还原的方法，实现让机器<span style="color:#666666;">让机器“看破”黑暗。本文的主要创新点为：</span></p>
</p>
<p><span style="color:#666666;">&nbsp; &nbsp; &nbsp; 1.提出了一个新的照片数据集，包含原始的</span><span style="color:rgb(102,102,102);">short-exposure low-light图像，并附有</span><span style="color:rgb(102,102,102);">long-exposure reference图像作为Groud truth，以往类似的研究使用的都是人工合成的图像；</span></p>
<p><span style="color:rgb(102,102,102);">&nbsp; &nbsp; &nbsp; &nbsp;2.与以往方法使用相机拍摄出的sRGB图像进行复原不同，本文使用的是原始的传感器数据。</span></p>
<p><span style="color:rgb(102,102,102);">&nbsp; &nbsp; &nbsp; &nbsp;3.<span style="color:#666666;">提出了一种端到端的学习方法，通过训练一个全卷积网络FCN</span>来直接处理快速成像系统中的低亮度图像。结构如图：</span></p>
<p><span style="color:rgb(102,102,102);"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180531110855773" alt=""><br /></span></p>
<p><span style="color:rgb(102,102,102);">&nbsp; &nbsp; &nbsp; 本文最后提出了该模型待改进的几个地方：</span></p>
<p><span style="color:rgb(102,102,102);">&nbsp; &nbsp; &nbsp; 1.数据集中目前不包含人和运动物体；</span></p>
<p><span style="color:rgb(102,102,102);">&nbsp; &nbsp; &nbsp; 2.模型中的放大率amplification ratio是人工选择的，如果能根据图像自动选择，效果会更好。</span></p>
<p><span style="color:rgb(102,102,102);">&nbsp; &nbsp; &nbsp; 3.可以进行进一步的运行时优化，目前处理一幅照片的时间不能满足实时处理的时限要求。</span></p>
<p><span style="color:#666666;">&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;.</span></p>
</p>
<p><span style="color:#666666;">下面的内容转载自：</span><a href="https://blog.csdn.net/linchunmian/article/details/80291921" rel="nofollow" data-token="41360b0a5552a1423ce411fbc87bbcd0">https://blog.csdn.net/linchunmian/article/details/80291921</a>，个人认为是对本文比较好的一篇翻译：</p>
</p>
<p style="background-color:rgb(255,255,255);"><span style="color:rgb(102,102,102);">整理下最近一篇论文的学习笔记。这是由UIUC的陈晨和Intel Labs的陈启峰、许佳、Vladlen Koltun 合作提出的一种在黑暗中也能快速、清晰的成像系统，让机器“看破”黑暗。以下是论文的主要部分。</span></p>
<p style="background-color:rgb(255,255,255);">
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);"><span style="font-weight:700;">摘要</span></span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">在暗光条件下，受到低信噪比和低亮度的影响，图片的质量会受到很大的影响。此外，低曝光率的照片会出现很多噪声，而长曝光时间会让照片变得模糊、不真实。目前，很多关于去噪、去模糊、图像增强等技术的研究已被相继提出，但是在一些极端条件下，这些技术的作用就很有限了。为了发展基于学习的低亮度图像处理技术，本文提出了一种在黑暗中也能快速、清晰的成像系统，效果令人非常惊讶。此外，我们引入了一个数据集，包含有原始的低曝光率、低亮度图片，同时还有对应的长曝光率图像。利用该数据集，提出了一种端到端训练模式的全卷积网络结构，用于处理低亮度图像。该网络直接使用原始传感器数据，并替代了大量的传统图像处理流程。最终，实验结果表明这种网络结构在新数据集上能够表现出出色的性能，并在未来工作中有很大前途。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);"><span style="font-weight:700;">简介</span></span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">任何的图像成像系统都存在噪声，但这很大地影响在弱光条件下图像的质量。高ISO 可以用于增加亮度，但它同时也会放大噪音。诸如缩放或直方图拉伸等图像后处理可以缓解这种噪声影响，但这并不能从根本上解决低信噪比 (SNR) 问题。在物理学上，这可以解释为在弱光条件下增加SNR，包括开放光圈，延长曝光时间以及使用闪光灯等，但这些也都有其自身的缺陷。例如，曝光时间的延长可能会引起相机抖动或物体运动模糊。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">众所周知，暗光条件下的快速成像系统一直都是计算摄影界的一大挑战，也是一直以来开放性的研究领域。目前，许多关于图像去噪，去模糊和低光图像增强等技术相继提出，但这些技术通常假设这些在昏暗环境下捕获到的图像带有中等程度的噪音。相反，我们更感兴趣的是在极端低光条件下，如光照严重受限 (例如月光) 和短时间曝光 (理想情况下是视频率) 等条件下的图像成像系统。在这种情况下，传统相机的处理方式显然已不适用，图像必须根据原始的传感器数据来重建。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">为此，本文提出了一种新的图像处理技术：通过一种数据驱动的方法来解决极端低光条件下快速成像系统的挑战。具体来说，我们训练深度神经网络来学习低光照条件下原始数据的图像处理技术，包括颜色转换，去马赛克，降噪和图像增强等。我们通过端对端的训练方式来避免放大噪声，还能表征这种环境下传统相机处理的累积误差。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">据我们所知，现有用于处理低光图像的方法，在合成数据或真实的低光图像上测试都缺乏事实根据。此外，用于处理不同真实环境下的低光图像数据集也相当匮乏。因此，我们收集了一个在低光条件下快速曝光的原始图像数据集。每个低光图像都有对应的长曝光时间的高质量图像用于参考。在新的数据集上我们的方法表现出不出色的结果：将低光图像放大300倍，成功减少了图像中的噪音并正确实现了颜色转换。我们系统地分析方法中的关键要素并讨论未来的研究方向。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">下图1展示了我们的设置。我们可以看到，在很高的ISO 8,000条件下，尽管使用全帧的索尼高光灵敏度相机，但相机仍会产生全黑的图像。在ISO 409,600条件下，图像仍会产生朦胧，嘈杂，颜色扭曲等现象。换而言之，即使是当前最先进的图像去噪技术也无法消除这种噪音，也无法解决颜色偏差问题。而我们提出的全卷积网络结构能够有效地克服这些问题。</span></p>
<p align="left" style="text-align:center;background:rgb(255,255,255);"><span style="color:rgb(102,102,102);"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153439309?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br /></span></p>
<pre style="padding-right:0px;padding-left:0px;font-size:14px;line-height:22px;text-align:center;background:rgb(255,255,255);"><span style="font-size:12px;"><span style="color:rgb(102,102,102);">图1卷积网络下的极端低光成像。黑暗的室内环境：:相机的照度 &lt;0.1 lux。Sony α7S II传感器曝光1/30秒。</span>左图：<span style="color:rgb(102,102,102);">ISO 8,000</span><span style="color:rgb(102,102,102);">相机产生的图像。中间图：</span><span style="color:rgb(102,102,102);">ISO 409,600</span><span style="color:rgb(102,102,102);">相机产生的图像，图像受到噪声和颜色偏差的影响。右图：由我们的全卷积网络生生的图像。</span></span></pre>
<p align="left" style="background:rgb(255,255,255);"><span style="font-weight:700;"><span style="color:rgb(102,102,102);">数据集 (SID)</span></span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">我们收集了一个新的数据集，用于原始低光图像的训练和基准测试。See-in-the-Dark(SID) 数据集包含5094张原始的短曝光图像，每张都有相应的长曝光时间的参考图像。值得注意的是，多张短曝光的图像可以对应于相同的长曝光时间的参考图像。例如，我们收集了短时间曝光图像用于评估去燥方法。序列中的每张图像都可视为一张独特的低光图像，这样包含真实世界伪像的图片能够更有利于模型的训练和培训测试。SID 数据集中长时间曝光的参考图像是424。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">此外，我们的数据集包含了室内和室外图像。室外图像通常是在月光或街道照明条件下拍摄。在室外场景下，相机的亮度一般在0.2 lux 和5 lux 之间。室内图像通常更暗。在室内场景中的相机亮度一般在0.03 lux 和0.3 lux 之间。输入图像的曝光时间设置为1/30和1/10秒。相应的参考图像 (真实图像) 的曝光时间通常会延长100到300倍：即10至30秒。各数据集的具体情况如下表1中所示。</span></p>
<p align="left" style="text-align:center;background:rgb(255,255,255);"><span style="color:rgb(102,102,102);"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153544352?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br /></span></p>
<p align="center" style="background:rgb(255,255,255);"><span style="font-size:12px;color:rgb(102,102,102);">表1. SID 数据集包含5094个原始的短曝光率图像，每张图像都有一个长曝光的参考图像。图像由顶部和底部两台相机收集得到。表中的指标参数分别是(从左到右)：输入与参考图像之间的曝光时间率，滤波器阵列，输入图像的曝光时间以及在每种条件下的图像数量。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">下图2显示了数据集中一部分的参考图像。在每种条件下，我们随机选择大约20％的图像是组成测试集，另外选定10％的数据用于模型验证。</span></p>
<p align="left" style="text-align:center;background:rgb(255,255,255);"><span style="color:rgb(102,102,102);"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153622727?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br /></span></p>
<p align="center" style="background:rgb(255,255,255);"><span style="font-size:12px;color:rgb(102,102,102);">图2 SID 数据库的实例。前两行是SID 数据集中室外的图像，底部两行是室内的图像。长曝光时间的参考图像 (地面实况) 显示在前面。短曝光的输入图像(基本上是黑色) 显示在背部。室外场景下相机的亮度一般在0.2到5 lux，而室内的相机亮度在0.03和0.3 lux 之间。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">数据采集时，相机固定在三脚架上。我们用无反光镜相机来避免由于镜面拍打引起的振动。在每个场景中，相机设置 (如光圈，ISO，焦距和焦距) 进行了调整，以最大限度地提高参考图像(长曝光时间)的质量。此外，利用远程的智能手机 app 将曝光时间缩短一倍缩小后的曝光时间为100至300。该相机专门用于参考图像 (长曝光时间) 的拍摄，而没有触及短曝光的图像。我们收集了一系列短曝光的图像用于方法的比较和评估，以突出我们方法的优势。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">虽然，数据集中的参考图像仍可能存在一些噪音，但感知质量足够高。我们目的是为了在光线不足的条件下产生在感知良好的图像，而不是彻底删除图像中所有噪音或最大化图像对比度。因此，这种参考图像的存在不会影响我们的方法评估。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="font-weight:700;"><span style="color:rgb(102,102,102);">方法</span></span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">从成像传感器中得到原始数据后，传统图像处理过程会应用一系列模块，例如白平衡、去马赛克、去噪、增加图像锐度、 γ 矫正等等。而这些图像处理模块只在某些特定相机中才有。一些研究提出使用局部线性、可学习的L3 过滤器来模拟现代成像系统中复杂的非线性流程，但是这些方法都无法成功解决在低光条件中快速成像的问题，也无法解决极低的SNR 问题。此外，通过智能手机相机拍摄的照片，利用bursting imaging成像方法，结合多张图像也可以生成效果较好的图像，但是这种方法的复杂程度较高。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">因此，我们提出了的端到端的学习方法，即训练一个全卷积网络FCN 来直接处理快速成像系统中的低亮度图像。纯粹的FCN 结构可以有效地代表许多图像处理算法。受此启发，我们调查并研究这种方法在极端低光条件下成像系统的应用。相比于传统图像处理方法使用的sRGB 图像，在这里我们使用原始传感器数据。下图3展示了我们所提出的方法结构。</span></p>
<p align="left" style="text-align:center;background:rgb(255,255,255);"><span style="color:rgb(102,102,102);"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153658118?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br /></span></p>
<p align="center" style="background:rgb(255,255,255);"><span style="font-size:12px;color:rgb(102,102,102);">图3 我们提出的图像处理方法</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">对于 Bayer 数组，我们将输入打包为四个通道并在每个通道上将空间分辨率降低一半。对于X-Trans 数组(图中未显示出)，原始数据以6×6排列块组成;我们通过交换相邻通道元素的方法将36个通道的数组打包成9个通道。此外，我们消除黑色像素并按照期望的倍数缩放数据(例如，x100或x300)。将处理后数据作为 FCN 模型的输入，输出是一个带12通道的图像，其空间分辨率只有输入的一半。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">我们将两个标准的 FCN 结构作为我们模型的核心架构：用于快速图像处理的多尺度上下文聚合网络 (CAN) 和U-net 网络。影响我们模型选择的另一个因素是内存消耗：在 GPU 中，我们选择的模型结构可以处理全分辨率的图像(例如，在4240×2832或6000×4000分辨率)。同时，我们避免使用完全连接结构及模型集成方式。我们的默认架构是 U-net。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">放大比率决定了模型的亮度输出。在我们的方法中，放大比率设置在外部指定并作为输入提供给模型，这类似于相机中的 ISO 设置。下图4显示了不同放大比率的影响。用户可以通过设置不同的放大率来调整输出图像的亮度。在测试时间，我们的方法能够抑制盲点噪声并实现颜色转换，并在sRGB 空间网络直接处理图像，得到网络的输出。</span></p>
<p align="left" style="text-align:center;background:rgb(255,255,255);"><span style="color:rgb(102,102,102);"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153724857?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br /></span></p>
<p align="center" style="background:rgb(255,255,255);"><span style="font-size:12px;color:rgb(102,102,102);">图4 SID 数据集中放大系数对室内图像 (Sony x100子集) 的影响。类似于摄像机中的ISO设置，这里的放大系数是作为外部输入提供给我们的模型。更高的放大倍数可以产生更明亮的图像。我们在我们的方法中引入了不同的放大因子，并展示了模型的输出图像。</span></p>
<p style="background:rgb(255,255,255);"><span style="font-weight:700;"><span style="color:rgb(102,102,102);">模型训练</span></span></p>
<p style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">我们使用 L1 损失和 Adam 优化器，从零开始训练我们的网络。在训练期间，网络输入是原始的短曝光图像，在 sRGB 空间中的真实数据是相应的长曝光时间图像(由一个原始图像处理库 libraw 处理过得参考图像)。我们为每台相机训练一个网络，并将原始图像和参考图像之间曝光时间的倍数差作为我们的放大因子(例如，x100，x250，或x300)。在每次训练迭代中，我们随机裁剪一个512×512的补丁用于训练并利用翻转、旋转等操作来随机增强数据。初始学习率设定为0.0001，在2000次迭代后学习率降为0.00001，训练一共进行4000次迭代。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="font-weight:700;"><span style="color:rgb(102,102,102);">实验结果分析</span></span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">首先，与传统方法的对比，我们提出的方法具有放大的功能。如下图5,6,7所示，传统的图像处理方法在极端低光条件下容易受到严重的噪声影响，导致生成图像颜色失真。我们提出的方法能够有效地抑制图像噪声，生成色彩均衡、逼真的图像。此外，由于在数据集中对齐数据序列，我们的方法也更优于 post-hocdenoising、brust denoising 等图像去燥方法</span></p>
<p align="center" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153750652?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></span></p>
<p align="center" style="background:rgb(255,255,255);"><span style="font-size:12px;color:rgb(102,102,102);">图5 (a) 由富士胶片 X-T2 相机拍摄的夜间图像，ISO 800，光圈f / 7.1，曝光时间1/30s，相机亮度约为1 lux。(b) 传统的图像处理方法不能有效处理原始数据中的噪声和颜色偏差。(c) 基于相同的数据，我们方法处理的结果。</span></p>
<p align="center" style="background:rgb(255,255,255);"><span style="font-size:12px;color:rgb(102,102,102);"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153810914?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br /></span></p>
<p align="center" style="background:rgb(255,255,255);"><span style="font-size:12px;color:rgb(102,102,102);">图6 将 SID 数据集训练好的模型应用于用 iPhone 6s智能手机拍摄的原始低光图像。(a) iPhone 6s 在夜间所拍摄的原始图像，ISO 400，光圈f/2.2，曝光时间0. 05s。经传统的图像处理方法处理后的图像及缩放到相匹配的亮度的参考图像。</span><span style="color:rgb(102,102,102);font-size:12px;">(b)</span><span style="color:rgb(102,102,102);font-size:12px;">我们提出的方法处理后的结果</span></p>
<p align="center" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);font-size:12px;"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153840238?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br /></span></p>
<p align="center" style="background:rgb(255,255,255);"><span style="font-size:12px;color:rgb(102,102,102);">图7 Sony x300拍摄的图像。(a) 由传统图像处理方法处理的低光图像及其线性缩放的结果。(b) 同样用传统方法，并通过 BM3D 去噪方法处理后的结果。 (c) 我们提出的方法处理后的结果。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">此外，我们还进行了一系列的控制实验，来分析方法中各组分对模型性能的影响，包括模型结构，输入的颜色空间，损失函数，数据排列，图像后处理等因素。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="font-weight:700;"><span style="color:rgb(102,102,102);">结语</span></span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">由于图像低光子数和低信噪比的影响，快速低光成像系统是一个艰巨的挑战。黑暗中快速成像系统更是被认为是一种不切实际、与传统的信号处理相悖的技术。在本文中，我们创建了一个黑暗中的图像数据集 (SID) 以支持数据驱动方法的研究。利用 SID 数据集，我们提出一种基于 FCN 模型结构，通过端到端训练，改善了传统的处理低光图像的方法。实验结果表明我们的方法能够成功抑制噪声并正确地实现颜色转换，表现出不错的性能，并展现了不错的研究前景。未来的工作我们可以进一步研究低光成像网络的泛化能力。此外，对于模型的性能优化也是值得研究的一个热点方向。我们还将在未来的工作中进一步改善图像质量，如通过系统优化网络架构和训练程序。我们希望SID 数据集和我们的实验结果可以支持并刺激未来该领域的研究。</span></p>
<p align="left" style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">原文链接：</span><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.01934" rel="nofollow noopener noreferrer" data-token="10354b74d4deff9e5f377b2ee3e5bfbf">arxiv.org/abs/1805.01934</a></p>
<p style="background:rgb(255,255,255);"><span style="color:rgb(102,102,102);">项目地址：<a href="https://link.zhihu.com/?target=http%3A//web.engr.illinois.edu/~cchen156/SID.html" rel="nofollow noopener noreferrer" data-token="0d2205017e859bc5979af930e9998c38">web.engr.illinois.edu/~cchen156/SID.html</a></span></p>
<p style="background-color:rgb(255,255,255);"><span style="color:rgb(102,102,102);">GitHub地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/cchen156/Learning-to-See-in-the-Dark" rel="nofollow noopener noreferrer" data-token="1e92eca0f5ba1ab05c99e2ebc61066ef">github.com/cchen156/Learning-to-See-in-the-Dark</a></span></p>
<p style="background-color:rgb(255,255,255);"><span style="color:rgb(102,102,102);">以上是这篇论文解读的内容。欢迎指点补充。</span></p>
<p>
  <span style="color:#666666;"></span></p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>CVPR 2018 挑战赛</title>
		<link>https://uzzz.org/article/1530.html</link>
				<pubDate>Sun, 11 Mar 2018 04:42:34 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[挑战赛]]></category>
		<category><![CDATA[机器学习]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1530.html</guid>
				<description><![CDATA[﻿﻿ 6.18-22 日，CVPR 2018 将在美国盐湖城举办。 所有workshop，见如下网址http://cvpr2018.thecvf.com/program/workshops，有时间的同学参考下。 Date &#38; Time Location Workshop Organizer(s) Monday, June 18, 2018 TBA First International Workshop on Disguised Faces in the Wild Nalini Ratha Monday, June 18, 2018 TBA Fine-grained Instructional Video undERstanding (FIVER) Jason Corso Monday, June 18, 2018 TBA Low-Power Image Recognition Challenge Yung-Hsiang Lu Monday, June 18, 2018 TBA NVIDIA AI City Challenge Milind Naphade Monday, June 18, 2018 TBA DeepGlobe: A Challenge for Parsing the Earth through Satellite Images Ilke Demir Monday, June 18, 2018 TBA VQA Challenge and Visual Dialog Workshop Yash Goyal Monday, June 18, 2018 TBA Visual Understanding of Humans in Crowd Scene and the 2nd Look Into Person (LIP) Challenge Xiaodan Liang, Jian Zhao Monday, June 18, 2018 TBA Language and Vision Andrei Barbu Monday, June 18, 2018 TBA Robust Vision Challenge Andreas Geiger Monday, June 18, 2018 TBA Workshop and Challenge on Learnt Image Compression George Toderici Monday, June 18, 2018 (PM) TBA Large-Scale Landmark Recognition: A Challenge Bohyung Han Monday, June 18, 2018 TBA The DAVIS Challenge on Video Object Segmentation 2018 Jordi Pont-Tuset Monday, June]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<div>
   ﻿﻿
  </div>
</p>
<p><span style="font:400 16px/32px 'microsoft yahei';text-align:left;color:rgb(49,66,78);text-transform:none;text-indent:0px;letter-spacing:normal;text-decoration:none;word-spacing:0px;">6.18-22 日，CVPR 2018 将在美国盐湖城举办。</span></p>
</p>
</p>
<p><span style="font:400 16px/32px 'microsoft yahei';text-align:left;color:rgb(49,66,78);text-transform:none;text-indent:0px;letter-spacing:normal;text-decoration:none;word-spacing:0px;">所有workshop，见如下网址<a href="http://cvpr2018.thecvf.com/program/workshops" rel="nofollow" data-token="784f017767aa3efbeb809c8f548e330f">http://cvpr2018.thecvf.com/program/workshops</a>，有时间的同学参考下。</span></p>
<p>  <span style="font:400 16px/32px 'microsoft yahei';text-align:left;color:rgb(49,66,78);text-transform:none;text-indent:0px;letter-spacing:normal;text-decoration:none;word-spacing:0px;"></span></p>
<table class="table">
<tbody>
<tr class="blue-bottom">
<th>
<h4>Date &amp; Time</h4>
</th>
<th>
<h4>Location</h4>
</th>
<th>
<h4>Workshop</h4>
</th>
<th>
<h4>Organizer(s)</h4>
</th>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://iab-rubric.org/dfw.html" rel="nofollow" data-token="e36985b57c99e68362285f0b104f9f19"><u><span style="color:#0066cc;">First International Workshop on Disguised Faces in the Wild</span></u></a></td>
<td><a>Nalini Ratha</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://fiver.eecs.umich.edu/" rel="nofollow" data-token="fb2554dbe827cafdf118a46869ff861f"><u><span style="color:#0066cc;">Fine-grained Instructional Video undERstanding (FIVER)</span></u></a></td>
<td><a>Jason Corso</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="https://rebootingcomputing.ieee.org/lpirc" rel="nofollow" data-token="5701b8020da7dbf157bdf9d9e3d5af04"><u><span style="color:#0066cc;">Low-Power Image Recognition Challenge</span></u></a></td>
<td><a>Yung-Hsiang Lu</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://www.aicitychallenge.org/" rel="nofollow" data-token="757500cbd5cce3551b9f90fdcc3ee645"><u><span style="color:#0066cc;">NVIDIA AI City Challenge</span></u></a></td>
<td><a>Milind Naphade</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://deepglobe.org/" rel="nofollow" data-token="eb849b2d80137fe81b6acbc5874bcfa8"><u><span style="color:#0066cc;">DeepGlobe: A Challenge for Parsing the Earth through Satellite Images</span></u></a></td>
<td><a>Ilke Demir</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://visualqa.org/workshop" rel="nofollow" data-token="3f779ef1175a508d982f06433df2558b"><u><span style="color:#0066cc;">VQA Challenge and Visual Dialog Workshop</span></u></a></td>
<td><a>Yash Goyal</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="https://vuhcs.github.io/" rel="nofollow" data-token="64f7758c8546b87f136400a0f3f5c4cf"><u><span style="color:#0066cc;">Visual Understanding of Humans in Crowd Scene and the 2nd Look Into Person (LIP) Challenge</span></u></a></td>
<td><a>Xiaodan Liang, Jian Zhao</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://languageandvision.com/" rel="nofollow" data-token="ee22cdb8f17647e298d6051abde95966"><u><span style="color:#0066cc;">Language and Vision</span></u></a></td>
<td><a>Andrei Barbu</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://www.robustvision.net/" rel="nofollow" data-token="744ae101d24bb26ac002a577d5da50ba"><u><span style="color:#0066cc;">Robust Vision Challenge</span></u></a></td>
<td><a>Andreas Geiger </a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://compression.cc/" rel="nofollow" data-token="aff5ccf0420201c4af4a69e9999bc509"><u><span style="color:#0066cc;">Workshop and Challenge on Learnt Image Compression</span></u></a></td>
<td><a>George Toderici</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018 <strong>(PM)</strong></td>
<td>TBA</td>
<td><a href="https://landmarkscvprw18.github.io/" rel="nofollow" data-token="4cf083b59d7f5e4f3486777eea653735"><u><span style="color:#0066cc;">Large-Scale Landmark Recognition: A Challenge</span></u></a></td>
<td><a>Bohyung Han</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://davischallenge.org/challenge2018/" rel="nofollow" data-token="810468c573e52182b24ec0b550310ae1"><u><span style="color:#0066cc;">The DAVIS Challenge on Video Object Segmentation 2018</span></u></a></td>
<td><a>Jordi Pont-Tuset</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://ug2challenge.org/" rel="nofollow" data-token="0bdb59a4f25fa09f893eaa4345ebece3"><u><span style="color:#0066cc;">Bridging the Gap between Computational Photography and Visual Recognition: the UG<sup><span style="font-size:12px;">2</span></sup> Prize Challenge</span></u></a></td>
<td><a>Walter J. Scheirer</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://visualslam.ai/" rel="nofollow" data-token="2e513afce1683f9f93c672f263783264"><u><span style="color:#0066cc;">1st International Workshop on Deep Learning for Visual SLAM</span></u></a></td>
<td><a>Ronald Clark</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="https://diffcvml2018.wordpress.com/" rel="nofollow" data-token="cf50e6a4967d070ad39271810513da1d"><u><span style="color:#0066cc;">4th International Workshop on Diff-CVML: Differential Geometry in Computer Vision and Machine Learning</span></u></a></td>
<td><a>Anuj Srivastava</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://vislab.ucr.edu/Biometrics2018/index.php" rel="nofollow" data-token="75292b9d7a008bb64c39dce59e1f7d8e"><u><span style="color:#0066cc;">CVPR 2018 Workshop on Biometrics </span></u></a></td>
<td><a>Ajay Kumar</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="https://embeddedvisionworkshop.wordpress.com/" rel="nofollow" data-token="90042e995e447bc742d74402479f459b"><u><span style="color:#0066cc;">Embedded Vision Workshop</span></u></a></td>
<td><a>Zoran Nikolic</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://www.vision.ee.ethz.ch/en/ntire18/" rel="nofollow" data-token="1f10388baaaab85aba00c68c2ebf1444"><u><span style="color:#0066cc;">3rd New Trends in Image Restoration and Enhancement workshop and challenges</span></u></a></td>
<td><a>Radu Timofte</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018 &amp; Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="http://www.wad.ai/" rel="nofollow" data-token="2066051782b66b376e2906d7e3b0c3b0"><u><span style="color:#0066cc;">Workshop on Autonomous Driving</span></u></a></td>
<td><a>Ruigang Yang, Jose Alvarez and Fisher Yu</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://www.deep-vision.net/" rel="nofollow" data-token="4ebd4e451d3c740ec007fba0fb198b8b"><u><span style="color:#0066cc;">Deep-Vision Workshop</span></u></a></td>
<td><a>Jose Alvarez</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://www.vision.ee.ethz.ch/webvision/workshop/2018/index.html" rel="nofollow" data-token="76144fc64bbd731a33dddc8e5f87bf5e"><u><span style="color:#0066cc;">The 2nd CVPR Workshop on Visual Understanding by Learning from Web Data</span></u></a></td>
<td><a>Wen LI</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="https://project.inria.fr/humans2018/" rel="nofollow" data-token="4c12f2614dcdb7288d46b575d6770179"><u><span style="color:#0066cc;">1st International Workshop on HUman pose, Motion, Activities aNd Shape in 3D</span></u></a></td>
<td><a>Gregory Rogez</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://www.visionmeetscognition.org/" rel="nofollow" data-token="1b77684173541de3eb99b17d8ada016f"><u><span style="color:#0066cc;">The 4th CVPR Workshop on Vision Meets Cognition: Functionality, Physics, Intentionality and Causality</span></u></a></td>
<td><a>Yixin Zhu</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td>Brave New Ideas for Video Understanding</td>
<td><a>Lorenzo Torresani and Efstratios Gavves</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="https://sites.google.com/site/cvprmcv18/" rel="nofollow" data-token="0fdae15a83b3419a7c5d958a890d6834"><u><span style="color:#0066cc;">The Fifth Workshop on Medical Computer Vision and Health Informatics</span></u></a></td>
<td><a>Le Lu</a></td>
</tr>
<tr>
<td>Monday, June 18, 2018</td>
<td>TBA</td>
<td><a href="http://www.otcbvs.com/" rel="nofollow" data-token="fdeb4a7d2b0a78089a854d0824f35f03"><u><span style="color:#0066cc;">14th Workshop on Perception Beyond the Visible Spectrum (PBVS 2018)</span></u></a></td>
<td><a>Riad I. Hammoud</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="http://www.es.ele.tue.nl/cvpm18/" rel="nofollow" data-token="fdf26e94a18260b7d9b5915c2c501fe5"><u><span style="color:#0066cc;">The 1st International Workshop on Computer Vision for Physiological Measurement</span></u></a></td>
<td><a>Wenjin Wang</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="http://www.viametoolkit.org/cvpr-2018-workshop/" rel="nofollow" data-token="0ffd3aa013c48b198f15d19642490d50"><u><span style="color:#0066cc;">Automated Analysis of Marine Video for Environmental Monitoring</span></u></a></td>
<td><a>Anthony Hoogs</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018 <strong>(AM)</strong></td>
<td>TBA</td>
<td><a href="http://cmla2018.actlabs.org/" rel="nofollow" data-token="548665ece0da9fc46a88bdcf004f585a"><u><span style="color:#0066cc;">The 2nd Workshop on Computational Models for Learning Systems and Educational Assessment</span></u></a></td>
<td><a>Saad Khan</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="http://trajnet.stanford.edu/workshops/2018/" rel="nofollow" data-token="ea22d2c27df18a0ee83d0bec8071d0f1"><u><span style="color:#0066cc;">The First Workshop on Joint Detection, Tracking, and Prediction in the wild</span></u></a></td>
<td><a>Amir Abbas Sadeghian</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="http://www.cis.rit.edu/~glpci/vocvalc2018/" rel="nofollow" data-token="8e83ec4e9b021eb5600fc39828e21680"><u><span style="color:#0066cc;">2nd International Workshop on Visual Odometry and Computer Vision Applications Based on Location Clues</span></u></a></td>
<td><a>Guoyu Lu</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="http://www.activity-net.org/challenges/2018/index.html" rel="nofollow" data-token="dfaf968c43cc9c0d70205c7bb5f52f18"><u><span style="color:#0066cc;">ActivityNet Large Scale Activity Recognition Challenge 2018</span></u></a></td>
<td><a>Bernard Ghanem</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="http://vision.soic.indiana.edu/bright-and-dark-workshop-2018/" rel="nofollow" data-token="d2c03c271c5521fdea43173992987f6b"><u><span style="color:#0066cc;">The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security</span></u></a></td>
<td><a>David Crandall </a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="https://sites.google.com/view/ecv2018/" rel="nofollow" data-token="27465857c91ee0d95ea6cf5923185b41"><u><span style="color:#0066cc;">Efficient Deep Learning for Computer Vision</span></u></a></td>
<td><a>Peter Vajda, Ramesh Sarukkai</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="http://www.vap.aau.dk/cvsports/" rel="nofollow" data-token="45c4ad73695c5bbc6c770d53835edeca"><u><span style="color:#0066cc;">4th IEEE International Workshop on Computer Vision in Sports (CVsports)</span></u></a></td>
<td><a>Thomas B. Moeslund</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="http://wisionlab.cs.wisc.edu/ccd2018/" rel="nofollow" data-token="447a724980980dcb2d7fdd602e38d046"><u><span style="color:#0066cc;">7th IEEE International Workshop on Computational Cameras and Displays</span></u></a></td>
<td><a>Mohit Gupta</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="https://wicvworkshop.github.io/" rel="nofollow" data-token="b0bcd548124b7df8916f191cf6cad108"><u><span style="color:#0066cc;">Women in Computer Vision</span></u></a></td>
<td><a>Dena Bazazian</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="http://people.cs.pitt.edu/~kovashka/ads_workshop/" rel="nofollow" data-token="559826d2332772677492284911c58031"><u><span style="color:#0066cc;">Towards Automatic Understanding of Visual Advertisements</span></u></a></td>
<td><a>Adriana Kovashka</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="https://bridgesto3d.github.io/" rel="nofollow" data-token="515834ab0ca462f7d3648a3c14ffc490"><u><span style="color:#0066cc;">Bridges to 3D Vision Workshop in Conjunction with CVPR 2018</span></u></a></td>
<td><a>David Ford Fouhey</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="https://sites.google.com/site/mbcc2018w/" rel="nofollow" data-token="4cd27458c2a2b5d03db62f4b866b082d"><u><span style="color:#0066cc;">Mutual benefits of cognitive and computer vision: How can we use one to understand the other?</span></u></a></td>
<td><a>Ali Borji</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="https://web.northeastern.edu/smilelab/AMFG2018/" rel="nofollow" data-token="57e7c29d6e691ccdc7b96d90ed7c6fe8"><u><span style="color:#0066cc;">The 8th IEEE International Workshop on Analysis and Modeling of Faces and Gestures </span></u></a></td>
<td><a>Ming Shao</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="https://vbsd2018.github.io/" rel="nofollow" data-token="9bb0bc18ea401775f5256d342515ae04"><u><span style="color:#0066cc;">Vision with Biased or Scarce Data</span></u></a></td>
<td><a>Jan Ernst</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="http://www.beyond-supervised.ai/" rel="nofollow" data-token="642833722308041a7c49d727015da77d"><u><span style="color:#0066cc;">Beyond Supervised Learning</span></u></a></td>
<td><a>Amir R. Zamir</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="https://cvmi2018.github.io/home/dates.html" rel="nofollow" data-token="f218bc3262cfb1f8eebdbafb8df39fc4"><u><span style="color:#0066cc;">The 3rd IEEE International Workshop on Computer Vision for Microscopy Image Analysis (CVMI)</span></u></a></td>
<td><a>Mei Chen</a></td>
</tr>
<tr>
<td>Friday, June 22, 2018</td>
<td>TBA</td>
<td><a href="http://fgvc.org/FGVC5/" rel="nofollow" data-token="aadf5ea7935b46b4360f4acb6fc0b091"><u><span style="color:#0066cc;">The Fifth Workshop on Fine-Grained Visual Categorization</span></u></a></td>
<td><a>Ryan Farrell</a></td>
</tr>
</tbody>
</table></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>百度开源移动端深度学习框架mobile-deep-learning</title>
		<link>https://uzzz.org/article/1696.html</link>
				<pubDate>Tue, 26 Sep 2017 05:07:41 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[人工智能应用]]></category>
		<category><![CDATA[机器学习]]></category>
		<category><![CDATA[深度学习]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1696.html</guid>
				<description><![CDATA[2017 年 9 月 25 日，百度在 GitHub 开源了移动端深度学习框架 mobile-deep-learning（MDL）的全部代码以及脚本，希望这个项目在社区的带动下能够更好地发展。 写在前面 深度学习技术已经在互联网的诸多方向产生影响，每天科技新闻中关于深度学习和神经网络的讨论越来越多。深度学习技术在近两年飞速发展，各种互联网产品都争相应用深度学习技术，产品对深度学习的引入也将进一步影响人们的生活。随着移动设备的广泛使用，在移动互联网产品应用深度学习和神经网络技术已经成为必然趋势。 与深度学习紧密联系在一起的图像技术同样在业界广泛应用。传统计算机视觉和深度学习的结合使图像技术得以快速发展。 GitHub 地址：https://github.com/baidu/mobile-deep-learning 移动端深度学习技术应用 百度应用案例 在移动端应用深度学习技术比较典型的就是 CNN（Convolutional Neural Network）技术，即常被人提起的卷积神经网络。mobile-deep-learning（MDL）是一个基于卷积神经网络实现的移动端框架。 MDL 在移动端主要有哪些应用场景呢？比较常见的如分辨出一张图片中的物体是什么，也就是 分类；或者识别出一张图片中的物体在哪、有多大，也就是 主体识别。 下面这个 App 叫拾相，可以在 Android 平台的应用宝中找到。它可以自动为用户将照片分类，对于拥有大量照片的用户来讲，这个功能很有吸引力。 另外，在手机百度搜索框右侧可以打开图像搜索，打开图像搜索后的界面效果如下图。当用户在通用垂直类别下开启自动拍开关（图中下方标注）时，手停稳它就会自动找到物体进行框选，并无需拍照直接发起图像搜索。整个过程可以给用户带来流畅的体验，无需用户手动拍照。图片中的框体应用的就是典型的深度学习主体识别技术，使用的就是 mobile-deep-learning（MDL）框架。MDL 目前在手机百度中稳定运行了多个版本，经过数次迭代后可靠性大幅提升。 业界其他案例 互联网行业在移动端应用神经网络的案例已经越来越多。 目前的流派主要有两种，其一是完全在客户端运行神经网络，这种方式的优点显而易见，那就是不需要经过网络，如果能保证运行速度，用户体验会非常流畅。如果能保证移动端高效运行神经网络，可以使用户感觉不到加载过程。使用完全脱离互联网网络在移动端运算神经网络的 App 已经举例，如前述拾相和手机百度中的图像搜索。 其二是另一种，运算神经网络过程依赖互联网网络，客户端只负责 UI 展示。在客户端神经网络落地之前，绝大部分 App 都使用了这种运算在服务端、展示在客户端的方式。这种方式的优点是实现相对容易，开发成本更低。 为了更好理解上述两种神经网络的实现方法，下面展示两个识别植物花卉的例子，分别用到了识花和形色两个 App。这两款 App 都使用了典型分类方法，都可以在 iOS 平台的 App Store 中找到。下图是一张莲花图片，这张图片使用识花和形色两个 App 分类都能得到较好的分类结果。你可以尝试安装这两款 App 并根据使用效果来判断它们分别使用了上述哪一种方法。 识花 近一年来涌现出很多花卉识别的 App。微软「识花」是微软亚洲研究院推出的一款用于识别花卉的 App，用户可以在拍摄后选择花卉，App 会给出该类花卉的相关信息。精准的花卉分类是其对外宣传的一大亮点。 形色 这款「形色」App，只需要对准植物 (花、草、树) 拍照，就能快速给出植物的名字，还有不少有趣的植物知识，如这个植物还有什么别名、植物的花语、相关古诗词、植物文化、趣味故事以及养护方法，看完收获不少。 移动端应用深度学习的难点 一直以来由于技术门槛和硬件条件的限制，在移动端应用深度学习的成功案例不多。传统移动端 UI 工程师在编写神经网络代码时，可以查阅到的移动端深度学习资料也很少。另一方面，时下的互联网竞争又颇为激烈，先入咸阳者王，可以率先将深度学习技术在移动端应用起来，就可以更早地把握时代先机。 移动端设备的运算能力相对 PC 端非常弱小。由于移动端的 CPU 要将功耗指标维持在很低的水平，制约了性能指标的提升。在 App 中做神经网络运算会使 CPU 运算量猛增。如何协调好用户功耗指标和性能指标就显得至关重要。 百度图像搜索客户端团队在 2015 年底就开始针对移动端深度学习技术应用进行攻关。最终，挑战性问题被逐一解决，现今相关代码已经在很多 App 上运行，这些 App 有日 PV 亿级的产品，也有创业期的产品。 在移动端应用深度学习技术本已困难重重，而在手机百度这种量级的产品上应用，更是要面对各种机型和硬件、手机百度的指标要求。如何使神经网络技术稳定高效运转是最大的考验。拆解问题就是移动端团队面对的首要问题。我们简单总结后发现移动端与服务器端进行对比更容易呈现问题和难点，继而在服务器端和客户端做了以下深度学习技术应用对比。 难点与服务器端对比内存内存：服务器端弱限制 – 移动端内存有限耗电量耗电量：服务器端不限制 – 移动端严格限制依赖库体积依赖库体积：服务器端不限制 – 移动端强限制模型体积模型大小：服务器端常规模型体积 200M 起 – 移动端不宜超过 10M性能性能：服务器端强大 GPU BOX – 移动端 CPU 和 GPU 在开发过程中，团队逐步解决掉以上困难，形成了现在的 MDL 深度学习框架。为了让更多移动端工程师能够快速用轮子、专注业务，百度开源了全部相关代码，社区也欢迎任何人加入到造轮子的开发过程中来。 MDL 框架设计 设计思路 作为一款移动端深度学习框架，我们充分考虑到移动应用自身及运行环境的特点，在速度、体积、资源占用率等方面提出了严格的要求，因为其中任何一项指标对用户体验都有重大影响。 同时，可扩展性、鲁棒性、兼容性也是我们设计之初就考虑到了的。为了保证框架的可扩展性，我们对 layer 层进行了抽象，方便框架使用者根据模型的需要，自定义实现特定类型的层，我们期望 MDL 通过添加不同类型的层实现对更多网络模型的支持，而不需要改动其他位置的代码；为了保证框架的鲁棒性，MDL 通过反射机制，将 C++ 底层异常抛到应用层，应用层通过捕获异常对异常进行相应处理，如通过日志收集异常信息、保证软件可持续优化等；目前行业内各种深度学习训练框架种类繁多，而 MDL 不支持模型训练能力，为了保证框架的兼容性，我们提供 Caffe 模型转 MDL 的工具脚本，使用者通过一行命令就可以完成模型的转换及量化过程，后续我们会陆续支持 PaddlePaddle、TensorFlow&#160;等模型转 MDL，兼容更多种类的模型。 总体架构 MDL 框架的总体架构设计图如下： MDL 框架主要包括模型转换模块（MDL Converter）、模型加载模块（Loader）、网络管理模块（Net）、矩阵运算模块（Gemmers）及供 Android 端调用的 JNI 接口层（JNI Interfaces）。其中，模型转换模块主要负责将 Caffe 模型转为 MDL 模型，同时支持将 32bit 浮点型参数量化为 8bit 参数，从而极大地压缩模型体积；模型加载模块主要完成模型的反量化及加载校验、网络注册等过程，网络管理模块主要负责网络中各层 Layer 的初始化及管理工作；MDL 提供了供 Android 端调用的 JNI 接口层，开发者可以通过调用 JNI 接口轻松完成加载及预测过程。 MDL 定位简单可用 MDL 开源项目在实施之初就已经有了清晰定位。在设备繁杂且性能较低的移动端平台技术研发过程中，能够为新颖的深度学习技术找到合适场景并应用到自己的产品中是非常吸引人的。但如果让每个移动端工程师在应用深度学习过程中都要重新写一次全部神经网络的实现，会增加较大成本。MDL 的定位是简单地使用和部署神经网络，如果使用基本功能则不需要进行过多配置和修改，甚至连机器学习库的编译过程都不需要，只需要关注具体业务实现、如何使用即可。 与此同时 MDL 简单清晰的代码结构也可以作为学习材料，为刚刚接触深度学习的研发工程师提供参考。因为我们在支持手机平台交叉编译同时，也支持 Linux 和 Mac 的 x86 平台编译，在调整深度学习代码的同时可以直接在工作电脑上编译运行，而不需要部署到 arm 平台。所需要的只是简单的几行代码，具体可以查阅 MDL 的 GitHub Readme。 # https://github.com/baidu/mobile-deep-learning #]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <img alt="大数据" class="aligncenter size-full wp-image-97548" height="436" src="http://www.99lt.com/uploads/allimg/170926/1050362033-0.jpg" width="640" style="width:500px;visibility:visible;border:0px;vertical-align:middle;display:block;text-align:center;"></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 2017 年 9 月 25 日，百度在 GitHub 开源了移动端<a href="http://www.99lt.com/ai/deep-learning/" rel="nofollow" style="text-decoration:none;color:rgb(11,59,140);" data-token="570869b1d39301b697bf1770145fba62"><span>深度学习</span></a>框架 mobile-deep-learning（MDL）的全部代码以及脚本，希望这个项目在社区的带动下能够更好地发展。</p>
<h2 style="font-size:18px;font-weight:normal;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;line-height:18px;color:rgb(68,68,68);border-bottom-width:1px;border-bottom-style:solid;border-bottom-color:rgb(238,238,238);"> <span>写在前面</span></h2>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 深度学习技术已经在互联网的诸多方向产生影响，每天科技新闻中关于深度学习和<a href="http://www.99lt.com/ai/neural-network/" rel="nofollow" style="text-decoration:none;color:rgb(11,59,140);" data-token="844ab9043c908f34f8cb20d912b1207a"><span>神经网络</span></a>的讨论越来越多。深度学习技术在近两年飞速发展，各种互联网产品都争相应用深度学习技术，产品对深度学习的引入也将进一步影响人们的生活。随着移动设备的广泛使用，在移动互联网产品应用深度学习和<a href="http://www.99lt.com/ai/neural-network/" rel="nofollow" style="text-decoration:none;color:rgb(11,59,140);" data-token="844ab9043c908f34f8cb20d912b1207a"><span>神经网络</span></a>技术已经成为必然趋势。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 与深度学习紧密联系在一起的图像技术同样在业界广泛应用。传统计算机视觉和深度学习的结合使图像技术得以快速发展。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> GitHub 地址：https://github.com/baidu/mobile-deep-learning</p>
<h2 style="font-size:18px;font-weight:normal;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;line-height:18px;color:rgb(68,68,68);border-bottom-width:1px;border-bottom-style:solid;border-bottom-color:rgb(238,238,238);"> <span>移动端深度学习技术应用</span></h2>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <span>百度应用案例</span></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 在移动端应用深度学习技术比较典型的就是 CNN（Convolutional Neural Network）技术，即常被人提起的卷积神经网络。mobile-deep-learning（MDL）是一个基于卷积神经网络实现的移动端框架。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> MDL 在移动端主要有哪些应用场景呢？比较常见的如分辨出一张图片中的物体是什么，也就是 分类；或者识别出一张图片中的物体在哪、有多大，也就是 主体识别。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 下面这个 App 叫拾相，可以在 Android 平台的应用宝中找到。它可以自动为用户将照片分类，对于拥有大量照片的用户来讲，这个功能很有吸引力。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <img alt="大数据" class="aligncenter size-full wp-image-97544" height="1138" src="http://www.99lt.com/uploads/allimg/170926/1050361338-1.jpg" width="640" style="width:500px;visibility:visible;border:0px;vertical-align:middle;display:block;text-align:center;"></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 另外，在手机百度搜索框右侧可以打开图像搜索，打开图像搜索后的界面效果如下图。当用户在通用垂直类别下开启自动拍开关（图中下方标注）时，手停稳它就会自动找到物体进行框选，并无需拍照直接发起图像搜索。整个过程可以给用户带来流畅的体验，无需用户手动拍照。图片中的框体应用的就是典型的深度学习主体识别技术，使用的就是 mobile-deep-learning（MDL）框架。MDL 目前在手机百度中稳定运行了多个版本，经过数次迭代后可靠性大幅提升。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <img alt="大数据" class="aligncenter size-full wp-image-97545" height="236" src="http://www.99lt.com/uploads/allimg/170926/1050363243-2.jpg" width="640" style="width:500px;visibility:visible;border:0px;vertical-align:middle;display:block;text-align:center;"></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <img alt="大数据" class="aligncenter size-full wp-image-97546" height="1144" src="http://www.99lt.com/uploads/allimg/170926/1050363016-3.jpg" width="640" style="width:500px;visibility:visible;border:0px;vertical-align:middle;display:block;text-align:center;"></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <span>业界其他案例</span></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 互联网行业在移动端应用神经网络的案例已经越来越多。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 目前的流派主要有两种，其一是完全在客户端运行神经网络，这种方式的优点显而易见，那就是不需要经过网络，如果能保证运行速度，用户体验会非常流畅。如果能保证移动端高效运行神经网络，可以使用户感觉不到加载过程。使用完全脱离互联网网络在移动端运算神经网络的 App 已经举例，如前述拾相和手机百度中的图像搜索。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 其二是另一种，运算神经网络过程依赖互联网网络，客户端只负责 UI 展示。在客户端神经网络落地之前，绝大部分 App 都使用了这种运算在服务端、展示在客户端的方式。这种方式的优点是实现相对容易，开发成本更低。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 为了更好理解上述两种神经网络的实现方法，下面展示两个识别植物花卉的例子，分别用到了识花和形色两个 App。这两款 App 都使用了典型分类方法，都可以在 iOS 平台的 App Store 中找到。下图是一张莲花图片，这张图片使用识花和形色两个 App 分类都能得到较好的分类结果。你可以尝试安装这两款 App 并根据使用效果来判断它们分别使用了上述哪一种方法。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <img alt="大数据" class="aligncenter size-full wp-image-97540" height="555" src="http://www.99lt.com/uploads/allimg/170926/10503A1c-4.jpg" width="640" style="width:500px;visibility:visible;border:0px;vertical-align:middle;display:block;text-align:center;"></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <span>识花</span></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 近一年来涌现出很多花卉识别的 App。微软「识花」是微软亚洲研究院推出的一款用于识别花卉的 App，用户可以在拍摄后选择花卉，App 会给出该类花卉的相关信息。精准的花卉分类是其对外宣传的一大亮点。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <img alt="大数据" class="aligncenter size-full wp-image-97541" height="1139" src="http://www.99lt.com/uploads/allimg/170926/10503635S-5.jpg" width="640" style="width:500px;visibility:visible;border:0px;vertical-align:middle;display:block;text-align:center;"></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <span>形色</span></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 这款「形色」App，只需要对准植物 (花、草、树) 拍照，就能快速给出植物的名字，还有不少有趣的植物知识，如这个植物还有什么别名、植物的花语、相关古诗词、植物文化、趣味故事以及养护方法，看完收获不少。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <img alt="大数据" class="aligncenter size-full wp-image-97542" height="1139" src="http://www.99lt.com/uploads/allimg/170926/1050361U1-6.jpg" width="640" style="width:500px;visibility:visible;border:0px;vertical-align:middle;display:block;text-align:center;"></p>
<h2 style="font-size:18px;font-weight:normal;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;line-height:18px;color:rgb(68,68,68);border-bottom-width:1px;border-bottom-style:solid;border-bottom-color:rgb(238,238,238);"> <span>移动端应用深度学习的难点</span></h2>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 一直以来由于技术门槛和硬件条件的限制，在移动端应用深度学习的成功案例不多。传统移动端 UI 工程师在编写神经网络代码时，可以查阅到的移动端深度学习资料也很少。另一方面，时下的互联网竞争又颇为激烈，先入咸阳者王，可以率先将深度学习技术在移动端应用起来，就可以更早地把握时代先机。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 移动端设备的运算能力相对 PC 端非常弱小。由于移动端的 CPU 要将功耗指标维持在很低的水平，制约了性能指标的提升。在 App 中做神经网络运算会使 CPU 运算量猛增。如何协调好用户功耗指标和性能指标就显得至关重要。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 百度图像搜索客户端团队在 2015 年底就开始针对移动端深度学习技术应用进行攻关。最终，挑战性问题被逐一解决，现今相关代码已经在很多 App 上运行，这些 App 有日 PV 亿级的产品，也有创业期的产品。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 在移动端应用深度学习技术本已困难重重，而在手机百度这种量级的产品上应用，更是要面对各种机型和硬件、手机百度的指标要求。如何使神经网络技术稳定高效运转是最大的考验。拆解问题就是移动端团队面对的首要问题。我们简单总结后发现移动端与服务器端进行对比更容易呈现问题和难点，继而在服务器端和客户端做了以下深度学习技术应用对比。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 难点与服务器端对比内存内存：服务器端弱限制 – 移动端内存有限耗电量耗电量：服务器端不限制 – 移动端严格限制依赖库体积依赖库体积：服务器端不限制 – 移动端强限制模型体积模型大小：服务器端常规模型体积 200M 起 – 移动端不宜超过 10M性能性能：服务器端强大 GPU BOX – 移动端 CPU 和 GPU</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 在开发过程中，团队逐步解决掉以上困难，形成了现在的 MDL 深度学习框架。为了让更多移动端工程师能够快速用轮子、专注业务，百度开源了全部相关代码，社区也欢迎任何人加入到造轮子的开发过程中来。</p>
<h2 style="font-size:18px;font-weight:normal;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;line-height:18px;color:rgb(68,68,68);border-bottom-width:1px;border-bottom-style:solid;border-bottom-color:rgb(238,238,238);"> <span>MDL 框架设计</span></h2>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <span>设计思路</span></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 作为一款移动端深度学习框架，我们充分考虑到移动应用自身及运行环境的特点，在速度、体积、资源占用率等方面提出了严格的要求，因为其中任何一项指标对用户体验都有重大影响。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 同时，可扩展性、鲁棒性、兼容性也是我们设计之初就考虑到了的。为了保证框架的可扩展性，我们对 layer 层进行了抽象，方便框架使用者根据模型的需要，自定义实现特定类型的层，我们期望 MDL 通过添加不同类型的层实现对更多网络模型的支持，而不需要改动其他位置的代码；为了保证框架的鲁棒性，MDL 通过反射机制，将 C++ 底层异常抛到应用层，应用层通过捕获异常对异常进行相应处理，如通过日志收集异常信息、保证软件可持续优化等；目前行业内各种深度学习训练框架种类繁多，而 MDL 不支持模型训练能力，为了保证框架的兼容性，我们提供 Caffe 模型转 MDL 的工具脚本，使用者通过一行命令就可以完成模型的转换及量化过程，后续我们会陆续支持 PaddlePaddle、<a href="http://www.99lt.com/plus/search.php?kwtype=0&amp;q=TensorFlow" rel="nofollow" style="text-decoration:none;color:rgb(11,59,140);" data-token="be3fddd633d8e908d170bdbbd8b7be8c"><span>TensorFlow</span></a>&nbsp;等模型转 MDL，兼容更多种类的模型。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <span>总体架构</span></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> MDL 框架的总体架构设计图如下：</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <img alt="大数据" class="aligncenter size-full wp-image-97543" height="446" src="http://www.99lt.com/uploads/allimg/170926/1050361013-7.jpg" width="640" style="width:500px;visibility:visible;border:0px;vertical-align:middle;display:block;text-align:center;"></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> MDL 框架主要包括模型转换模块（MDL Converter）、模型加载模块（Loader）、网络管理模块（Net）、矩阵运算模块（Gemmers）及供 Android 端调用的 JNI 接口层（JNI Interfaces）。其中，模型转换模块主要负责将 Caffe 模型转为 MDL 模型，同时支持将 32bit 浮点型参数量化为 8bit 参数，从而极大地压缩模型体积；模型加载模块主要完成模型的反量化及加载校验、网络注册等过程，网络管理模块主要负责网络中各层 Layer 的初始化及管理工作；MDL 提供了供 Android 端调用的 JNI 接口层，开发者可以通过调用 JNI 接口轻松完成加载及预测过程。</p>
<h2 style="font-size:18px;font-weight:normal;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;line-height:18px;color:rgb(68,68,68);border-bottom-width:1px;border-bottom-style:solid;border-bottom-color:rgb(238,238,238);"> <span>MDL 定位简单可用</span></h2>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> MDL 开源项目在实施之初就已经有了清晰定位。在设备繁杂且性能较低的移动端平台技术研发过程中，能够为新颖的深度学习技术找到合适场景并应用到自己的产品中是非常吸引人的。但如果让每个移动端工程师在应用深度学习过程中都要重新写一次全部神经网络的实现，会增加较大成本。MDL 的定位是简单地使用和部署神经网络，如果使用基本功能则不需要进行过多配置和修改，甚至连<a href="http://www.99lt.com/ai/machine-learning/" rel="nofollow" style="text-decoration:none;color:rgb(11,59,140);" data-token="92cd3a5751af69c5fde7d1c55aef91d5"><span>机器学习</span></a>库的编译过程都不需要，只需要关注具体业务实现、如何使用即可。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 与此同时 MDL 简单清晰的代码结构也可以作为学习材料，为刚刚接触深度学习的研发工程师提供参考。因为我们在支持手机平台交叉编译同时，也支持 Linux 和 Mac 的 x86 平台编译，在调整深度学习代码的同时可以直接在工作电脑上编译运行，而不需要部署到 arm 平台。所需要的只是简单的几行代码，具体可以查阅 MDL 的 GitHub Readme。</p>
<pre style="overflow:auto;font-family:'courier new';line-height:20px;color:rgb(248,248,212);border:none;background:rgb(74,74,74);"># https://github.com/baidu/mobile-deep-learning
 # mac or linux:
 ./build.sh mac
 cd build/release/x86/build
 ./mdlTest</pre>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 复杂的编译过程往往比开发的时间更长，在 MDL 中只要一行./build.sh android 就能把 so、测试 test 都搞定，部署非常简便。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> ./build.sh android</p>
<h2 style="font-size:18px;font-weight:normal;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;line-height:18px;color:rgb(68,68,68);border-bottom-width:1px;border-bottom-style:solid;border-bottom-color:rgb(238,238,238);"> <span>MDL 的性能及兼容性</span></h2>
<ul class="list-paddingleft-2" style="color:rgb(102,102,102);font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:16px;line-height:27px;">
<li style="list-style:none;">体积 armv7 300k+</li>
<li style="list-style:none;">速度 iOS GPU mobilenet 可以达到 40ms、squeezenet 可以达到 30ms</li>
</ul>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> MDL 从立项到开源，已经迭代了一年多。移动端比较关注的多个指标都表现良好，如体积、功耗、速度。百度内部产品线在应用前也进行过多次对比，和已开源的相关项目对比，MDL 能够在保证速度和能耗的同时支持多种深度学习模型，如 mobilenet、googlenet v1、squeezenet 等，且具有 iOS GPU 版本，squeezenet 一次运行最快可以达到 3-40ms。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <span>同类框架对比</span></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 框架Caffe2TensorFlowncnnMDL(CPU)MDL(GPU)硬件CPUCPUCPUCPUGPU速度慢慢快快极快体积大大小小小兼容Android&amp;iOSAndroid&amp;iOSAndroid&amp;iOSAndroid&amp;iOSiOS</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 与支持 CNN 的移动端框架对比，MDL 速度快、性能稳定、兼容性好、demo 完备。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> <span>兼容性</span></p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> MDL 在 iOS 和 Android 平台均可以稳定运行，其中 iOS10 及以上平台有基于 GPU 运算的 API，性能表现非常出色，在 Android 平台则是纯 CPU 运行。高中低端机型运行状态和手机百度及其他 App 上的覆盖都有绝对优势。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> MDL 同时也支持 Caffe 模型直接转换为 MDL 模型。</p>
<h3 style="font-size:16px;font-weight:normal;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;line-height:1.1;color:rgb(68,68,68);border-bottom-color:rgb(238,238,238);border-bottom-width:1px;border-bottom-style:solid;"> MDL 特性一览</h3>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 在移动 AI 相关研发启动之初，百度图像搜索团队对比了大部分已经开源的同类 CNN 框架，百家争鸣的同时也暴露了该方向的问题。一些框架实验数据表现优秀，实际产品中或是表现较差且性能极不稳定，或是机型无法全覆盖，或是体积达不到上线标准。为了避免这些问题，MDL 加入了以下 Features：</p>
<ul class="list-paddingleft-2" style="color:rgb(102,102,102);font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:16px;line-height:27px;">
<li style="list-style:none;">一键部署，脚本参数就可以切换 iOS 或者 Android</li>
<li style="list-style:none;">支持 Caffe 模型全自动转换为 MDL 模型</li>
<li style="list-style:none;">支持 GPU 运行</li>
<li style="list-style:none;">已经测试过可以稳定运行 MobileNet、GoogLeNet v1、squeezenet 模型</li>
<li style="list-style:none;">体积极小，无任何第三方依赖，纯手工打造</li>
<li style="list-style:none;">提供量化脚本，直接支持 32 位 float 转 8 位 uint，模型体积量化后在 4M 上下</li>
<li style="list-style:none;">与 ARM 相关算法团队线上线下多次沟通，针对 ARM 平台会持续优化</li>
<li style="list-style:none;">NEON 使用涵盖了卷积、归一化、池化等运算过程</li>
<li style="list-style:none;">loop unrolling 循环展开，为提升性能减少不必要的 CPU 消耗，全部展开判断操作</li>
<li style="list-style:none;">将大量繁重的计算任务前置到 overhead 过程</li>
</ul>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 后续规划</p>
<ul class="list-paddingleft-2" style="color:rgb(102,102,102);font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:16px;line-height:27px;">
<li style="list-style:none;">为了让 MDL 体积进一步缩小，MDL 并未使用 protobuf 做为模型配置存储，而是使用了 Json 格式。目前 MDL 支持 Caffe 模型转换到 MDL 模型，未来会支持全部主流模型转换为 MDL 模型。</li>
<li style="list-style:none;">随着移动端设备运算性能的提升，GPU 在未来移动端运算领域将会承担非常重要的角色，MDL 对于 GPU 的实现极为看重。目前 MDL 已经支持 iOS GPU 运行，iOS10 以上版本机型均可以使用。根据目前得到的统计数据显示，iOS10 已经涵盖绝大部分 iOS 系统，在 iOS10 以下可以使用 CPU 运算。除此之外，虽然 Android 平台目前的 GPU 运算能力与 CPU 相比整体偏弱，但日益涌现的新机型 GPU 已经越来越强大。MDL 后面也将加入 GPU 的 Feature 实现，基于 OpenCL 的 Android 平台 GPU 运算会让高端机型的运算性能再提升一个台阶。</li>
</ul>
<h2 style="font-size:18px;font-weight:normal;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;line-height:18px;color:rgb(68,68,68);border-bottom-width:1px;border-bottom-style:solid;border-bottom-color:rgb(238,238,238);"> <span>欢迎开发者贡献代码</span></h2>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 移动端神经网络的稳定高效运行，离不开诸多开发者的编码。MDL 长期本着靠谱运行、实用而不虚美的宗旨，希望能为移动端深度学习技术添砖加瓦。强烈欢迎有识之士济济加入，将深度学习技术在移动端广泛应用、播扬海内。</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> 最后再次奉上 MDL 的 GitHub 目录：</p>
<p style="text-indent:0em;color:rgb(102,102,102);line-height:27px;font-size:16px;font-family:'Microsoft Yahei', 'Helvetica Neue', Helvetica, Arial, sans-serif;"> https://github.com/baidu/mobile-deep-learning</p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>Deep Learning源代码收集-持续更新…</title>
		<link>https://uzzz.org/article/1590.html</link>
				<pubDate>Sun, 22 Sep 2013 15:25:37 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[机器学习]]></category>
		<category><![CDATA[计算机视觉]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1590.html</guid>
				<description><![CDATA[Deep Learning源代码收集-持续更新… zouxy09@qq.com http://blog.csdn.net/zouxy09 &#160; 收集了一些Deep Learning的源代码。主要是Matlab和C++的，当然也有python的。放在这里，后续遇到新的会持续更新。下表没有的也欢迎大家提供，以便大家使用和交流。谢谢。 &#160; 最近一次更新：2013-9-22 Theano http://deeplearning.net/software/theano/ code from: http://deeplearning.net/ &#160; Deep Learning Tutorial notes and code https://github.com/lisa-lab/DeepLearningTutorials code from: lisa-lab &#160; A Matlab toolbox for Deep Learning https://github.com/rasmusbergpalm/DeepLearnToolbox code from: RasmusBerg Palm &#160; deepmat Matlab Code for Restricted/Deep BoltzmannMachines and Autoencoder https://github.com/kyunghyuncho/deepmat code from: KyungHyun Cho http://users.ics.aalto.fi/kcho/ &#160; Training a deep autoencoder or a classifieron MNIST digits http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html code from: Ruslan Salakhutdinov and GeoffHinton &#160; CNN &#8211; Convolutional neural network class http://www.mathworks.cn/matlabcentral/fileexchange/24291 Code from:&#160;matlab &#160; Neural Network for Recognition ofHandwritten Digits (CNN) http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi &#160; cuda-convnet A fast C++/CUDA implementation ofconvolutional neural networks http://code.google.com/p/cuda-convnet/ &#160; matrbm a small library that can train RestrictedBoltzmann Machines, and also Deep Belief Networks of stacked RBM&#8217;s. http://code.google.com/p/matrbm/ code from: Andrej Karpathy &#160; Exercise &#160;from UFLDL Tutorial: http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial and tornadomeet’s bolg: http://www.cnblogs.com/tornadomeet/tag/Deep%20Learning/ and https://github.com/dkyang/UFLDL-Tutorial-Exercise &#160; Conditional Restricted Boltzmann Machines http://www.cs.nyu.edu/~gwtaylor/publications/nips2006mhmublv/code.html from Graham Taylor http://www.cs.nyu.edu/~gwtaylor/ &#160; Factored Conditional Restricted BoltzmannMachines http://www.cs.nyu.edu/~gwtaylor/publications/icml2009/code/index.html from Graham Taylor http://www.cs.nyu.edu/~gwtaylor/ &#160; Marginalized Stacked Denoising Autoencodersfor Domain Adaptation http://www1.cse.wustl.edu/~mchen/code/mSDA.tar code from: http://www.cse.wustl.edu/~kilian/code/code.html &#160; Tiled Convolutional Neural Networks http://cs.stanford.edu/~quocle/TCNNweb/pretraining.tar.gz http://cs.stanford.edu/~pangwei/projects.html &#160; tiny-cnn: A C++11 implementation of convolutionalneural networks https://github.com/nyanp/tiny-cnn]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<p align="center"><strong><span style="font-size:18px;">Deep Learning源代码收集-持续更新…</span></strong></p>
<p align="center"><a href="mailto:zouxy09@qq.com" rel="nofollow" data-token="f3812e343a40b008a7d87ce4306d840b"><span style="font-size:18px;">zouxy09@qq.com</span></a></p>
<p align="center"><a href="http://blog.csdn.net/zouxy09" rel="nofollow" data-token="c1fde63ac92a115a2d64a7ef97648083"><span style="font-size:18px;">http://blog.csdn.net/zouxy09</span></a></p>
<p><span style="font-size:18px;">&nbsp;</span></p>
<p><span style="font-size:18px;"><span> </span>收集了一些Deep Learning的源代码。主要是Matlab和C++的，当然也有python的。放在这里，后续遇到新的会持续更新。下表没有的也欢迎大家提供，以便大家使用和交流。谢谢。</span></p>
<p><span style="font-size:18px;">&nbsp;</span></p>
<p><span style="font-size:18px;"><span style="color:#FF0000;"><span> </span>最近一次更新：</span><span style="color:#FF0000;">2013-9-22</span></span></p>
<p><span style="font-size:14px;"></span></p>
<p><span style="font-size:14px;">Theano</span></p>
<p><a href="http://deeplearning.net/software/theano/" rel="nofollow" data-token="4983f47c8aad28d8705cd9ba57253a48"><span style="font-size:14px;">http://deeplearning.net/software/theano/</span></a></p>
<p><span style="font-size:14px;">code from: <a href="http://deeplearning.net/" rel="nofollow" data-token="5e1038ed14d723f8c3fcb48cfb719897">http://deeplearning.net/</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Deep Learning Tutorial notes and code</span></p>
<p><a href="https://github.com/lisa-lab/DeepLearningTutorials" rel="nofollow" data-token="08e1c71148e358578c2bf5e2ad0801e6"><span style="font-size:14px;">https://github.com/lisa-lab/DeepLearningTutorials</span></a></p>
<p><span style="font-size:14px;">code from: lisa-lab</span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">A Matlab toolbox for Deep Learning</span></p>
<p><a href="https://github.com/rasmusbergpalm/DeepLearnToolbox" rel="nofollow" data-token="bbdab6971400447d697177c1a05f8e81"><span style="font-size:14px;">https://github.com/rasmusbergpalm/DeepLearnToolbox</span></a></p>
<p><span style="font-size:14px;">code from: RasmusBerg Palm</span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">deepmat</span></p>
<p><span style="font-size:14px;">Matlab Code for Restricted/Deep BoltzmannMachines and Autoencoder</span></p>
<p><a href="https://github.com/kyunghyuncho/deepmat" rel="nofollow" data-token="c3e1ba7579f7138b5a15e65e54e506e0"><span style="font-size:14px;">https://github.com/kyunghyuncho/deepmat</span></a></p>
<p><span style="font-size:14px;">code from: KyungHyun Cho <a href="http://users.ics.aalto.fi/kcho/" rel="nofollow" data-token="2d438d2ea4894b3beec0a9f35b227174">http://users.ics.aalto.fi/kcho/</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Training a deep autoencoder or a classifieron MNIST digits</span></p>
<p><a href="http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html" rel="nofollow" data-token="7d21cfc6172b385be4cbf98dd9164428"><span style="font-size:14px;">http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html</span></a></p>
<p><span style="font-size:14px;">code from: Ruslan Salakhutdinov and GeoffHinton</span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">CNN &#8211; Convolutional neural network class</span></p>
<p><a href="http://www.mathworks.cn/matlabcentral/fileexchange/24291" rel="nofollow" data-token="ca485d0326f9e0eed798dc378bf11d0f"><span style="font-size:14px;">http://www.mathworks.cn/matlabcentral/fileexchange/24291</span></a></p>
<p><span style="font-size:14px;">Code from:&nbsp;matlab</span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Neural Network for Recognition ofHandwritten Digits (CNN)</span></p>
<p><a href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi" rel="nofollow" data-token="28c4b95ea79ac00a669b83c4b2c2dacd"><span style="font-size:14px;">http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">cuda-convnet</span></p>
<p><span style="font-size:14px;">A fast C++/CUDA implementation ofconvolutional neural networks</span></p>
<p><a href="http://code.google.com/p/cuda-convnet/" rel="nofollow" data-token="c5581188f59af5b87be5373856b63f84"><span style="font-size:14px;">http://code.google.com/p/cuda-convnet/</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">matrbm</span></p>
<p><span style="font-size:14px;">a small library that can train RestrictedBoltzmann Machines, and also Deep Belief Networks of stacked RBM&#8217;s.</span></p>
<p><a href="http://code.google.com/p/matrbm/" rel="nofollow" data-token="e7392467c7fa5ddb61ffaa1965b2e756"><span style="font-size:14px;">http://code.google.com/p/matrbm/</span></a></p>
<p><span style="font-size:14px;">code from: Andrej Karpathy</span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Exercise &nbsp;from UFLDL Tutorial:</span></p>
<p><a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial" rel="nofollow" data-token="98e065e2c21f098c56b139cef25e9bfe"><span style="font-size:14px;">http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial</span></a></p>
<p><span style="font-size:14px;">and tornadomeet’s bolg: <a href="http://www.cnblogs.com/tornadomeet/tag/Deep%20Learning/" rel="nofollow" data-token="a3c0f91e853048dc9086de5abda7e554">http://www.cnblogs.com/tornadomeet/tag/Deep%20Learning/</a></span></p>
<p><span style="font-size:14px;">and <a href="https://github.com/dkyang/UFLDL-Tutorial-Exercise" rel="nofollow" data-token="619da06206e62cc6d7277d012a484be2">https://github.com/dkyang/UFLDL-Tutorial-Exercise</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Conditional Restricted Boltzmann Machines</span></p>
<p><a href="http://www.cs.nyu.edu/~gwtaylor/publications/nips2006mhmublv/code.html" rel="nofollow" data-token="babb4b5aee0c8962dcd6d593eea87ca6"><span style="font-size:14px;">http://www.cs.nyu.edu/~gwtaylor/publications/nips2006mhmublv/code.html</span></a></p>
<p><span style="font-size:14px;">from Graham Taylor <a href="http://www.cs.nyu.edu/~gwtaylor/" rel="nofollow" data-token="ea0802a363df0f355266168be74f7fc5">http://www.cs.nyu.edu/~gwtaylor/</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Factored Conditional Restricted BoltzmannMachines</span></p>
<p><a href="http://www.cs.nyu.edu/~gwtaylor/publications/icml2009/code/index.html" rel="nofollow" data-token="2c82b6b65245e131f9dd65bff9f1e905"><span style="font-size:14px;">http://www.cs.nyu.edu/~gwtaylor/publications/icml2009/code/index.html</span></a></p>
<p><span style="font-size:14px;">from Graham Taylor <a href="http://www.cs.nyu.edu/~gwtaylor/" rel="nofollow" data-token="ea0802a363df0f355266168be74f7fc5">http://www.cs.nyu.edu/~gwtaylor/</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Marginalized Stacked Denoising Autoencodersfor Domain Adaptation</span></p>
<p><a href="http://www1.cse.wustl.edu/~mchen/code/mSDA.tar" rel="nofollow" data-token="0bf0e8607a2796788ca615ed6057f8fa"><span style="font-size:14px;">http://www1.cse.wustl.edu/~mchen/code/mSDA.tar</span></a></p>
<p><span style="font-size:14px;">code from: <a href="http://www.cse.wustl.edu/~kilian/code/code.html" rel="nofollow" data-token="b8fc839c6e7610039194a3c3317e3fbc">http://www.cse.wustl.edu/~kilian/code/code.html</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Tiled Convolutional Neural Networks</span></p>
<p><a href="http://cs.stanford.edu/~quocle/TCNNweb/pretraining.tar.gz" rel="nofollow" data-token="664147aab17fe720a4b05b7d73a267be"><span style="font-size:14px;">http://cs.stanford.edu/~quocle/TCNNweb/pretraining.tar.gz</span></a></p>
<p><a href="http://cs.stanford.edu/~pangwei/projects.html" rel="nofollow" data-token="5d184ec08427259d1f78ab9403bc2ac6"><span style="font-size:14px;">http://cs.stanford.edu/~pangwei/projects.html</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">tiny-cnn:</span></p>
<p><span style="font-size:14px;">A C++11 implementation of convolutionalneural networks</span></p>
<p><a href="https://github.com/nyanp/tiny-cnn" rel="nofollow" data-token="1911b67bf46c83775efc41b4e26aa3a1"><span style="font-size:14px;">https://github.com/nyanp/tiny-cnn</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">myCNN</span></p>
<p><a href="https://github.com/aurofable/18551_Project/tree/master/server/2009-09-30-14-33-myCNN-0.07" rel="nofollow" data-token="698a2a7e55571631a4a983a6d7d30cd7"><span style="font-size:14px;">https://github.com/aurofable/18551_Project/tree/master/server/2009-09-30-14-33-myCNN-0.07</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Adaptive Deconvolutional Network Toolbox</span></p>
<p><a href="http://www.matthewzeiler.com/software/DeconvNetToolbox2/DeconvNetToolbox.zip" rel="nofollow" data-token="bc0d5eadac3988a17a4187119fa29c9e"><span style="font-size:14px;">http://www.matthewzeiler.com/software/DeconvNetToolbox2/DeconvNetToolbox.zip</span></a></p>
<p><a href="http://www.matthewzeiler.com/" rel="nofollow" data-token="5fd1f30972c9c85b0f78f6a963f755ac"><span style="font-size:14px;">http://www.matthewzeiler.com/</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Deep Learning手写字符识别C++代码</span></p>
<p><a href="http://download.csdn.net/detail/lucky_greenegg/5413211" rel="nofollow" data-token="555b601dc67bbd0ca5a079affb2ecd97"><span style="font-size:14px;">http://download.csdn.net/detail/lucky_greenegg/5413211</span></a></p>
<p><span style="font-size:14px;">from: <a href="http://blog.csdn.net/lucky_greenegg/article/details/8949578" rel="nofollow" data-token="dbb484194b5c828fcc8a868a168c051a">http://blog.csdn.net/lucky_greenegg/article/details/8949578</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">convolutionalRBM.m</span></p>
<p><span style="font-size:14px;">A MATLAB / MEX / CUDA-MEX implementation ofConvolutional Restricted Boltzmann Machines.</span></p>
<p><a href="https://github.com/qipeng/convolutionalRBM.m" rel="nofollow" data-token="6ff00fe1940cfe82a0927ee5add01d5c"><span style="font-size:14px;">https://github.com/qipeng/convolutionalRBM.m</span></a></p>
<p><span style="font-size:14px;">from: <a href="http://qipeng.me/software/convolutional-rbm.html" rel="nofollow" data-token="7ad9a567f6eeb30a181eac089dec5def">http://qipeng.me/software/convolutional-rbm.html</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">rbm-mnist</span></p>
<p><span style="font-size:14px;">C++ 11 implementation of Geoff Hinton&#8217;sDeep Learning matlab code</span></p>
<p><a href="https://github.com/jdeng/rbm-mnist" rel="nofollow" data-token="af7f9b46d5196d8b6ea0f9a95de260fb"><span style="font-size:14px;">https://github.com/jdeng/rbm-mnist</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Learning Deep Boltzmann Machines</span></p>
<p><a href="http://web.mit.edu/~rsalakhu/www/code_DBM/code_DBM.tar" rel="nofollow" data-token="9e89c43b8f2e4cba51bb3bb39326e7ab"><span style="font-size:14px;">http://web.mit.edu/~rsalakhu/www/code_DBM/code_DBM.tar</span></a></p>
<p><a href="http://web.mit.edu/~rsalakhu/www/DBM.html" rel="nofollow" data-token="e0d7c4439a79b1286e726838fbfeb650"><span style="font-size:14px;">http://web.mit.edu/~rsalakhu/www/DBM.html</span></a></p>
<p><span style="font-size:14px;">Code provided by Ruslan Salakhutdinov</span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Efficient sparse coding algorithms</span></p>
<p><a href="http://web.eecs.umich.edu/~honglak/softwares/fast_sc.tgz" rel="nofollow" data-token="de1f77ba414245882970274c9288487a"><span style="font-size:14px;">http://web.eecs.umich.edu/~honglak/softwares/fast_sc.tgz</span></a></p>
<p><a href="http://web.eecs.umich.edu/~honglak/softwares/nips06-sparsecoding.htm" rel="nofollow" data-token="427915573b14843a46710d230793aa5a"><span style="font-size:14px;">http://web.eecs.umich.edu/~honglak/softwares/nips06-sparsecoding.htm</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Linear Spatial Pyramid Matching UsingSparse Coding for Image Classification</span></p>
<p><a href="http://www.ifp.illinois.edu/~jyang29/codes/CVPR09-ScSPM.rar" rel="nofollow" data-token="cf88744b8e0976d4b35865841d081f8c"><span style="font-size:14px;">http://www.ifp.illinois.edu/~jyang29/codes/CVPR09-ScSPM.rar</span></a></p>
<p><a href="http://www.ifp.illinois.edu/~jyang29/ScSPM.htm" rel="nofollow" data-token="93f4aa055a60e82db710da21234e662d"><span style="font-size:14px;">http://www.ifp.illinois.edu/~jyang29/ScSPM.htm</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">SPAMS </span></p>
<p><span style="font-size:14px;">(SPArse Modeling Software) is anoptimization toolbox for solving various sparse estimation problems.</span></p>
<p><a href="http://spams-devel.gforge.inria.fr/" rel="nofollow" data-token="0004697ef479de42855fe0f6080223e4"><span style="font-size:14px;">http://spams-devel.gforge.inria.fr/</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">sparsenet</span></p>
<p><span style="font-size:14px;">Sparse coding simulation software</span></p>
<p><a href="http://redwood.berkeley.edu/bruno/sparsenet/" rel="nofollow" data-token="2a040194694ccf720b41c178fb01569a"><span style="font-size:14px;">http://redwood.berkeley.edu/bruno/sparsenet/</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">fast dropout training</span></p>
<p><a href="https://github.com/sidaw/fastdropout" rel="nofollow" data-token="015929138bb62cdcd437c82b6ad7b74c"><span style="font-size:14px;">https://github.com/sidaw/fastdropout</span></a></p>
<p><a href="http://nlp.stanford.edu/~sidaw/home/start" rel="nofollow" data-token="a63bc976ec47727d1bf4e13d33dbbf06"><span style="font-size:14px;">http://nlp.stanford.edu/~sidaw/home/start</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Deep Learning of Invariant Features viaSimulated Fixations in Video</span></p>
<p><a href="http://ai.stanford.edu/~wzou/deepslow_release.tar.gz" rel="nofollow" data-token="735d50275cf84ddf47d23b6a76d0ce47"><span style="font-size:14px;">http://ai.stanford.edu/~wzou/deepslow_release.tar.gz</span></a></p>
<p><a href="http://ai.stanford.edu/~wzou/" rel="nofollow" data-token="73aacf54079358dcb918cce069784445"><span style="font-size:14px;">http://ai.stanford.edu/~wzou/</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Sparse filtering</span></p>
<p><a href="http://cs.stanford.edu/~jngiam/papers/NgiamKohChenBhaskarNg2011_Supplementary.pdf" rel="nofollow" data-token="2fa3be1a32e7fa9a3c810e480e5e2047"><span style="font-size:14px;">http://cs.stanford.edu/~jngiam/papers/NgiamKohChenBhaskarNg2011_Supplementary.pdf</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">k-means</span></p>
<p><a href="http://www.stanford.edu/~acoates/papers/kmeans_demo.tgz" rel="nofollow" data-token="2df1ddb6830fee5a5dc16881079c81a0"><span style="font-size:14px;">http://www.stanford.edu/~acoates/papers/kmeans_demo.tgz</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">others:</span></p>
<p><a href="http://deeplearning.net/software_links/" rel="nofollow" data-token="03127354a17b2465b8c0009c718eadcf"><span style="font-size:14px;">http://deeplearning.net/software_links/</span></a></p>
<p></p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>Deep Learning源代码收集-持续更新…</title>
		<link>https://uzzz.org/article/1658.html</link>
				<pubDate>Sun, 22 Sep 2013 15:25:37 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[机器学习]]></category>
		<category><![CDATA[计算机视觉]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1658.html</guid>
				<description><![CDATA[Deep Learning源代码收集-持续更新… zouxy09@qq.com http://blog.csdn.net/zouxy09 &#160; 收集了一些Deep Learning的源代码。主要是Matlab和C++的，当然也有python的。放在这里，后续遇到新的会持续更新。下表没有的也欢迎大家提供，以便大家使用和交流。谢谢。 &#160; 最近一次更新：2013-9-22 Theano http://deeplearning.net/software/theano/ code from: http://deeplearning.net/ &#160; Deep Learning Tutorial notes and code https://github.com/lisa-lab/DeepLearningTutorials code from: lisa-lab &#160; A Matlab toolbox for Deep Learning https://github.com/rasmusbergpalm/DeepLearnToolbox code from: RasmusBerg Palm &#160; deepmat Matlab Code for Restricted/Deep BoltzmannMachines and Autoencoder https://github.com/kyunghyuncho/deepmat code from: KyungHyun Cho http://users.ics.aalto.fi/kcho/ &#160; Training a deep autoencoder or a classifieron MNIST digits http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html code from: Ruslan Salakhutdinov and GeoffHinton &#160; CNN &#8211; Convolutional neural network class http://www.mathworks.cn/matlabcentral/fileexchange/24291 Code from:&#160;matlab &#160; Neural Network for Recognition ofHandwritten Digits (CNN) http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi &#160; cuda-convnet A fast C++/CUDA implementation ofconvolutional neural networks http://code.google.com/p/cuda-convnet/ &#160; matrbm a small library that can train RestrictedBoltzmann Machines, and also Deep Belief Networks of stacked RBM&#8217;s. http://code.google.com/p/matrbm/ code from: Andrej Karpathy &#160; Exercise &#160;from UFLDL Tutorial: http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial and tornadomeet’s bolg: http://www.cnblogs.com/tornadomeet/tag/Deep%20Learning/ and https://github.com/dkyang/UFLDL-Tutorial-Exercise &#160; Conditional Restricted Boltzmann Machines http://www.cs.nyu.edu/~gwtaylor/publications/nips2006mhmublv/code.html from Graham Taylor http://www.cs.nyu.edu/~gwtaylor/ &#160; Factored Conditional Restricted BoltzmannMachines http://www.cs.nyu.edu/~gwtaylor/publications/icml2009/code/index.html from Graham Taylor http://www.cs.nyu.edu/~gwtaylor/ &#160; Marginalized Stacked Denoising Autoencodersfor Domain Adaptation http://www1.cse.wustl.edu/~mchen/code/mSDA.tar code from: http://www.cse.wustl.edu/~kilian/code/code.html &#160; Tiled Convolutional Neural Networks http://cs.stanford.edu/~quocle/TCNNweb/pretraining.tar.gz http://cs.stanford.edu/~pangwei/projects.html &#160; tiny-cnn: A C++11 implementation of convolutionalneural networks https://github.com/nyanp/tiny-cnn]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<p align="center"><strong><span style="font-size:18px;">Deep Learning源代码收集-持续更新…</span></strong></p>
<p align="center"><a href="mailto:zouxy09@qq.com" rel="nofollow" data-token="f3812e343a40b008a7d87ce4306d840b"><span style="font-size:18px;">zouxy09@qq.com</span></a></p>
<p align="center"><a href="http://blog.csdn.net/zouxy09" rel="nofollow" data-token="c1fde63ac92a115a2d64a7ef97648083"><span style="font-size:18px;">http://blog.csdn.net/zouxy09</span></a></p>
<p><span style="font-size:18px;">&nbsp;</span></p>
<p><span style="font-size:18px;"><span> </span>收集了一些Deep Learning的源代码。主要是Matlab和C++的，当然也有python的。放在这里，后续遇到新的会持续更新。下表没有的也欢迎大家提供，以便大家使用和交流。谢谢。</span></p>
<p><span style="font-size:18px;">&nbsp;</span></p>
<p><span style="font-size:18px;"><span style="color:#FF0000;"><span> </span>最近一次更新：</span><span style="color:#FF0000;">2013-9-22</span></span></p>
<p><span style="font-size:14px;"></span></p>
<p><span style="font-size:14px;">Theano</span></p>
<p><a href="http://deeplearning.net/software/theano/" rel="nofollow" data-token="4983f47c8aad28d8705cd9ba57253a48"><span style="font-size:14px;">http://deeplearning.net/software/theano/</span></a></p>
<p><span style="font-size:14px;">code from: <a href="http://deeplearning.net/" rel="nofollow" data-token="5e1038ed14d723f8c3fcb48cfb719897">http://deeplearning.net/</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Deep Learning Tutorial notes and code</span></p>
<p><a href="https://github.com/lisa-lab/DeepLearningTutorials" rel="nofollow" data-token="08e1c71148e358578c2bf5e2ad0801e6"><span style="font-size:14px;">https://github.com/lisa-lab/DeepLearningTutorials</span></a></p>
<p><span style="font-size:14px;">code from: lisa-lab</span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">A Matlab toolbox for Deep Learning</span></p>
<p><a href="https://github.com/rasmusbergpalm/DeepLearnToolbox" rel="nofollow" data-token="bbdab6971400447d697177c1a05f8e81"><span style="font-size:14px;">https://github.com/rasmusbergpalm/DeepLearnToolbox</span></a></p>
<p><span style="font-size:14px;">code from: RasmusBerg Palm</span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">deepmat</span></p>
<p><span style="font-size:14px;">Matlab Code for Restricted/Deep BoltzmannMachines and Autoencoder</span></p>
<p><a href="https://github.com/kyunghyuncho/deepmat" rel="nofollow" data-token="c3e1ba7579f7138b5a15e65e54e506e0"><span style="font-size:14px;">https://github.com/kyunghyuncho/deepmat</span></a></p>
<p><span style="font-size:14px;">code from: KyungHyun Cho <a href="http://users.ics.aalto.fi/kcho/" rel="nofollow" data-token="2d438d2ea4894b3beec0a9f35b227174">http://users.ics.aalto.fi/kcho/</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Training a deep autoencoder or a classifieron MNIST digits</span></p>
<p><a href="http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html" rel="nofollow" data-token="7d21cfc6172b385be4cbf98dd9164428"><span style="font-size:14px;">http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html</span></a></p>
<p><span style="font-size:14px;">code from: Ruslan Salakhutdinov and GeoffHinton</span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">CNN &#8211; Convolutional neural network class</span></p>
<p><a href="http://www.mathworks.cn/matlabcentral/fileexchange/24291" rel="nofollow" data-token="ca485d0326f9e0eed798dc378bf11d0f"><span style="font-size:14px;">http://www.mathworks.cn/matlabcentral/fileexchange/24291</span></a></p>
<p><span style="font-size:14px;">Code from:&nbsp;matlab</span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Neural Network for Recognition ofHandwritten Digits (CNN)</span></p>
<p><a href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi" rel="nofollow" data-token="28c4b95ea79ac00a669b83c4b2c2dacd"><span style="font-size:14px;">http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">cuda-convnet</span></p>
<p><span style="font-size:14px;">A fast C++/CUDA implementation ofconvolutional neural networks</span></p>
<p><a href="http://code.google.com/p/cuda-convnet/" rel="nofollow" data-token="c5581188f59af5b87be5373856b63f84"><span style="font-size:14px;">http://code.google.com/p/cuda-convnet/</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">matrbm</span></p>
<p><span style="font-size:14px;">a small library that can train RestrictedBoltzmann Machines, and also Deep Belief Networks of stacked RBM&#8217;s.</span></p>
<p><a href="http://code.google.com/p/matrbm/" rel="nofollow" data-token="e7392467c7fa5ddb61ffaa1965b2e756"><span style="font-size:14px;">http://code.google.com/p/matrbm/</span></a></p>
<p><span style="font-size:14px;">code from: Andrej Karpathy</span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Exercise &nbsp;from UFLDL Tutorial:</span></p>
<p><a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial" rel="nofollow" data-token="98e065e2c21f098c56b139cef25e9bfe"><span style="font-size:14px;">http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial</span></a></p>
<p><span style="font-size:14px;">and tornadomeet’s bolg: <a href="http://www.cnblogs.com/tornadomeet/tag/Deep%20Learning/" rel="nofollow" data-token="a3c0f91e853048dc9086de5abda7e554">http://www.cnblogs.com/tornadomeet/tag/Deep%20Learning/</a></span></p>
<p><span style="font-size:14px;">and <a href="https://github.com/dkyang/UFLDL-Tutorial-Exercise" rel="nofollow" data-token="619da06206e62cc6d7277d012a484be2">https://github.com/dkyang/UFLDL-Tutorial-Exercise</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Conditional Restricted Boltzmann Machines</span></p>
<p><a href="http://www.cs.nyu.edu/~gwtaylor/publications/nips2006mhmublv/code.html" rel="nofollow" data-token="babb4b5aee0c8962dcd6d593eea87ca6"><span style="font-size:14px;">http://www.cs.nyu.edu/~gwtaylor/publications/nips2006mhmublv/code.html</span></a></p>
<p><span style="font-size:14px;">from Graham Taylor <a href="http://www.cs.nyu.edu/~gwtaylor/" rel="nofollow" data-token="ea0802a363df0f355266168be74f7fc5">http://www.cs.nyu.edu/~gwtaylor/</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Factored Conditional Restricted BoltzmannMachines</span></p>
<p><a href="http://www.cs.nyu.edu/~gwtaylor/publications/icml2009/code/index.html" rel="nofollow" data-token="2c82b6b65245e131f9dd65bff9f1e905"><span style="font-size:14px;">http://www.cs.nyu.edu/~gwtaylor/publications/icml2009/code/index.html</span></a></p>
<p><span style="font-size:14px;">from Graham Taylor <a href="http://www.cs.nyu.edu/~gwtaylor/" rel="nofollow" data-token="ea0802a363df0f355266168be74f7fc5">http://www.cs.nyu.edu/~gwtaylor/</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Marginalized Stacked Denoising Autoencodersfor Domain Adaptation</span></p>
<p><a href="http://www1.cse.wustl.edu/~mchen/code/mSDA.tar" rel="nofollow" data-token="0bf0e8607a2796788ca615ed6057f8fa"><span style="font-size:14px;">http://www1.cse.wustl.edu/~mchen/code/mSDA.tar</span></a></p>
<p><span style="font-size:14px;">code from: <a href="http://www.cse.wustl.edu/~kilian/code/code.html" rel="nofollow" data-token="b8fc839c6e7610039194a3c3317e3fbc">http://www.cse.wustl.edu/~kilian/code/code.html</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Tiled Convolutional Neural Networks</span></p>
<p><a href="http://cs.stanford.edu/~quocle/TCNNweb/pretraining.tar.gz" rel="nofollow" data-token="664147aab17fe720a4b05b7d73a267be"><span style="font-size:14px;">http://cs.stanford.edu/~quocle/TCNNweb/pretraining.tar.gz</span></a></p>
<p><a href="http://cs.stanford.edu/~pangwei/projects.html" rel="nofollow" data-token="5d184ec08427259d1f78ab9403bc2ac6"><span style="font-size:14px;">http://cs.stanford.edu/~pangwei/projects.html</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">tiny-cnn:</span></p>
<p><span style="font-size:14px;">A C++11 implementation of convolutionalneural networks</span></p>
<p><a href="https://github.com/nyanp/tiny-cnn" rel="nofollow" data-token="1911b67bf46c83775efc41b4e26aa3a1"><span style="font-size:14px;">https://github.com/nyanp/tiny-cnn</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">myCNN</span></p>
<p><a href="https://github.com/aurofable/18551_Project/tree/master/server/2009-09-30-14-33-myCNN-0.07" rel="nofollow" data-token="698a2a7e55571631a4a983a6d7d30cd7"><span style="font-size:14px;">https://github.com/aurofable/18551_Project/tree/master/server/2009-09-30-14-33-myCNN-0.07</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Adaptive Deconvolutional Network Toolbox</span></p>
<p><a href="http://www.matthewzeiler.com/software/DeconvNetToolbox2/DeconvNetToolbox.zip" rel="nofollow" data-token="bc0d5eadac3988a17a4187119fa29c9e"><span style="font-size:14px;">http://www.matthewzeiler.com/software/DeconvNetToolbox2/DeconvNetToolbox.zip</span></a></p>
<p><a href="http://www.matthewzeiler.com/" rel="nofollow" data-token="5fd1f30972c9c85b0f78f6a963f755ac"><span style="font-size:14px;">http://www.matthewzeiler.com/</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Deep Learning手写字符识别C++代码</span></p>
<p><a href="http://download.csdn.net/detail/lucky_greenegg/5413211" rel="nofollow" data-token="555b601dc67bbd0ca5a079affb2ecd97"><span style="font-size:14px;">http://download.csdn.net/detail/lucky_greenegg/5413211</span></a></p>
<p><span style="font-size:14px;">from: <a href="http://blog.csdn.net/lucky_greenegg/article/details/8949578" rel="nofollow" data-token="dbb484194b5c828fcc8a868a168c051a">http://blog.csdn.net/lucky_greenegg/article/details/8949578</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">convolutionalRBM.m</span></p>
<p><span style="font-size:14px;">A MATLAB / MEX / CUDA-MEX implementation ofConvolutional Restricted Boltzmann Machines.</span></p>
<p><a href="https://github.com/qipeng/convolutionalRBM.m" rel="nofollow" data-token="6ff00fe1940cfe82a0927ee5add01d5c"><span style="font-size:14px;">https://github.com/qipeng/convolutionalRBM.m</span></a></p>
<p><span style="font-size:14px;">from: <a href="http://qipeng.me/software/convolutional-rbm.html" rel="nofollow" data-token="7ad9a567f6eeb30a181eac089dec5def">http://qipeng.me/software/convolutional-rbm.html</a></span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">rbm-mnist</span></p>
<p><span style="font-size:14px;">C++ 11 implementation of Geoff Hinton&#8217;sDeep Learning matlab code</span></p>
<p><a href="https://github.com/jdeng/rbm-mnist" rel="nofollow" data-token="af7f9b46d5196d8b6ea0f9a95de260fb"><span style="font-size:14px;">https://github.com/jdeng/rbm-mnist</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Learning Deep Boltzmann Machines</span></p>
<p><a href="http://web.mit.edu/~rsalakhu/www/code_DBM/code_DBM.tar" rel="nofollow" data-token="9e89c43b8f2e4cba51bb3bb39326e7ab"><span style="font-size:14px;">http://web.mit.edu/~rsalakhu/www/code_DBM/code_DBM.tar</span></a></p>
<p><a href="http://web.mit.edu/~rsalakhu/www/DBM.html" rel="nofollow" data-token="e0d7c4439a79b1286e726838fbfeb650"><span style="font-size:14px;">http://web.mit.edu/~rsalakhu/www/DBM.html</span></a></p>
<p><span style="font-size:14px;">Code provided by Ruslan Salakhutdinov</span></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Efficient sparse coding algorithms</span></p>
<p><a href="http://web.eecs.umich.edu/~honglak/softwares/fast_sc.tgz" rel="nofollow" data-token="de1f77ba414245882970274c9288487a"><span style="font-size:14px;">http://web.eecs.umich.edu/~honglak/softwares/fast_sc.tgz</span></a></p>
<p><a href="http://web.eecs.umich.edu/~honglak/softwares/nips06-sparsecoding.htm" rel="nofollow" data-token="427915573b14843a46710d230793aa5a"><span style="font-size:14px;">http://web.eecs.umich.edu/~honglak/softwares/nips06-sparsecoding.htm</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Linear Spatial Pyramid Matching UsingSparse Coding for Image Classification</span></p>
<p><a href="http://www.ifp.illinois.edu/~jyang29/codes/CVPR09-ScSPM.rar" rel="nofollow" data-token="cf88744b8e0976d4b35865841d081f8c"><span style="font-size:14px;">http://www.ifp.illinois.edu/~jyang29/codes/CVPR09-ScSPM.rar</span></a></p>
<p><a href="http://www.ifp.illinois.edu/~jyang29/ScSPM.htm" rel="nofollow" data-token="93f4aa055a60e82db710da21234e662d"><span style="font-size:14px;">http://www.ifp.illinois.edu/~jyang29/ScSPM.htm</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">SPAMS </span></p>
<p><span style="font-size:14px;">(SPArse Modeling Software) is anoptimization toolbox for solving various sparse estimation problems.</span></p>
<p><a href="http://spams-devel.gforge.inria.fr/" rel="nofollow" data-token="0004697ef479de42855fe0f6080223e4"><span style="font-size:14px;">http://spams-devel.gforge.inria.fr/</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">sparsenet</span></p>
<p><span style="font-size:14px;">Sparse coding simulation software</span></p>
<p><a href="http://redwood.berkeley.edu/bruno/sparsenet/" rel="nofollow" data-token="2a040194694ccf720b41c178fb01569a"><span style="font-size:14px;">http://redwood.berkeley.edu/bruno/sparsenet/</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">fast dropout training</span></p>
<p><a href="https://github.com/sidaw/fastdropout" rel="nofollow" data-token="015929138bb62cdcd437c82b6ad7b74c"><span style="font-size:14px;">https://github.com/sidaw/fastdropout</span></a></p>
<p><a href="http://nlp.stanford.edu/~sidaw/home/start" rel="nofollow" data-token="a63bc976ec47727d1bf4e13d33dbbf06"><span style="font-size:14px;">http://nlp.stanford.edu/~sidaw/home/start</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Deep Learning of Invariant Features viaSimulated Fixations in Video</span></p>
<p><a href="http://ai.stanford.edu/~wzou/deepslow_release.tar.gz" rel="nofollow" data-token="735d50275cf84ddf47d23b6a76d0ce47"><span style="font-size:14px;">http://ai.stanford.edu/~wzou/deepslow_release.tar.gz</span></a></p>
<p><a href="http://ai.stanford.edu/~wzou/" rel="nofollow" data-token="73aacf54079358dcb918cce069784445"><span style="font-size:14px;">http://ai.stanford.edu/~wzou/</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">Sparse filtering</span></p>
<p><a href="http://cs.stanford.edu/~jngiam/papers/NgiamKohChenBhaskarNg2011_Supplementary.pdf" rel="nofollow" data-token="2fa3be1a32e7fa9a3c810e480e5e2047"><span style="font-size:14px;">http://cs.stanford.edu/~jngiam/papers/NgiamKohChenBhaskarNg2011_Supplementary.pdf</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">k-means</span></p>
<p><a href="http://www.stanford.edu/~acoates/papers/kmeans_demo.tgz" rel="nofollow" data-token="2df1ddb6830fee5a5dc16881079c81a0"><span style="font-size:14px;">http://www.stanford.edu/~acoates/papers/kmeans_demo.tgz</span></a></p>
<p><span style="font-size:14px;">&nbsp;</span></p>
<p><span style="font-size:14px;">others:</span></p>
<p><a href="http://deeplearning.net/software_links/" rel="nofollow" data-token="03127354a17b2465b8c0009c718eadcf"><span style="font-size:14px;">http://deeplearning.net/software_links/</span></a></p>
<p></p>
</p></div>
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
