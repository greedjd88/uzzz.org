<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>图像处理 &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/category/tuxiangchuli/feed" rel="self" type="application/rss+xml" />
	<link>https://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Thu, 06 Dec 2018 13:21:08 +0000</lastBuildDate>
	<language>zh-CN</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>图像处理 &#8211; 有组织在!</title>
	<link>https://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>图计算论文笔记&#8211;Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<link>https://uzzz.org/article/1706.html</link>
				<pubDate>Thu, 06 Dec 2018 13:21:08 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[图像处理]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1706.html</guid>
				<description><![CDATA[Graph Convolutional Neural Networks for Web-Scale Recommender Systems abstract introduction related work method problem setup model architecture Forward propagation algorithm Impor]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<div class="toc">
<h3>Graph Convolutional Neural Networks for Web-Scale Recommender Systems</h3>
<ul>
<li><a href="#abstract_2" rel="nofollow" data-token="71936de71c86fba194524a83123debe3">abstract</a></li>
<li><a href="#introduction_5" rel="nofollow" data-token="1bb1e8f5bd351e71fddb2d8e001d3ef5">introduction</a></li>
<li><a href="#related_work_11" rel="nofollow" data-token="1b8ce216f53bb0e4a956cdf095f7fb0a">related work</a></li>
<li><a href="#method_13" rel="nofollow" data-token="27f6a7883ac31c844e2649a2e93ef9e1">method</a></li>
<ul>
<li><a href="#problem_setup_17" rel="nofollow" data-token="451aac041df9dad1182f77cc4ee452f3">problem setup</a></li>
<li><a href="#model_architecture_23" rel="nofollow" data-token="34670093421c08007162afd3c41db238">model architecture</a></li>
<ul>
<li><a href="#Forward_propagation_algorithm_25" rel="nofollow" data-token="3b7fb45992b2a969c28e0aaabeadc238">Forward propagation algorithm</a></li>
<li><a href="#Importancebased_neighborhoods_30" rel="nofollow" data-token="4934a57f79400a894fa83b0d5a78c4df">Importance-based neighborhoods</a></li>
<li><a href="#Stacking_convolutions_36" rel="nofollow" data-token="aa464ced07fecdb719d5b82aacd1c092">Stacking convolutions</a></li>
</ul>
<li><a href="#model_training_45" rel="nofollow" data-token="83d6415a7abaaf825c14e402132d9c13">model training</a></li>
<ul>
<li><a href="#MultiGPU_training_with_large_minibatches_58" rel="nofollow" data-token="e9404979ecb6776b8007611e702c32ca">Multi-GPU training with large minibatches</a></li>
<li><a href="#Producerconsumer_minibatch_construction_59" rel="nofollow" data-token="33e005e93bd82ae8ee5c1d46dba11f58">Producer-consumer minibatch construction.</a></li>
<li><a href="#Sampling_negative_items_60" rel="nofollow" data-token="7e97beadb81ad93076a2eba863675830">Sampling negative items</a></li>
</ul>
<li><a href="#Node_Embeddings_via_MapReduce_61" rel="nofollow" data-token="4aa646c69c5e144a56cf0e8ac7385c54">Node Embeddings via MapReduce</a></li>
</ul>
<li><a href="#_64" rel="nofollow" data-token="62ed42c22ef1bd199a47b6d4e5414311">总结</a></li>
</ul></div>
</p>
<h1><a id="abstract_2"></a>abstract</h1>
<p>在大规模数据上使用GCN做数据挖掘</p>
<h1><a id="introduction_5"></a>introduction</h1>
<ul>
<li>a random-walk-based GCN–PinSage 处理 3 billion nodes and 18 billion edges的图</li>
<li>为了在大规模图上运行，使用了On-the-fly convolutions；Producer-consumer minibatch construction； Efficient MapReduce inference</li>
<li>同时，使用的减少网络的复杂度的方法：Constructing convolutions via random walks；Importance pooling</li>
<li>使用Curriculumtraining来学习</li>
</ul>
<h1><a id="related_work_11"></a>related work</h1>
<h1><a id="method_13"></a>method</h1>
<p>GCN的方法：一个节点可以形成一个local network，对很多个节点的local network进行GCN，这样，GCN网络的权重被每个network共享。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2018120620332672.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYXZhbHNjeA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2><a id="problem_setup_17"></a>problem setup</h2>
<ul>
<li>将Pinterest上的pin和board组成二分图</li>
<li>pin可以看成item，board可以看成用户定义的contexts 或 collections。同时pin/item有information，有text和image features。</li>
<li>embedding item 考虑了item的information，也就是text和image，同时也考虑了网络的结构</li>
<li>目的是为了embedding pin也就是item，来通过相似的item推荐（nearest neighbor lookup，given a pin, find related pins），或者是来ranking这些item。</li>
</ul>
<h2><a id="model_architecture_23"></a>model architecture</h2>
<p>localized convolution operation：首先输入node feature，然后学习NN，来transform和aggregate图的feature，从而得到node的embedding。</p>
<h3><a id="Forward_propagation_algorithm_25"></a>Forward propagation algorithm</h3>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181206204948684.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYXZhbHNjeA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 这个算法是整个图的圈出来的部分：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181206205046621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYXZhbHNjeA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3><a id="Importancebased_neighborhoods_30"></a>Importance-based neighborhoods</h3>
<p>如何确定用户的邻居？<br /> 之前的GCN中是通过k-hot，<br /> 本片论文使用random work找到top T nodes<br /> 这里的方法是参考了一篇论文。</p>
<h3><a id="Stacking_convolutions_36"></a>Stacking convolutions</h3>
<p>在算法1 中使用层叠，也就是当前的点的表达用之前得到的表达，也就是点u的邻居节点不再使用初始值，而是使用通过那个邻居节点的邻居节点得到的向量来进行输入。也就是下图红色方框圈出的部分：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181206210028659.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYXZhbHNjeA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> Note that the model parameters in Algorithm 1 (Q, q, W, and w) are shared across the nodes but differ between layers.<br /> 注意，其中的参数是在同层的点中share的，不同层不share。<br /> 也就是我们要学习的参数有：<br /> (Q(k),q(k),W(k),w(k),∀k ∈ {1,…,K}) k表示第k层。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181206210521608.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYXZhbHNjeA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2><a id="model_training_45"></a>model training</h2>
<ul>
<li>目标函数max-margin ranking loss，其实就是hinge loss（svm的loss）</li>
<li>建立一个set L 里面是pair（q，i），q和i是相关的，使i被更好的推荐给query item q。也就是使用网络embedding出的q和i的向量要比较相似。</li>
</ul>
<p>loss function：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181206211046398.png" alt="在这里插入图片描述"></p>
<p>解释一下变种的hinge loss：<br /> l(y,y′)=max(0,m−y+y′)<br /> 其中，y是正样本的得分，y’是负样本的得分，m是margin（自己选一个数）<br /> 即我们希望正样本分数越高越好，负样本分数越低越好，但二者得分之差最多到m就足够了，差距增大并不会有任何奖励。<br /> 我们可以看到，如果证样本的得分较大，副样本的得分较小，在max后，得到0，相反得到m−y+y′，因为m−y+y′&gt;0，并且要min loss，所以在两个得分相差不大或者负样本得分高的时候进行更新。</p>
<h3><a id="MultiGPU_training_with_large_minibatches_58"></a>Multi-GPU training with large minibatches</h3>
<h3><a id="Producerconsumer_minibatch_construction_59"></a>Producer-consumer minibatch construction.</h3>
<h3><a id="Sampling_negative_items_60"></a>Sampling negative items</h3>
<h2><a id="Node_Embeddings_via_MapReduce_61"></a>Node Embeddings via MapReduce</h2>
<p>以上这些就不再写了，可以看论文，都是描述性的话，总体就是如何快速计算大数据。</p>
<h1><a id="_64"></a>总结</h1>
<p>我觉得这个论文的方法和我之前做推荐系统的时候用的random work+skip gram 很像。。。</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>LOj10131 暗的连锁</title>
		<link>https://uzzz.org/article/982.html</link>
				<pubDate>Thu, 06 Sep 2018 12:38:43 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[图像处理]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/982.html</guid>
				<description><![CDATA[题目描述 原题来自：POJ 3417 Dark 是一张无向图，图中有&#160;N&#160;个节点和两类边，一类边被称为主要边，而另一类被称为附加边。Dark 有&#160;N–1&#160;条主要边，并且 Dark 的任意两个节点之间都存在一条只由主要边构成的路径。另外，Dark 还有&#160;M条附加边。 你的任务是把 Dark 斩为不连通的两部分。]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<p>题目描述</p>
<p><strong>原题来自：<a href="http://poj.org/problem?id=3417" rel="nofollow" data-token="3b600b9faee153f6c1032ed51a871274">POJ 3417</a></strong></p>
<p>Dark 是一张无向图，图中有&nbsp;N&nbsp;个节点和两类边，一类边被称为主要边，而另一类被称为附加边。Dark 有&nbsp;N–1&nbsp;条主要边，并且 Dark 的任意两个节点之间都存在一条只由主要边构成的路径。另外，Dark 还有&nbsp;M条附加边。</p>
<p>你的任务是把 Dark 斩为不连通的两部分。一开始 Dark 的附加边都处于无敌状态，你只能选择一条主要边切断。一旦你切断了一条主要边，Dark 就会进入防御模式，主要边会变为无敌的而附加边可以被切断。但是你的能力只能再切断 Dark 的一条附加边。</p>
<p>现在你想要知道，一共有多少种方案可以击败 Dark。注意，就算你第一步切断主要边之后就已经把 Dark 斩为两截，你也需要切断一条附加边才算击败了 Dark。</p>
<p>输入格式</p>
<p>第一行包含两个整数&nbsp;N&nbsp;和&nbsp;M；</p>
<p>之后&nbsp;N–1&nbsp;行，每行包括两个整数&nbsp;A&nbsp;和&nbsp;B表示&nbsp;A&nbsp;和&nbsp;B&nbsp;之间有一条主要边；</p>
<p>之后&nbsp;M&nbsp;行以同样的格式给出附加边。</p>
<p>输出格式</p>
<p>输出一个整数表示答案。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>主要边是原图的一棵生成树，附加边是图中的非树边。</p>
<p>对于每条非树边（x,y），它会和树上x到y的路径构成一个环，当第一步切断x到y路径中的一条边时，第二步就需要切断非树边（x,y），才能保证原图不联通。</p>
<p>但我们第二步只能切断一条边啊，所以如果第一步的路径对应的要切断两条或以上的非树边，就没有办法了。</p>
<p>所以我们枚举每条非树边（x,y），把x到y路径上所有边的边权+1，对于每条树边，如果边权为0，则切断它之后原图已经不联通，第二条边随便切一条，共m种方案。如果边权为1，则第二步必须切断对应的那条边，方案数1，如果边权大于2则没有方案。</p>
<p>这个边权我们可以利用树上差分来维护，对于非树边（x,y），令结点x和y的点权加1，lca（x,y）的点权减2，这样每个点子树的点权和就是该点到它的父亲的边的边权。证明就不详细讲了。</p>
<p>代码：</p>
<pre class="has">
<code>#include&lt;cstdio&gt;
#include&lt;iostream&gt;
#include&lt;algorithm&gt;
#include&lt;cstring&gt;
#include&lt;cmath&gt;
#define maxn 100010
#define maxm 500010
using namespace std;
int head[maxn],f[maxn][30],dep[maxn],cnt,num[maxn],fr[maxm],t[maxm],ff[maxn];
struct edge
{
	int next;
	int to;
}e[maxm];
void insert(int u,int v)
{
	e[++cnt].next=head[u];
	head[u]=cnt;
	e[cnt].to=v;
}
void dfs(int now,int fa)
{
	dep[now]=dep[fa]+1;
	for(int i=0;i&lt;=19;i++) f[now][i+1]=f[f[now][i]][i];
	for(int i=head[now];i;i=e[i].next){
		int v=e[i].to;
		if(v==fa) continue;
		f[v][0]=now;
		dfs(v,now);
	}
}
int lca(int x,int y)
{
	if(dep[x]&lt;dep[y]) swap(x,y);
	for(int i=20;i&gt;=0;i--){
		if(dep[f[x][i]]&gt;=dep[y]) x=f[x][i];
		if(x==y) return x;
	}
	for(int i=20;i&gt;=0;i--){
		if(f[x][i]!=f[y][i]){
			x=f[x][i];
			y=f[y][i];
		}
	}
	return f[x][0];
}
void getf(int now,int fa)
{
	ff[now]+=num[now];
	for(int i=head[now];i;i=e[i].next){
		int v=e[i].to;
		if(v==fa) continue;
		getf(v,now);
		ff[now]+=ff[v];
	}
}
int main()
{
	int n,m,ans=0;
	cin&gt;&gt;n&gt;&gt;m;
	for(int i=1;i&lt;n;i++){
		int x,y;
		scanf("%d%d",&amp;x,&amp;y);
		insert(x,y);
		insert(y,x);
	}
	dfs(1,0);
	for(int i=1;i&lt;=m;i++) scanf("%d%d",&amp;fr[i],&amp;t[i]);
	for(int i=1;i&lt;=m;i++){
		num[fr[i]]++;
		num[t[i]]++;
		num[lca(fr[i],t[i])]-=2;
	}
	getf(1,0);
	for(int i=2;i&lt;=n;i++){
		if(ff[i]==0) ans+=m;
		if(ff[i]==1) ans++;
	}
	cout&lt;&lt;ans;
	
	return 0;
} </code></pre>
<p>&nbsp;</p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>图像处理经典文章合集</title>
		<link>https://uzzz.org/article/1484.html</link>
				<pubDate>Sun, 05 Aug 2018 06:09:57 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[图像处理]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1484.html</guid>
				<description><![CDATA[Colorization and Color Transfer（图像上色和颜色迁移） Semantic Colorization with Internet Images, Chia et al. SIGGRAPH ASIA 2011&#160;Color Harmonization, Cohen-Or, Sorkine, Gal, Leyvand, and]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<p>Colorization and Color Transfer（图像上色和颜色迁移）</p>
<p><a href="http://www.ece.nus.edu.sg/stfpage/eletp/Papers/sigasia11.pdf" rel="nofollow" data-token="f8f87c4efd9b2d6d7c8057eff7897dbf">Semantic Colorization with Internet Images</a>, Chia et al. SIGGRAPH ASIA 2011&nbsp;<a href="http://portal.acm.org/citation.cfm?id=1141911.1141933" rel="nofollow" data-token="97ce3cadebd4642f3cd7aabb1f9a06af">Color Harmonization</a>, Cohen-Or, Sorkine, Gal, Leyvand, and Xu.&nbsp;<a href="http://cs.nyu.edu/~sorkine/ProjectPages/Harmonization/" rel="nofollow" data-token="8ed9a00da6738df7dfa79c2a468ea36e">Web Page</a>&nbsp;<a href="http://ieeexplore.ieee.org/iel5/4408818/4408819/04409120.pdf?arnumber=4409120" rel="nofollow" data-token="215e7eddbf0f2cff0d0f2afccbdc311c">Computing the alpha-Channel with Probabilistic Segmentation for Image Colorization</a>, Dalmau-Cedeno, Rivera, and Mayorga&nbsp;<a href="http://www.kyb.mpg.de/publications/attachments/CVPR2008-Gehler_%5B0%5D.pdf" rel="nofollow" data-token="a06f6f33688f796aa8792f678e7a0272">Bayesian Color Constancy Revisited</a>, Gehler, Rother, Blake, Minka, and Sharp&nbsp;<a href="http://portal.acm.org/citation.cfm?id=1073204.1073241" rel="nofollow" data-token="554a0910f236ea67202e387fe5b55386">Color2Gray: Salience-Preserving Color Removal</a>, Gooch, Olsen, Tumblin, and Gooch&nbsp;<a href="http://portal.acm.org/citation.cfm?id=1291288" rel="nofollow" data-token="00ed884e126c62bd62010e9ce3ce7d74">Color Conceptualization</a>, Hou and Zhang&nbsp;<a href="http://people.csail.mit.edu/ehsu/work/sig08lme/lme-sig2008-sm.pdf" rel="nofollow" data-token="49c5ddc9ab713a64a024de9f068d41a3">Light Mixture Estimation for Spatially Varying White Balance</a>, Hsu, Mertens, Paris, Avidan, and Durand.&nbsp;<a href="http://people.csail.mit.edu/ehsu/work/sig08lme/" rel="nofollow" data-token="d5b961b287cfa782510af0c82c96f98e">Web Page</a>&nbsp;<a href="http://research.microsoft.com/pubs/68993/luminance_eccv04.pdf" rel="nofollow" data-token="9af0aceb26e953478bad3470277c3a9d">Bayesian Correction of Image Intensity with Spatial Consideration</a>, Jia, Sun, Tang, and Shum&nbsp;<a href="http://cg.postech.ac.kr/research/robust_c2g/kim09tog.pdf" rel="nofollow" data-token="f8bc651e341ae44e06e98246b52d426b">Robust Color-to-gray via Nonlinear Global Mapping</a>, Kim, Jang, Demouth, and Lee. SIGGRAPH Asia 2009&nbsp;<a href="http://cg.postech.ac.kr/research/robust_c2g/" rel="nofollow" data-token="78c06606b62c446680c47ea08af64dd0">Web Page</a>&nbsp;<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?isnumber=4286981&amp;arnumber=4286993" rel="nofollow" data-token="f2dcedbade2c45fbeb46072a32115bbc">Variational Models for Image Colorization via Chromaticity and Brightness Decomposition</a>, Kang and March&nbsp;<a href="http://people.csail.mit.edu/alevin/papers/colorization-siggraph04.pdf" rel="nofollow" data-token="84d086a3018d1df88ba8a00ad05f4d73">Colorization using Optimization</a>, Levin, Lischinski, and Weiss<br /><a href="http://appsrv.cse.cuhk.edu.hk/~ttwong/cgi-bin/paper-download/download.cgi?path=incolor&amp;dl=paper/incolor.pdf" rel="nofollow" data-token="2e3fa46bb9a6627752f61aae4fab1019">Intrinsic Colorization</a>, Liu et al. SIGGRAPH ASIA 2008&nbsp;<a href="http://www.cse.cuhk.edu.hk/~ttwong/papers/incolor/incolor.html" rel="nofollow" data-token="e943827509c5222c7b8f735becb50349">Web Page</a>&nbsp;<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1544887" rel="nofollow" data-token="6966ab669c7869ea9076dd3b7a19c918">N-Dimensional Probability Density Function Transfer and Its Application to Colour Transfer</a>, Pitie&nbsp;<em>et al.</em>&nbsp;<a href="http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6WCX-4MVN15C-2&amp;_user=1576494&amp;_rdoc=1&amp;_fmt=&amp;_orig=search&amp;_sort=d&amp;view=c&amp;_acct=C000053838&amp;_version=1&amp;_urlVersion=0&amp;_userid=1576494&amp;md5=af5dd852c321b239f2ebae65cc96149f" rel="nofollow" data-token="4f36faa0bbcb5deaaf5e19e14d123fd6">Automated Colour Grading using Colour Distribution Transfer</a>, Pitie&nbsp;<em>et al.</em>&nbsp;<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1530560" rel="nofollow" data-token="53841b5138e8f4d56eab18221143dd93">Color by Linear Neighborhood Embedding</a>, Qiu and Guan&nbsp;<a href="http://portal.acm.org/citation.cfm?id=1141911.1142017" rel="nofollow" data-token="9add234df401c789e272af64d62437a6">Manga Colorization</a>, Qu, Wong, and Heng&nbsp;<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=946629" rel="nofollow" data-token="c271c668c470703d36d232785ccba130">Color Transfer between Images</a>, Reinhard, Ashikhmin, Gooch, and Shirley&nbsp;<a href="http://www.cse.cuhk.edu.hk/~leojia/all_final_papers/color_cvpr05.PDF" rel="nofollow" data-token="90058883b0e9a6147f6405156336c7ef">Local Color Transfer via Probabilistic Segmentation by Expectation-Maximization</a>, Tai, Jia, and Tang<br /><a href="http://appsrv.cse.cuhk.edu.hk/~ttwong/cgi-bin/paper-download/download.cgi?path=colormood&amp;dl=colormood.pdf" rel="nofollow" data-token="8bad7ec73ab49bcd5afb96e741b4e99e">Data-Driven Image Color Theme Enhancement</a>, Wang, Yu, Wong, Chen, and Xu. SIGGRAPH Asia 2010&nbsp;<a href="http://www.cse.cuhk.edu.hk/~ttwong/papers/colormood/colormood.html" rel="nofollow" data-token="a17605e4cd044f361f09c7cd11e0cd9a">Web Page</a>&nbsp;<a href="http://portal.acm.org/citation.cfm?id=1128923.1128974" rel="nofollow" data-token="9a6f30abc0165918f2d266e5073dc33e">Color Transfer in Correlated Color Space</a>, Xiao and Ma&nbsp;<a href="http://ieeexplore.ieee.org/iel5/83/33959/01621234.pdf" rel="nofollow" data-token="dad9b66e69565a7fe4722bf0a3a0a2eb">Fast Image and Video Colorization using Chrominance Blending</a>, Yatziv and Sapiro</p>
<p><a name="t1"></a><a name="TOC-Texture-Synthesis-and-Inpainting"></a>Texture Synthesis and Inpainting（纹理和成和修复）</p>
<p><a href="http://dl.acm.org/citation.cfm?id=1276390" rel="nofollow" data-token="bff384afdca046f9ada9f2baf7813d63">Seam Carving for Content-Aware Image Resizing</a>, Avidan and Shamir.&nbsp;<a href="http://en.wikipedia.org/wiki/Seam_carving" rel="nofollow" data-token="c314dc10f29a553bdac5940d213f7713">Wikipedia</a>&nbsp;<a href="http://www.cs.utah.edu/~michael/ts/ts.pdf" rel="nofollow" data-token="6b80c9db08a712631f9365e8504659a1">Synthesizing Natural Textures</a>, Ashikhmin&nbsp;<a href="http://www.cs.princeton.edu/gfx/pubs/Barnes_2009_PAR/patchmatch.pdf" rel="nofollow" data-token="7a1360b70044d27caed8311de841c233">PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing</a>, Barnes, Shechtman, Finkelstein, and Goldman. SIGGRAPH 2009.&nbsp;<a href="http://www.cs.princeton.edu/gfx/pubs/Barnes_2009_PAR/index.php" rel="nofollow" data-token="a302483e9be884bc97d3af0ba8efbe72">Web Page</a>&nbsp;<a href="http://www.iua.upf.es/~mbertalmio/bertalmi.pdf" rel="nofollow" data-token="4d90c2c8a4ea698d0d267de7923c4725">Image Inpainting</a>, Bertalmio, Sapiro, Caselles, and Ballester&nbsp;<a href="http://artis.imag.fr/Publications/2007/BNTS07/" rel="nofollow" data-token="c087b296ad25b0453e0b73a2a5c57c93">Video Watercolorization using Bidirectional Texture Advection</a>, Bousseau, Neyret, Thollot, and Salesin<br /><a href="http://appsrv.cse.cuhk.edu.hk/~ttwong/cgi-bin/paper-download/download.cgi?path=camouflage&amp;dl=paper/camouflage.pdf" rel="nofollow" data-token="5d254644714262c0978f59a358304466">Camouflage Images</a>, Chu et al. SIGGRAPH 2010&nbsp;<a href="http://www.cse.cuhk.edu.hk/~ttwong/papers/camouflage/camouflage.html" rel="nofollow" data-token="fb6f1c0e7d73bbd87fd6c8ea095636dd">Web Page</a>&nbsp;<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1211538" rel="nofollow" data-token="6f8f6df8aa972449285ab4ad0e02f4db">Object Removal by Exemplar-Based Inpainting</a>, Criminisi, Perez, and Toyama&nbsp;<a href="http://liama.ia.ac.cn/wiki/user:dong:home" rel="nofollow" data-token="8623f534dffd77ccbef856022db3c723">Weiming DONG&#8217;s web page</a>&nbsp;contains useful information about texture synthesis and image resizing&nbsp;<a href="http://graphics.cs.cmu.edu/people/efros/research/quilting/quilting.pdf" rel="nofollow" data-token="f844698cfd8f2b5703999f102fa43fa5">Image Quilting for Texture Synthesis and Transfer</a>, Efros and Freeman&nbsp;<a href="http://graphics.cs.cmu.edu/people/efros/research/NPS/efros-iccv99.pdf" rel="nofollow" data-token="b0b357f264e01a2212421d1833e1a1cb">Texture Synthesis by Non-parametric Sampling</a>, Efros and Leung&nbsp;<a href="http://graphics.cs.uiuc.edu/~jch/papers/rototexture.pdf" rel="nofollow" data-token="4e6aaaf9e6957e2ba224c98c454ec754">RotoTexture: Automated Tools for Texturing Raw Video</a>, Fang and Hart&nbsp;<a href="http://graphics.cs.uiuc.edu/~jch/papers/textureshop.pdf" rel="nofollow" data-token="8234044fca5ca65b607ee27e19808912">Textureshop: Texture Synthesis as a Photograph Editing Tool</a>, Fang and Hart&nbsp;<a href="http://www1.cs.columbia.edu/~charhan/tex.pdf" rel="nofollow" data-token="cdb8c35c5c74f55842ce8469f11427f3">Multiscale Texture Synthesis</a>, Han, Risser, Ramamoorthi, and Grinspun&nbsp;<a href="http://graphics.cs.cmu.edu/projects/scene-completion/" rel="nofollow" data-token="8c051283a440ea1f05822b3c06916a36">Scene Completion Using Millions of Photographs</a>, Hays and Efros&nbsp;<a href="http://mrl.nyu.edu/publications/image-analogies/analogies-72dpi.pdf" rel="nofollow" data-token="3490b330945c344a0afc172a27f95cf7">Image Analogies</a>, Hertzmann, Jacobs, Oliver, Curless, and Salesin&nbsp;<a href="http://www.cc.gatech.edu/cpl/projects/graphcuttextures/gc-final-lowres.pdf" rel="nofollow" data-token="3e76dfe4960ce9e402d947b336edea81">Graphcut Textures: Image and Video Synthesis Using Graph Cuts</a>, Kwatra , Schodl , Essa , Turk, and Bobick&nbsp;<a href="http://www.faculty.idc.ac.il/arik/papers/vidRet.pdf" rel="nofollow" data-token="26866871a1e58cff2c837b32c8eeba1d">Improved Seam Carving for Video Retargeting</a>, Rubinstein, Shamir, and Avidan.&nbsp;<a href="http://www.faculty.idc.ac.il/arik/VidRet.mov" rel="nofollow" data-token="ee8598c77df876c61abe0eba68905f54">Video</a>&nbsp;<a href="http://www.faculty.idc.ac.il/arik/SCWeb/multiop/multiop-lowres.pdf" rel="nofollow" data-token="94ffe85eba383c885edc6e2ad1b4dbdf">Multi-operator Media Retargeting</a>, Rubinstein, Shamir, and Avidan. SIGGRAPH 2009.&nbsp;<a href="http://www.faculty.idc.ac.il/arik/SCWeb/multiop/index.html" rel="nofollow" data-token="5be5d87fa1ad10e63d31dae14ac38d47">Web Page</a>&nbsp;<a href="http://www.cs.brown.edu/~black/Papers/cvpr2005.pdf" rel="nofollow" data-token="d021b2874d6a9957d90874a8b0eadd5e">Fields of Experts: A Framework for Learning Image Priors</a>, Roth and Black&nbsp;<a href="http://www-cvpr.iai.uni-bonn.de/pub/pub/schoenemann_et_al_iccv09.pdf" rel="nofollow" data-token="00aefb255253892ef261b3fb2171c21c">Curvature Regularity for Region-based Image Segmentation and Inpainting: A Linear Programming Relaxation</a>, Schoenemann, Kahl, and Cremers. ICCV 2009.&nbsp;<a href="http://graphics.stanford.edu/papers/texture-synthesis-sig00/texture.pdf" rel="nofollow" data-token="c6405b61671998bb5d9c803c491599c7">Fast Texture Synthesis using Tree-structured Vector Quantization</a>, Wei and Levoy&nbsp;<a href="http://www.cs.tau.ac.il/~wolf/papers/retargeting173.pdf" rel="nofollow" data-token="b80bcfb5aff6faa9f16e5cb76e9b9760">Non-homogeneous Content-driven Video-retargeting</a>, Wolf, Guttmann, and Cohen-Or&nbsp;<a href="http://portal.acm.org/citation.cfm?id=1015730" rel="nofollow" data-token="3e0029e28ad84acf556248042b06ebc2">Feature Matching and Deformation for Texture Synthesis</a>, Wu and Yu</p>
<p><a name="t2"></a><a name="TOC-HDR-and-Tone-Mapping"></a>HDR and Tone Mapping（高动态范围成像和色调映射）</p>
<p><a href="http://www.coolhall.com/homepage/pubs/hdrdisp_eval/hdrdisp_project.html" rel="nofollow" data-token="b916c49ec277c246a6d082ad23fd2ca2">Do HDR Displays Support LDR Content? A Psychophysical Evaluation</a>, Akyu&#8221;z, Reinhard, Fleming, Riecke, Bu&#8221;lthoff&nbsp;<a href="http://people.csail.mit.edu/soonmin/CV/bw_photo_ring_toss_standard.pdf" rel="nofollow" data-token="5d46ac587734f840f4535f7c5ee7d7a0">Two-scale Tone Management for Photographic Look</a>, Bae, Paris, and Durand&nbsp;<a href="http://groups.csail.mit.edu/graphics/bilagrid/" rel="nofollow" data-token="17f9e9c9286a9bb0a2f4e96304d419f3">Real-time Edge-Aware Image Processing with the Bilateral Grid</a>, Chen, Paris, Durand&nbsp;<a href="http://www.debevec.org/Research/HDR/debevec-siggraph97.pdf" rel="nofollow" data-token="c699caa04b556dfbebb19e7f2be2be5e">Recovering High Dynamic Range Radiance Maps from Photographs</a>, Debevec and Malik&nbsp;<a href="http://people.csail.mit.edu/fredo/PUBLI/Siggraph2002/DurandBilateral.pdf" rel="nofollow" data-token="39e8387def29154dcd89604b6834471d">Fast Bilateral Filtering for the Display of High-Dynamic-Range Images</a>, Durand and Dorsey&nbsp;<a href="http://www.cs.huji.ac.il/~danix/epd/epd-small.pdf" rel="nofollow" data-token="97fe1a7d4ba4ca8c104fa882388c9e6f">Edge-Preserving Decompositions for Multi-Scale Tone and Detail Manipulation</a>, Farbman, Fattal, Lischinski, and Szeliski. SIGGRAPH 2009.&nbsp;<a href="http://www.cs.huji.ac.il/~danix/epd/index.html" rel="nofollow" data-token="d34a0f29df888e7035a3adb78c8261dd">Web Page</a><br /><a href="http://www.mpi-inf.mpg.de/~granados/papers/granados10_opthdr.pdf" rel="nofollow" data-token="76b74768c9d4ae0051add74b7919b60d">Optimal HDR reconstruction with linear digital cameras</a>, Granados et al., CVPR 2010.&nbsp;<a href="http://www.cs.huji.ac.il/~danix/hdr/hdrc.pdf" rel="nofollow" data-token="2fcf7381f2c16df50085f5b194934022">Gradient Domain High Dynamic Range Compression</a>, Fattal, Lischinski, and Werman&nbsp;<a href="http://web4.cs.ucl.ac.uk/staff/M.Kim/publications/KimWeyKautz_SG2009s.pdf" rel="nofollow" data-token="e14c9d8057376054657d0189bc3f643a">Modeling Human Color Perception under Extended Luminance Levels</a>, Kim, Weyrich, and Kautz. SIGGRAPH 2009.&nbsp;<a href="http://web4.cs.ucl.ac.uk/staff/M.Kim/publications/siggraph2009/sig2009.html" rel="nofollow" data-token="2248cbd6991831153ab58bf0954722ac">Web Page</a>&nbsp;<a href="http://graphics.berkeley.edu/papers/Kirk-PBT-2011-08/Kirk-PBT-2011-08.pdf" rel="nofollow" data-token="efba0af9675d63a2f22aca8595c160a2">Perceptually Based Tone Mapping for Low-Light Conditions</a>, Kirk and O&#8217;Brien. SIGGRAPH 2011.&nbsp;<a href="http://graphics.berkeley.edu/papers/Kirk-PBT-2011-08/index.html" rel="nofollow" data-token="d45b1fd1bbde65667e1f69dceb001b3c">Web Page</a>&nbsp;<a href="http://web.mit.edu/yzli/www/hdr_companding.htm" rel="nofollow" data-token="39d6c70202d024ee8049f74ea686e0f9">Compressing and Companding High Dynamic Range Images with Subband Architectures</a>, Li, Sharan, and Adelson&nbsp;<a href="http://research.microsoft.com/users/stevelin/color.pdf" rel="nofollow" data-token="61e24ca72e756cb0759a759d26c4a2f6">Radiometric Calibration Using a Single Image</a>&nbsp;Lin, Gu, Yamazaki, and Shum&nbsp;<a href="http://research.microsoft.com/users/stevelin/grayscale.pdf" rel="nofollow" data-token="72b361f4ff65ff8523ccee9ba8d74ac4">Determining the Radiometric Response Function from a Single Grayscale Image</a>, Lin and Zhang&nbsp;<a href="http://www.cs.huji.ac.il/~danix/itm/itm.pdf" rel="nofollow" data-token="ae60a2e0f5334d70ddfe2ed1e280d5a6">Interactive Local Adjustment of Tonal Values</a>, Lischinski, Farbman, Uyttendaele, and Szeliski.&nbsp;<a href="http://www.cs.huji.ac.il/~danix/itm/" rel="nofollow" data-token="c747c80acf260a74679ab97e203f6deb">Web Page</a>&nbsp;<a href="http://research.edm.uhasselt.be/~tmertens/papers/exposure_fusion_reduced.pdf" rel="nofollow" data-token="2e4d407ce19c7e3385e89ad96c54bd47">Exposure Fusion</a>, Mertens, Kautz, Van Reeth&nbsp;<a href="http://www1.cs.columbia.edu/CAVE//publinks/mitsunaga_CVPR_1999.pdf" rel="nofollow" data-token="5d882694b372da8e66826f43c336a8ef">Radiometric Self Calibration</a>, Mitsunaga and Nayar&nbsp;<a href="http://www.cs.utah.edu/~reinhard/cdrom/tonemap.pdf" rel="nofollow" data-token="fa4f545d99fd6b56cd7983fdb1932867">Photographic Tone Reproduction for Digital Images</a>, Reinhard, Stark, Shirley and Ferwerda&nbsp;<a href="http://www.cs.ubc.ca/labs/imager/tr/2007/Rempel_Ldr2Hdr/" rel="nofollow" data-token="4e01214da00e4611a4f9835ff3484124">Ldr2Hdr: On-the-Fly Reverse Tone Mapping of Legacy Video and Photographs</a>, Rempel, Trentacoste, Seetzen, Young, Heidrich, Whitehead, and Ward&nbsp;<a href="http://research.microsoft.com/apps/pubs/default.aspx?id=69433" rel="nofollow" data-token="0171b8f71b5f564206a928f3b706973d">High Dynamic Range Image Hallucination</a>, Wang, Wei, Zhou, Guo, and Shum&nbsp;<a href="http://www.anyhere.com/gward/papers/jgtpap2.pdf" rel="nofollow" data-token="c44ecc0dd29a42503606401c840c2a44">Fast, Robust Image Registration for Compositing High Dynamic Range Photographs from Hand-Held Exposures</a>, Ward<a name="9"></a></p>
<p><a name="t3"></a><a name="TOC-Intrinsic-Images"></a>Intrinsic Images（本征图像）</p>
<p><a href="http://www1.cs.columbia.edu/CAVE/publications/pdfs/Agarwal_TOG05.pdf" rel="nofollow" data-token="d0d67120d9e3f53d174dec1dca542e18">Removing Photography Artifacts using Gradient Projection and Flash-Exposure Sampling</a>, Agrawal, Raskar, Nayar, and Li&nbsp;<a href="http://artis.imag.fr/Publications/2009/BPD09/intrinsic_main.pdf" rel="nofollow" data-token="598ff04afd7b42c85237bdf09ef756a4">User-Assisted Intrinsic Images</a>, Bousseau, Paris, and Durand. SIGGRAPH Asia 2009.&nbsp;<a href="http://artis.imag.fr/Publications/2009/BPD09/" rel="nofollow" data-token="63aa3ae23b136f32f8e6f26405f4e4bf">Web Page</a>&nbsp;<a href="http://people.csail.mit.edu/fredo/PUBLI/flash/flash.pdf" rel="nofollow" data-token="af8262be6202ad36e392c2e1a2f49248">Flash Photography Enhancement via Intrinsic Relighting</a>, Eisemann and Durand&nbsp;<a href="http://www.merl.com/reports/docs/TR98-05.pdf" rel="nofollow" data-token="a1ec35ae712611c0cba828a94d7f9b80">Bayesian Model of Surface Perception</a>, Freeman and Viola&nbsp;<a href="http://infoscience.epfl.ch/record/111884/files/FFD_ICCV.pdf" rel="nofollow" data-token="532b056a23bc7b94d29c241e533f8d3e">Detecting Illumination in Images</a>, Finlayson, Fredembach, and Drew&nbsp;<a href="http://people.csail.mit.edu/rgrosse/iccv09-intrinsic.pdf" rel="nofollow" data-token="1e165cd7960087ebfeb1e3338a9f2aed">Ground Truth Dataset and Baseline Evaluations for Intrinsic Image Algorithms</a>,&nbsp;Grosse, Johnson, Adelson, and Freeman. ICCV 2009.&nbsp;<a href="http://www.cs.technion.ac.il/~ron/PAPERS/retinex_ijcv2003.pdf" rel="nofollow" data-token="7e0010a14cb7d0376ec7114adb256a40">A Variational Framework for Retinex</a>, Kimmel, Elad, Shaked, Keshet, and Sobel&nbsp;<a href="http://cs.nyu.edu/~fergus/papers/dark_flash.pdf" rel="nofollow" data-token="15891f31e285d477ade0f25850d9fcf8">Dark Flash Photography</a>, Krishnan amd Fergus. SIGGRAPH 2009.&nbsp;<a href="http://www.cs.nyu.edu/~fergus/research/dark_flash.html" rel="nofollow" data-token="da6462d99a92d1bbb7511ad0a3722599">Web Page</a>&nbsp;<a href="http://www.opticsinfobase.org/abstract.cfm?id=54240" rel="nofollow" data-token="6f29e353459459dbb65ea4aa1788cf58">Lightness and Retinex Theory</a>, Land and McCann&nbsp;<a href="http://research.microsoft.com/en-us/people/yasumat/intrinsicseq_eccv04.pdf" rel="nofollow" data-token="f7aea2e92d970a3f902cdbb3e32205d9">Estimating Intrinsic Images from Image Sequenceswith Biased Illumination</a>, Matsushita, Lin, Kang, Shum. ECCV 2004&nbsp;<a href="http://gl.ict.usc.edu/Research/PFPR/" rel="nofollow" data-token="3ca64e2120559c8c4ac0c2071fea11ee">Post-production Facial Performance Relighting using Reflectance Transfer</a>, Peers, Tamura, Matusik, and Debevec&nbsp;<a href="http://research.microsoft.com/users/stevelin/HL-texture.pdf" rel="nofollow" data-token="faba7da1db2e6d87d8095492a298b42d">Separation of Highlight Reflections from Textured Surfaces</a>, Tan, Lin, and Quan&nbsp;<a href="http://people.csail.mit.edu/mtappen/pami.pdf" rel="nofollow" data-token="148268f5f574399aac2cdd39c9b71b1a">Recovering Intrinsic Images from a Single Image</a>, Tappen, Freeman, and Adelson&nbsp;<a href="http://people.csail.mit.edu/mtappen/cvpr06-distrib.pdf" rel="nofollow" data-token="9bd3bd90b1e03f8cba8425706412a696">Estimating Intrinsic Component Images using Non-Linear Regression</a>, Tappen, Adelson, and Freeman&nbsp;<a href="http://www.cs.huji.ac.il/~yweiss/iccv01.pdf" rel="nofollow" data-token="095658e83ec6667ed3af07a954016441">Deriving Intrinsic Images from Image Sequences</a>, Weiss<a name="13"></a><a name="14"></a></p>
<p><a name="t4"></a><a name="TOC-Deblurring-Denoising-and-Super-Resolution"></a>Deblurring, Denoising, and Super-Resolution（图像去模糊，去噪和超分辨率）</p>
<p><a href="http://www.umiacs.umd.edu/~aagrawal/eg10/AgrawalEurographics10LowRes.pdf" rel="nofollow" data-token="704c4da68ea908b2b20cc66315aa5c3c">Reinterpretable Imager: Towards Variable Post Capture Space, Angle &amp; Time Resolution in Photography</a>, Agrawal, Veeraraghavan, and Raskar. Eurographics 2010.&nbsp;<a href="http://www.umiacs.umd.edu/~aagrawal/sig09/AgrawalSIGGRAPH09VideoMotionDeblurring.pdf" rel="nofollow" data-token="1e58e73deabec024c34c161b1202b319">Invertible Motion Blur in Video</a>, Agrawal, Xu, and Raskar. SIGGRAPH 2009.&nbsp;<a href="http://www.umiacs.umd.edu/~aagrawal/cvpr09/OptimalCapture/AgrawalCVPR09OptimalCapture.pdf" rel="nofollow" data-token="c9b258a040549fb4549b3ba6e24b32ff">Optimal Single Image Capture for Motion Deblurring</a>, Agrawal and Raskar. CVPR 2009.&nbsp;<a href="http://www.umiacs.umd.edu/~aagrawal/cvpr09/OptimalCodes/AgrawalCVPR09OptimalCodes.pdf" rel="nofollow" data-token="fd5c14c299b6ad7ec1cf49c9ee7f37af">Coded Exposure Deblurring: Optimized Codes for PSF Estimation and Invertibility</a>, Agrawal and Xu. CVPR 2009.&nbsp;<a href="http://bengal.missouri.edu/~kes25c/nl2.pdf" rel="nofollow" data-token="285b695ca131869d379575cc5fd21e29">A Non-local Algorithm for Image Denoising</a>, Buades, Coll, and Morel.&nbsp;<a href="http://www.eecs.harvard.edu/~zickler/papers/svblur_CVPR2010.pdf" rel="nofollow" data-token="649a8bf705596391e95819d695b2b5ff">Analyzing Spatially-varying Blur</a>, Chakrabarti, Zickler, and Freeman. CVPR 2010.&nbsp;<a href="http://rosaec.snu.ac.kr/publish/2009/ID/ChLe-SIGGRAPH-2009.pdf" rel="nofollow" data-token="32e244d0144266d32c1f19026d2669d2">Fast Motion Deblurring</a>, Cho and Lee. SIGGRAPH Asia 2009.&nbsp;<a href="http://cg.postech.ac.kr/research/fast_motion_deblurring/" rel="nofollow" data-token="3071197d4d7d7b872c5b01d651b8b9b5">Web Page</a>&nbsp;<a href="http://people.csail.mit.edu/taegsang/Documents/twoParab_ICCP.pdf" rel="nofollow" data-token="7346bd5fbd6b3d7d71688d9d1d90b4d2">Motion Blur Removal with Orthogonal Parabolic Exposures</a>, Cho, Levin, Durand, and Freeman. CVPR 2010.&nbsp;<a href="http://people.csail.mit.edu/taegsang/OrthoExposure.html" rel="nofollow" data-token="41d19497c6b0ff038000a6b0850adeb7">Web Page</a>&nbsp;<a href="http://cg.postech.ac.kr/research/deconv_outliers/deconv_outliers.pdf" rel="nofollow" data-token="4bf642bdcdfa450ff45dbd5a9c27fd55">Handling Outliers in Non-Blind Image Deconvolution</a>, Cho, Wang, and Lee. ICCV 2011.&nbsp;<a href="http://cg.postech.ac.kr/research/deconv_outliers/" rel="nofollow" data-token="639a0b0b735a6242e93db86084f92b08">Web Page</a>&nbsp;<a href="http://portal.acm.org/citation.cfm?doid=1477926.1477935" rel="nofollow" data-token="093361f58e26f72104358ed5d26dc17b">Display supersampling</a>, Damera-Venkata and Chang&nbsp;<a href="http://www.cs.huji.ac.il/~raananf/projects/upsampling/upsampling.html" rel="nofollow" data-token="89e13824a431d197730d5586f623aadb">Image Upsampling Via Imposed Edge Statistics</a>, Fattal&nbsp;<a href="http://www.cs.huji.ac.il/~raananf/papers/defog.pdf" rel="nofollow" data-token="4a87ae8de6e3c29763145d6cfd1736e3">Single Image Dehazing</a>, Fattal. &nbsp;&nbsp;&nbsp;<a href="http://www.cs.huji.ac.il/~raananf/projects/defog/index.html" rel="nofollow" data-token="776a0824bce12639caae3dc5ccba2a5b">Web Page</a>&nbsp;&nbsp;&nbsp;<a href="http://www.cs.huji.ac.il/~raananf/projects/defog/dehaze.m" rel="nofollow" data-token="48dd704045317d98b0e4b39d9b853266">Demo Code</a>&nbsp;<a href="http://www.cs.huji.ac.il/~raananf/projects/mlic/mlic.html" rel="nofollow" data-token="8d852bf04179960de1be74f331c509e8">Multiscale Shape and Detail Enhancement from Multi-light Image Collections</a>, Fattal, Agrawala, and Rusinkiewicz&nbsp;<a href="http://people.csail.mit.edu/~fergus/papers/deblur_fergus.pdf" rel="nofollow" data-token="70f0803e739ad7244719898d31476514">Removing Camera Shake from a Single Image</a>, Fergus, Singh, Hertzmann, Roweis, and Freeman&nbsp;<a href="http://people.csail.mit.edu/billf/papers/cgasres.pdf" rel="nofollow" data-token="2535975365032a69608eb7febf9a65f0">Example-Based Super-Resolution</a>, Freeman, Jones, and Pasztor&nbsp;<a href="https://sites.google.com/site/nthuaip2012/goog_480748349" rel="nofollow" data-token="84d2448efb706fe69866953c5954029c">Space-Variant Single-Image Blind Deconvolution&nbsp;</a><a href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/NIPS2010-Harmeling_6769%5B0%5D.pdf" rel="nofollow" data-token="4153b32cc1e0346ec290987318ebc196">for Removing Camera Shake</a>, Harmeling, Hirsch, and Scholkopf&nbsp;<a href="http://www.kyb.tuebingen.mpg.de/fileadmin/user_upload/files/publications/ICIP2010-Harmeling_6673%5B0%5D.pdf" rel="nofollow" data-token="8041892e9f72a41a043c8a9e5624717c">Multiframe Blind Deconvolution, Super-Resolution, and Saturation Correction via Incremental EM</a>, Harmeling, Sra, Hirsch, and Scholkopf&nbsp;<a href="http://research.microsoft.com/en-us/um/people/jiansun/papers/dehaze_cvpr2009.pdf" rel="nofollow" data-token="e2678abd9dd539d43f7f3254697ad71b">Single Image Haze Removal Using Dark Channel Prior</a>, He, Sun, Tang. CVPR 2009.&nbsp;<a href="http://research.microsoft.com/en-us/um/redmond/groups/ivm/twocolordeconvolution/two_color_deconvolution.pdf" rel="nofollow" data-token="d2c09727cc71c85b52e87e42bb303530">Image Deblurring and Denoising using Color Priors</a>, Joshi, Zitnick, Szeliski, and Kriegman. CVPR 2009.&nbsp;<a href="http://research.microsoft.com/en-us/um/people/neel/two_color_deconvolution/" rel="nofollow" data-token="b973a4d86883c76590751f5d3484ff54">Web Page</a>&nbsp;<a href="http://research.microsoft.com/en-us/um/redmond/groups/ivm/imudeblurring/imu_deblurring.pdf" rel="nofollow" data-token="2609b9f24d1f134501f957d07bd86c72">Image Deblurring using Inertial Measurement Sensors</a>, Joshi, Kang, Zitnick, and Szeliski. SIGGRAPH 2010.&nbsp;<a href="http://research.microsoft.com/en-us/um/redmond/groups/ivm/imudeblurring/" rel="nofollow" data-token="016ab139000cd05ea16b07f7c9f02a95">Web Page</a>&nbsp;<a href="http://johanneskopf.de/publications/jbu/paper/FinalPaper_0185.pdf" rel="nofollow" data-token="98170c497ee4c40a6a644bd05935d2d1">Joint Bilateral Upsampling</a>, Kopf, Cohen, Lischinski, Uyttendaele&nbsp;<a href="http://cs.nyu.edu/~dilip/research/papers/priors_cvpr11.pdf" rel="nofollow" data-token="96d145f70ec8dd0937d188467ca92014">Blind Deconvolution using a Normalized Sparsity Measure</a>,&nbsp;Krishnan, Tay, and Fergus. CVPR 2011.&nbsp;<a href="http://cs.nyu.edu/~dilip/research/blind-deconvolution/" rel="nofollow" data-token="43168b1913c746b89d05683a53fcea5a">Web Page</a>&nbsp;<a href="http://people.csail.mit.edu/alevin/papers/levin-deblurring-nips06.pdf" rel="nofollow" data-token="7b39de8faa408072b420b8e2590b4d4a">Blind Motion Deblurring Using Image Statistics</a>, Levin&nbsp;<a href="http://groups.csail.mit.edu/graphics/CodedAperture/" rel="nofollow" data-token="a0af75c3317caa73d7eeabe3e38962e9">Image and Depth from a Conventional Camera with a Coded Aperture</a>, Levin, Fergus, Durand, Freeman&nbsp;<br /><a href="http://groups.csail.mit.edu/graphics/CodedAperture/SparseDeconv-LevinEtAl07.pdf" rel="nofollow" data-token="364ef1c86a2ab74e067c0b3a741b2837">Sparse Deconvolution</a>&nbsp;<a href="http://www.wisdom.weizmann.ac.il/~levina/papers/lattice/LevinEtAL09-lattice.pdf" rel="nofollow" data-token="373f542a6ed0d3d24bdf353468b1d2dd">4D Frequency Analysis of Computational Cameras for Depth of Field Extension</a>, Levin, Hasinoff, Green, Durand, and Freeman. SIGGRAPH 2009.&nbsp;<a href="http://www.wisdom.weizmann.ac.il/~levina/papers/lattice/" rel="nofollow" data-token="8660ab78e499da624c45663b0d73c91e">Web Page</a>&nbsp;<a href="http://groups.csail.mit.edu/graphics/pubs/MotionInvariant/MotInv-s-LevinEtAl-SIGGRAPH08.pdf" rel="nofollow" data-token="2ad78f0b0b1082df84930f4b32d503b7">Motion-Invariant Photography</a>, Levin, Sand, Cho, Durand, Freeman. SIGGRAPH 2008.&nbsp;<a href="http://groups.csail.mit.edu/graphics/pubs/MotionInvariant/" rel="nofollow" data-token="b478ca07156de4dcd4e3f2cc0b7a4d0d">Web Page</a>&nbsp;<a href="http://people.csail.mit.edu/celiu/denoise/estnoise/noise.pdf" rel="nofollow" data-token="f1230447a84b87f2a286e8d001f26700">Noise Estimation from a Single Image</a>, Liu, Freeman, Szeliski, and Kang&nbsp;<a href="http://rivit.cs.byu.edu/morse/pubs/LSImageReconstruction.pdf" rel="nofollow" data-token="bb28cbb9d18c83a5f8cc87dfe806f535">Image Magnification Using Level-Set Reconstruction</a>, Morse and Schwartzwald&nbsp;<a href="http://www.robots.ox.ac.uk/~sjrob/Pubs/pickup06aNIPS.pdf" rel="nofollow" data-token="6728b239639c521a42a0a953a3df3d8b">Bayesian Image Super-Resolution, Continued</a>, Pickup, Capely, Roberts, and Zisserman&nbsp;<a href="http://www.cse.cuhk.edu.hk/~leojia/projects/upsampling/upsampling_sa08.pdf" rel="nofollow" data-token="72c7dcdf579911e8980bbb5f7b583e91">Fast Image/Video Upsampling</a>, Shan, Li, Jia, and Tang.&nbsp;<a href="http://www.cse.cuhk.edu.hk/~leojia/projects/upsampling/index.html" rel="nofollow" data-token="1245bead652095f2e54b72a4e58d33c4">Web Page</a>&nbsp;<a href="http://www.cse.cuhk.edu.hk/~leojia/projects/motion_deblurring/deblur_siggraph08.pdf" rel="nofollow" data-token="78523ea4f2d6087b39c6658035d64d69">High-quality Motion Deblurring from a Single Image</a>, Shan, Jia, and Argarwala.&nbsp;<a href="http://www.cse.cuhk.edu.hk/~leojia/projects/motion_deblurring/index.html" rel="nofollow" data-token="b878b6c8e24072f2fa0510bf4dfc5b26">Web Page</a>&nbsp;<a href="http://research.microsoft.com/~jiansun/papers/GradientSR_CVPR08.pdf" rel="nofollow" data-token="40527e19e4ee3237e0a703475fc2f8ae">Image Super-resolution using Gradient Profile Prior</a>, Sun, Sun, Xu, and Shum.&nbsp;<a href="http://research.microsoft.com/~jiansun/papers/GradientSR_CVPR08.pdf" rel="nofollow" data-token="40527e19e4ee3237e0a703475fc2f8ae">Deblurring Using Regularized Locally-Adaptive Kernel Regression</a>,&nbsp;Takeda, Farsiu, and Milanfar.&nbsp;<a href="http://users.soe.ucsc.edu/~htakeda/AKTV.htm" rel="nofollow" data-token="f23dc80e0464abcbc5c25efe55807e20">Web Page</a>&nbsp;<a href="http://www.cse.ucsc.edu/~milanfar/KernelRegression_Final.pdf" rel="nofollow" data-token="7468b71463d4cb50d59c87b5879c2ee5">Kernel Regression for Image Processing and Reconstruction</a>, Takeda, Farsiu, Milanfar.&nbsp;<a href="http://users.soe.ucsc.edu/~htakeda/KernelToolBox.htm" rel="nofollow" data-token="b03315999717d367b5c380a6592114d9">Web Page</a>&nbsp;<a href="http://people.csail.mit.edu/mtappen/iccv-sctv.pdf" rel="nofollow" data-token="31227bcf8015023593fafdd6b22c6a3d">Exploiting the Sparse Derivative Prior for Super-Resolution and Image Demosaicing</a>, Tappen, Russell, and Freeman&nbsp;<a href="http://research.microsoft.com/~cmbishop/downloads/Bishop-NIPS02-SuperRes.pdf" rel="nofollow" data-token="1e13b5b3e789f3f7bbc0938afd21c7f1">Bayesian Image Super-Resolution</a>, Tipping and Bishop&nbsp;<a href="http://www.di.ens.fr/willow/pdfs/cvpr10d.pdf" rel="nofollow" data-token="92b86f7304619fffb0d27ad48c12bd5b">Non-uniform Deblurring for Shaken Images</a>, Whyte, Sivic, Zisserman, and Ponce. CVPR 2010&nbsp;<a href="http://www.di.ens.fr/~josef/publications/whyte11.pdf" rel="nofollow" data-token="72700c6299f741ea0b248933666f913f">Deblurring Shaken and Partially Saturated Images</a>, Whyte, Sivic, and Zisserman. ICCP 2012&nbsp;<a href="http://www.ifp.illinois.edu/~jyang29/papers/manuscript_two_column.pdf" rel="nofollow" data-token="f442a6ea1c4ca2498167153035c1e46d">Image Super-Resolution via Sparse Representation</a>, Yang, Wright, Huang, and Ma&nbsp;<br /><a href="http://www.ifp.illinois.edu/~jyang29/papers/CVPR08-SR.pdf" rel="nofollow" data-token="421f47b9197a5fee96fd84351b66cd92">Image Super-resolution as Sparse Representation of Raw Image Patches</a>&nbsp;<a href="http://www.ifp.illinois.edu/~jyang29/codes/CVPR08-SR.rar" rel="nofollow" data-token="00dc74165f9eba29f34b8599dcc8270b">Code</a>&nbsp;<a href="http://research.microsoft.com/~jiansun/papers/Deblurring_SIGGRAPH07.pdf" rel="nofollow" data-token="e1b7af27cdf8fc9f0609574f0d20b9dc">Image Deblurring with Blurred/Noisy Image Pairs</a>, Yuan, Sun, Quan, and Shum&nbsp;<a href="http://research.microsoft.com/~jiansun/papers/ImageDeconv_Siggraph08.pdf" rel="nofollow" data-token="6b3a4d4a85194a6683c1367d8baeb1cb">Progressive Inter-scale and intra-scale Non-blind Image Deconvolution</a>, Yuan, Sun, Quan, and Shum&nbsp;<a href="http://pages.cs.wisc.edu/~lizhang/projects/motionhdr/0223.pdf" rel="nofollow" data-token="ef909b174fbecf2777332b3ca4c4a760">Denoising vs. Deblurring: HDR Imaging Techniques Using Moving Cameras</a>, Zhang, Deshpande, and Chen. CVPR 2010.&nbsp;<a href="http://pages.cs.wisc.edu/~lizhang/projects/motionhdr/" rel="nofollow" data-token="c168a6b503c98c378e872bf8b222f8bb">Web Page</a>&nbsp;<a href="http://www.comp.nus.edu/~zhuoshao/flashDeblur/flashdeblur_cvpr10_low.pdf" rel="nofollow" data-token="cc05e80e98551eb4a7f2d83e87e13b5f">Robust Flash Deblurring</a>, Zhuo and Sim. CVPR 2010.&nbsp;<a href="http://www.comp.nus.edu/~zhuoshao/flashDeblur/" rel="nofollow" data-token="8212da3dacff312392cf32fb4ed148c6">Web Page</a></p>
<p><a name="t5"></a><a name="TOC-1"></a></p>
<p><a name="t6"></a><a name="TOC-Matting-and-Editing"></a>Matting and Editing（抠图和图像编辑）</p>
<p><a href="http://grail.cs.washington.edu/projects/photomontage/photomontage.pdf" rel="nofollow" data-token="1a0abf055f6c454171d8dbcdbd5effc0">Interactive Digital Photomontage</a>, Agarwala, Dontcheva, Agrawala, Drucker, Colburn, Curless, Salesin, and Cohen&nbsp;<a href="http://juew.org/publication/VideoSnapCut_lr.pdf" rel="nofollow" data-token="ead18e9bde9b3de303816997a5a61797">Video SnapCut: Robust Video Object Cutout Using Localized Classifiers</a>, Bai, Wang, Simons, and Saprio. SIGGRAPH 2009.&nbsp;<a href="http://juew.org/projects/SnapCut/snapcut.htm" rel="nofollow" data-token="d797041155d421fd613e7a99c46c6077">Web Page</a>&nbsp;<a href="http://www.cs.princeton.edu/gfx/pubs/Barnes_2009_PAR/patchmatch.pdf" rel="nofollow" data-token="7a1360b70044d27caed8311de841c233">PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing</a>, Barnes, Shechtman, Finkelstein, and Goldman. SIGGRAPH 2009.&nbsp;<a href="http://www.cs.princeton.edu/gfx/pubs/Barnes_2009_PAR/index.php" rel="nofollow" data-token="a302483e9be884bc97d3af0ba8efbe72">Web Page</a>&nbsp;<a href="http://www1.cs.columbia.edu/CAVE/publications/pdfs/Bitouk_SIGGRAPH08.pdf" rel="nofollow" data-token="99585ce6888815afed67c972efae343a">Face Swapping: Automatically Replacing Faces in Photographs</a>, Bitouk, Kumar, Dhillon, Belhumeur, and Nayar. SIGGRAPH 2008.&nbsp;<a href="http://www.cs.columbia.edu/CAVE/projects/face_replace/" rel="nofollow" data-token="240ee27d6a3738b49d71f46ff5abe66f">Web Page</a>&nbsp;<a href="http://people.csail.mit.edu/taegsang/Documents/CVPRPatch.pdf" rel="nofollow" data-token="8504da06faa192de020aae254cbf4833">The Patch Transform and Its Applications to Image Editing</a>, Cho, Butman, Avidan, and Freeman.&nbsp;<a href="http://people.csail.mit.edu/taegsang/patchTransform.html" rel="nofollow" data-token="7866599a1b87bfba1eab075081113740">Web Page</a>&nbsp;<a href="http://grail.cs.washington.edu/projects/digital-matting/papers/cvpr2001.pdf" rel="nofollow" data-token="8092488637a779d80ec9dc8ff4174f04">A Bayesian Approach to Digital Matting</a>, Chuang, Curless, Salesin, and Szeliski<br /><a href="http://research.microsoft.com/pubs/81528/ACriminisi_ACM_TOG2010.pdf" rel="nofollow" data-token="f3b461ea50c9a66eb00b5b5a01cb601c">Geodesic Image and Video Editing</a>, Criminisi, Sharp, Rother, and Perez. SIGGRAPH 2011.&nbsp;<a href="http://www.cs.huji.ac.il/~danix/mvclone/files/mvc-final-opt.pdf" rel="nofollow" data-token="d258d0b27e1ada246b28ba1ecbc7c8f4">Coordinates for Instant Image Cloning</a>, Farbman, Hoffer, Lipman, Cohen-Or, and Lischinski. SIGGRAPH 2009.&nbsp;<a href="http://www.cs.huji.ac.il/~danix/mvclone/" rel="nofollow" data-token="39233d4c93f90441c0372131dece9e05">Web Page</a>&nbsp;<a href="http://www.inf.ufrgs.br/~eslgastal/SharedMatting/Gastal_Oliveira_EG2010_Shared_Matting.pdf" rel="nofollow" data-token="f8c0b8fd5c085c23d42c425653024ead">Shared Sampling for Real-Time Alpha Matting</a>, Gastal and Oliveira. Eurographics 2010.&nbsp;<a href="http://www.inf.ufrgs.br/~eslgastal/SharedMatting/" rel="nofollow" data-token="b819511311f579007382c3b44ca9b995">Web Page</a>&nbsp;<a href="http://www.robots.ox.ac.uk/~vgg/publications/papers/gulshan10.pdf" rel="nofollow" data-token="ea12cd278b9183b0ef6e6ab33c7174bb">Geodesic Star Convexity for Interactive Image Segmentation</a>, Gulshan, Rother, Criminisi, Blake, and Zisserman. CVPR 2010.&nbsp;<a href="http://www.robots.ox.ac.uk/~vgg/research/iseg/" rel="nofollow" data-token="c226ba4e778281bdc0c0a7e227dbd5d0">Web Page and Code</a>&nbsp;<a href="http://personal.ie.cuhk.edu.hk/~hkm007/publications/cvpr11matting.pdf" rel="nofollow" data-token="fc68018c3df176fcb27bfa9f72f7cdc4">A Global Sampling Method for Alpha Matting</a>, He, Rhemann, Rother, Tang, Sun. CVPR 2011.&nbsp;<a href="http://personal.ie.cuhk.edu.hk/~hkm007/publications/eccv10guidedfilter.pdf" rel="nofollow" data-token="4de2b99a520f0fc67d35ea7868657032">Guided Image Filtering</a>, He, Sun, Tang. ECCV 2011.&nbsp;<a href="http://personal.ie.cuhk.edu.hk/~hkm007/eccv10/guided-filter-code-v1.rar" rel="nofollow" data-token="880a1d8ecd423b6c5fffb4a3a3ae2a9a">Code</a>&nbsp;<a href="http://people.csail.mit.edu/ehsu/work/sig08lme/lme-sig2008-sm.pdf" rel="nofollow" data-token="49c5ddc9ab713a64a024de9f068d41a3">Light Mixture Estimation for Spatially Varying White Balance</a>, Hsu, Mertens, Paris, Avidan, and Durand.&nbsp;<a href="http://people.csail.mit.edu/ehsu/work/sig08lme/" rel="nofollow" data-token="d5b961b287cfa782510af0c82c96f98e">Web Page</a>&nbsp;<a href="http://vmcl.xjtu.edu.cn/cgzhanglei/publications/TOG_11/arcimboldo.pdf" rel="nofollow" data-token="45d678f9ca75102b97e2829fc47d815f">Arcimboldo-like Collage Using Internet Images</a>, Huang, Zhang, and Zhang.&nbsp;<a href="http://vmcl.xjtu.edu.cn/cgzhanglei/publications/TOG_11/arcimboldo_project.html" rel="nofollow" data-token="e48b3eaaba5e1cb4a614e583fd20a172">Web Page</a>&nbsp;<a href="http://www.cse.cuhk.edu.hk/~leojia/all_project_webpages/ddp/dragdroppasting.pdf" rel="nofollow" data-token="8e15fda081101f01e9f450abcc87e01d">Drag-and-Drop Pasting</a>, Jia, Sun, Tang, and Shum.&nbsp;<a href="http://www.cse.cuhk.edu.hk/~leojia/all_project_webpages/ddp/drag-and-drop_pasting.html" rel="nofollow" data-token="42107035117e49ad05d0745561afb86a">Web Page</a>&nbsp;<a href="http://grail.cs.washington.edu/photobios/paper.pdf" rel="nofollow" data-token="2d066532ef484aaba3d28927a6c9c6bf">Exploring Photobios</a>, Kemelmacher-Shlizerman, Shechtman, Garg, Seitz. SIGGRAPH 2011.&nbsp;<a href="http://grail.cs.washington.edu/photobios/" rel="nofollow" data-token="fd866b836e1424b8c933da54f6ee50c2">Web Page</a>&nbsp;<a href="http://www.cs.huji.ac.il/~alevin/papers/eccv04-blending.pdf" rel="nofollow" data-token="70fef6847e1cb6e3c296d7d227d8418d">Seamless Image Stitching in the Gradient Domain</a>, Levin, Zomet, Peleg, and WeissPhoto Clip Art, Lalonde, Hoiem, Efros, Rother, Winn, and Criminisi&nbsp;<a href="http://www.wisdom.weizmann.ac.il/~levina/papers/Matting-Levin-Lischinski-Weiss-CVPR06.pdf" rel="nofollow" data-token="6e5f99f3b1e2a299a63d3452930a1d40">A Closed Form Solution to Natural Image Matting</a>, Levin, Lischinski, and Weiss&nbsp;<a href="http://www.wisdom.weizmann.ac.il/~levina/matting.tar.gz" rel="nofollow" data-token="fa6a7b7cdeb51d7d632acedb3862effe">Code</a><br /><a href="http://www.wisdom.weizmann.ac.il/~levina/papers/spectral-matting-levin-etal-pami08.pdf" rel="nofollow" data-token="13bfcec79206e5868222512b5ae6d3c5">Spectral Matting</a>, Levin,&nbsp;Rav-Acha, and&nbsp;Lischinski&nbsp;<a href="http://research.microsoft.com/en-us/um/people/jiansun/papers/PaintSelection_SIGGRAPH09.pdf" rel="nofollow" data-token="8c499a3b875daed7ff72bbeb44801dbe">Paint Selection</a>, Liu, Sun, and Shum. SIGGRAPH 2009.&nbsp;<a href="http://www.irisa.fr/vista/Papers/2003_siggraph_perez.pdf" rel="nofollow" data-token="f1c2efbe14284dba0735baa6e74558f6">Poisson Image Editing</a>, Perez, Gangnet, and Blake&nbsp;<a href="http://research.microsoft.com/pubs/80299/cvpr09-matting-Eval.pdf" rel="nofollow" data-token="ac80d491cb027b45487b3a1743e86fab">A Perceptually Motivated Online Benchmark for Image Matting</a>, Rhemann, Rother, Wang, Gelautz, Kohli, and Rott.&nbsp;<a href="http://www.alphamatting.com/" rel="nofollow" data-token="0c345058398edd25d96fcc8e8364f374">Web Page</a><br /><a href="http://www.ims.tuwien.ac.at/media/documents/publications/1291.pdf" rel="nofollow" data-token="0ea0d86eaceb95255ed8d7e416d842c2">A Spatially Varying PSF-based Prior for Alpha Matting</a>, Rhemann, Rother, Kohli, and Gelautz. CVPR 2010.&nbsp;<a href="http://research.microsoft.com/en-us/um/people/ablake/papers/ablake/Rother_Siggraph06.pdf" rel="nofollow" data-token="5b649f188060fd23feda19d56e2e6171">AutoCollage</a>, Rother, Bordeaux, Hamadi, and Blake&nbsp;<a href="http://ai.stanford.edu/~ruzon/alpha/ruzonCvpr00.pdf" rel="nofollow" data-token="ff1e9529c40940bd6425095a591539b9">Alpha Estimation in Natural Images</a>, Ruzon and Tomasi<a href="http://research.microsoft.com/pubs/80300/cvpr09-matting-newModels.pdf" rel="nofollow" data-token="fd693fade486f907bcbf1ae5dd6724dc">New Appearance Models for Natural Image Matting</a>, Singaraju, Rother, and Rhemann&nbsp;<a href="http://www.cs.utah.edu/~bsumma/Interactive_Editing_of_Massive_Imagery_files/summa-editing_massive.pdf" rel="nofollow" data-token="c3009e5868c66271f022d2279c503ae9">Interactive Editing of Massive Imagery Made Simple: Turning Atlanta into Atlantis</a>, Summa, Scorzelli, Jiang, Bremer, and Pascucci. SIGGRAPH 2011.&nbsp;<a href="http://www.cs.utah.edu/~bsumma/Interactive_Editing_of_Massive_Imagery.html" rel="nofollow" data-token="a07c2cef7ab8d524f780256d76178728">Web Page</a>&nbsp;<a href="http://research.microsoft.com/~jiansun/papers/FlashMatting_SIGGRAPH06.pdf" rel="nofollow" data-token="ee58adff226e90f9610cfe9609098034">Flash Matting</a>, Sun, Li, Kang, and Shum&nbsp;<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5753119" rel="nofollow" data-token="cc38042ebcbc115db9fc41fea6b16fc8">Fast Poisson Blending Using Multi-splines</a>, Szeliski,&nbsp;Uyttendaele, and Steedly. ICCP 2011.&nbsp;<a href="http://vis.berkeley.edu/papers/softscissors/" rel="nofollow" data-token="84e9023a5a5f147b760f6de6580f22d2">Soft Scissors : An Interactive Tool for Realtime High Quality Matting</a>, Wang, Agrawala, and Cohen&nbsp;<a href="http://juew.org/publication/mattingSurvey.pdf" rel="nofollow" data-token="1293f145e325ca3c138b320963a92007">Image and Video Matting: A Survey</a>, Wang and Cohen<a name="15"></a></p>
<p><a name="t7"></a><a name="TOC-Warping-and-Morphing"></a>Warping and Morphing（图像扭曲和变形）</p>
<p><a href="http://graphics.stanford.edu/courses/cs468-07-winter/Papers/sig00_arasi.pdf" rel="nofollow" data-token="de03cfd7ecfbbb27010312ac7c80810a">As-Rigid-As-Possible Shape Interpolation</a>, Alexa, Cohen-Or, and Levin&nbsp;<a href="http://graphics.stanford.edu/courses/cs248-01/p35-beier.pdf" rel="nofollow" data-token="bdc7d9af33fc06cf8fb1bc2324922867">Feature-Based Image Metamorphosis</a>, Beier and Neely&nbsp;<a href="http://vis.berkeley.edu/papers/capp/projection-sig09.pdf" rel="nofollow" data-token="238c63e5777f2e83f226bf063ddcc111">Optimizing Content-Preserving Projections for Wide-Angle Images</a>, Carroll, Agrawala, and Agarwala. SIGGRAPH 2009.&nbsp;<a href="http://vis.berkeley.edu/papers/capp/" rel="nofollow" data-token="1c50f237ea535d2c5e2060be5921f520">Web Page</a>&nbsp;<a href="http://graphics.cs.uiuc.edu/~jch/papers/dpim.pdf" rel="nofollow" data-token="a1417021bfaf636a7a7e6a68ed9770c7">Detail Preserving Shape Deformation in Image Editing</a>, Fang and Hart&nbsp;<a href="http://cg.cs.tu-berlin.de/~sorkine/ProjectPages/FeatureAware/warp-final-web.pdf" rel="nofollow" data-token="9a092bc999a6b10eb248a121ed056b91">Feature-Aware Texturing</a>, Gal, Sorkine, and Cohen-Or&nbsp;<a href="http://www-ui.is.s.u-tokyo.ac.jp/~takeo/papers/rigid.pdf" rel="nofollow" data-token="378aa26396f4b87ef050539b72a6cdfb">As-Rigid-As-Possible Shape Manipulation</a>, Igarashi, Moscovich, and Hughes&nbsp;<a href="http://www-cs.ccny.cuny.edu/~wolberg/pub/cga98.pdf" rel="nofollow" data-token="fec1382df19b812dbbc64f7085325c65">Polymorph: Morphing Among Multiple Images&nbsp;</a>, Lee, Wolberg, and Shin&nbsp;<a href="http://www.cs.wisc.edu/graphics/Papers/Gleicher/fliu/siggraph09_preprint_small.pdf" rel="nofollow" data-token="e5052247a5862beea7f7de1bf83f15db">Content-Preserving Warps for 3D Video Stabilization</a>, Liu, Gleicher, Jin and Agarwala. SIGGRAPH 2009.&nbsp;<a href="http://pages.cs.wisc.edu/~fliu/project/3dstab.htm" rel="nofollow" data-token="f4fcd2fcf87043d97bfe4b88d3977ee4">Web Page</a>&nbsp;<a href="http://www1.cs.columbia.edu/~dhruv/interpolationweb/interpolation.pdf" rel="nofollow" data-token="8361599c0300d6e6c9f5533de216c2e0">Moving Gradients: A Path-Based Method for Plausible Image Interpolation</a>, Mahajan, Huang, Matusik, Ramamoorthi, and Belhumeur. SIGGRAPH 2009&nbsp;<a href="http://www.faculty.idc.ac.il/arik/SCWeb/multiop/multiop-lowres.pdf" rel="nofollow" data-token="94ffe85eba383c885edc6e2ad1b4dbdf">Multi-operator Media Retargeting</a>, Rubinstein, Shamir, and Avidan. SIGGRAPH 2009.&nbsp;<a href="http://www.faculty.idc.ac.il/arik/SCWeb/multiop/index.html" rel="nofollow" data-token="5be5d87fa1ad10e63d31dae14ac38d47">Web Page</a>&nbsp;<a href="http://grail.cs.washington.edu/projects/regenmorph/regenmorph.cvpr10.pdf" rel="nofollow" data-token="4353674b0027a096ecf1492e23840642">Regenerative Morphing</a>, Shechtman, Rav-Acha, Irani, and Seitz. CVPR 2010.&nbsp;<a href="http://grail.cs.washington.edu/projects/regenmorph/" rel="nofollow" data-token="65eb51d35e313819af1d0170eca7d824">Web Page</a>&nbsp;<a href="http://www-cs.ccny.cuny.edu/~wolberg/pub/vc98.pdf" rel="nofollow" data-token="449aa70c1be91b90282b7dc839dc15f1">Image Morphing: A Survey&nbsp;</a>, Wolberg<a name="10"></a></p>
<p><a name="t8"></a><a name="TOC-Useful-Techniques"></a>Useful Techniques（其他相关技术）</p>
<p><a href="http://graphics.stanford.edu/papers/gkdtrees/gkdtrees.pdf" rel="nofollow" data-token="954939cf3880fe7b06620db1feae3673">Gaussian KD-Trees for Fast High-Dimensional Filtering</a>, Adams, Gelfand, Dolson, and Levoy. SIGGRAPH 2009.<a href="http://graphics.stanford.edu/papers/gkdtrees/" rel="nofollow" data-token="5334b152d5323fdf32c3a345dcc13480">&nbsp;Web Page</a>&nbsp;<a href="http://graphics.stanford.edu/papers/permutohedral/permutohedral.pdf" rel="nofollow" data-token="67007166cfe7a873e1e138c92a1d2d06">Fast High-Dimensional Filtering Using the Permutohedral Lattice</a>, Adams, Baek, and Davis. Eurographics 2010.&nbsp;<a href="http://graphics.stanford.edu/papers/permutohedral/" rel="nofollow" data-token="8641a082d8de29171647913631fdcf3c">Web Page</a>&nbsp;<a href="http://www.cs.cornell.edu/~rdz/Papers/BVZ-pami01-final.pdf" rel="nofollow" data-token="c92ada4fce527544d755652c55381b96">Fast Approximate Energy Minimization via Graph Cuts</a>, Boykov, Veksler, and Zabih<a href="http://www.cs.huji.ac.il/~raananf/projects/eaw/paper.pdf" rel="nofollow" data-token="5b13c1a6eac6d0c533c93355e8eadd9d">Edge-Avoiding Wavelets and thier Applications</a>, Fattal&nbsp;<a href="http://www.cs.huji.ac.il/~raananf/projects/eaw/index.html" rel="nofollow" data-token="1f3c476e890099dcc913d289c4598d79">Web Page</a>&nbsp;<a href="http://www.cs.huji.ac.il/~yweiss/jordan.ps" rel="nofollow" data-token="64b8a0480cb7bec2f21c2443142f5926">Graphical Models: Probabilistic Inference&nbsp;</a>, Jordan and Weiss&nbsp;<a href="http://www.cs.berkeley.edu/~jordan/papers/loopy.ps" rel="nofollow" data-token="e2a3d65ce2e4c160d5d044443bb742f6">Loopy Belief Propagation for Approximate Inference: An Empirical Study&nbsp;</a>, Murphy, Weiss, and Jordan&nbsp;<a href="http://people.csail.mit.edu/sparis/bf/" rel="nofollow" data-token="8b81cba21fd59c096a2f56589cdfe6e5">Bilateral Filtering: Papers, Resources, Applications</a>, Paris and Durand&nbsp;<a href="http://www.merl.com/papers/docs/TR2008-030.pdf" rel="nofollow" data-token="5e782be420ec353c5f361279e223b13e">Constant time O(1) bilateral filtering</a>&nbsp;Porikli&nbsp;<a href="http://graphics.cs.cmu.edu/courses/15-463/2004_fall/www/Papers/MSR-TR-2004-92-Sep27.pdf" rel="nofollow" data-token="63fcdecba8715327d6d97e9b22c9ac55">Image Alignment and Stitching: A Tutorial</a>, Szeliski&nbsp;<a href="http://www.cse.ucsc.edu/~manduchi/Papers/ICCV98.pdf" rel="nofollow" data-token="bb50b0ab0036d79bb1d9d87a45da500f">Bilateral Filtering for Gray and Color Images</a>, Tomasi and Manduchi&nbsp;<a href="http://www.cse.cuhk.edu.hk/~leojia/papers/L0smooth_Siggraph_Asia2011.pdf" rel="nofollow" data-token="814c55a474c680c8803d50750e741cfb">Image Smoothing via L0 Gradient Minimization</a>, Xu, Lu, Xu, and Jia. SIGGRAPH Asia 2011.&nbsp;<a href="http://www.cse.cuhk.edu.hk/~leojia/projects/L0smoothing/index.html" rel="nofollow" data-token="cc845398f102da6f90151bb952690eb8">Web Page</a>&nbsp;<a href="http://vision.ai.uiuc.edu/~qyang6/publications/cvpr-09-qingxiong-yang.pdf" rel="nofollow" data-token="1f7c6a3fb5e1d314a790d71786ca42a4">Real-Time O(1) Bilateral Filtering</a>, Yang, Tan and Ahuja&nbsp;<a href="http://vision.ai.uiuc.edu/~qyang6/publications/qx_cvpr09_ctbf.zip" rel="nofollow" data-token="c5217b62e0c3244a4b3bb381405ddd35">Source Code</a>&nbsp;<a href="http://vision.ai.uiuc.edu/~qyang6/publications/cvpr-10-qingxiong-yang-svmbf.pdf" rel="nofollow" data-token="b846ad738acfee38224b7d16c970051d">SVM for Edge-Preserving Filtering</a>, Yang, Wang and Ahuja</p>
<p><a name="t9"></a><a name="TOC-...-and-Beyond"></a>&#8230; and Beyond</p>
<p><a href="http://grail.cs.washington.edu/projects/multipano/agarwala_sig06.pdf" rel="nofollow" data-token="3b47e6e8137d726eb3954b7fda895a91">Photographing long scenes with multi-viewpoint panoramas</a>, Agarwala, Agrawala, Cohen, Salesin, and Szeliski&nbsp;<a href="http://gvi.seas.harvard.edu/sites/all/files/faceReplace_sa2011.pdf" rel="nofollow" data-token="1e7f463efe7d30ce4fd5c53721a71787">Video Face Replacement</a>, Dale et al. SIGGRAPH ASIA 2011.&nbsp;<a href="http://gvi.seas.harvard.edu/paper/video-face-replacement" rel="nofollow" data-token="b9d78149ce83908db3b55ec5c675ff2c">Web Page</a>&nbsp;<a href="http://www.cs.huji.ac.il/labs/cglab/projects/convpyr/data/convpyr-small.pdf" rel="nofollow" data-token="c400a83d511c4d4766412ce8d9e844f4">Convolution Pyramids</a>, Farbman, Fattal, and Lischinski. SIGGRAPH ASIA 2011.&nbsp;<a href="http://grail.cs.washington.edu/projects/candid_video_portraits/paper.pdf" rel="nofollow" data-token="dfd2ec61066767e9097be64d3a2c225e">Candid Portrait Selection from Video</a>, Fiss, Argarwala, and Curless. SIGGRAPH ASIA 2011.&nbsp;<a href="http://grail.cs.washington.edu/projects/candid_video_portraits/" rel="nofollow" data-token="02e53bdc65e53cd26ea7cae5525fbcf2">Web Page</a>&nbsp;<a href="http://www.robots.ox.ac.uk/~awf/ibr/iccv.pdf" rel="nofollow" data-token="e711642a35b17e55f46538518c6b46cf">Image-Based Rendering Using Image-Based Priors</a>, Fitzgibbon, Wexler, and Zisserman&nbsp;<a href="http://portal.acm.org/citation.cfm?id=1015720" rel="nofollow" data-token="e547cb0f9512b2d662f2ceda5015d0c0">&#8220;GrabCut&#8221;&#8211;Interactive Foreground Extraction using Iterated Graph Cuts</a>, Rother, Kolmogorov, and Blake&nbsp;<a href="http://research.microsoft.com/en-us/um/cambridge/projects/visionimagevideoediting/segmentation/grabcut.htm" rel="nofollow" data-token="696225020bbf1cbc09a1114e5db82e99">Web Page</a>&nbsp;<a href="http://phototour.cs.washington.edu/Photo_Tourism.pdf" rel="nofollow" data-token="08912f89426d43fc060c7480d4703d02">Photo Tourism: Exploring Photo Collections in 3D</a>, Snavely, Seitz, and Szeliski&nbsp;<a href="http://phototour.cs.washington.edu/" rel="nofollow" data-token="19f021bc786f0cbea6af63d5dcd51df4">Web Page</a>&nbsp;</p>
<p><a name="t10"></a><a name="TOC-Books-for-General-Reference"></a>Books for General Reference</p>
<p><em>Digital Image Processing, Second Edition</em>, Gonzalez and Woods&nbsp;<em>Computer Vision: A Modern Approach</em>, Forsyth and Ponce&nbsp;<em>The Art and Science of Digital Compositing</em>, Brinkmann&nbsp;<em>Multiple View Geometry in Computer Vision</em>, Hartley and Zisserman&nbsp;<em>Linear Algebra and Its Applications</em>, Strang&nbsp;<em><a href="http://szeliski.org/Book/" rel="nofollow" data-token="88587538b3b7b34f7424cae8f9c6f647">Computer Vision: Algorithms and Applications</a></em>, Richard Szeliski</p>
</p></div>
</div>
]]></content:encoded>
							<enclosure url="http://www.faculty.idc.ac.il/arik/VidRet.mov" length="1635" type="video/quicktime" />
			</item>
		<item>
		<title>暗通道去雾法-对算法的理解Dark Channel Prior</title>
		<link>https://uzzz.org/article/899.html</link>
				<pubDate>Wed, 18 Apr 2018 09:42:01 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[图像处理]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/899.html</guid>
				<description><![CDATA[暗通道最早是由He提出的，并且也发展得很快，具体的在这里不说，这篇文章，只是谈一下暗通道去雾的实现过程 1.该方法最原始的模型是I=Jt+A(1-t)，这个模型在我的其他文章中有详细介绍，这里不详细讲解。可以看出带有雾的图像的I是由没有散射的原图像J和大气光(air light)共同组成的。这里刚好看到t，1-t和为1，实则不然，只是凑巧这样，原作者有详细的]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<p><span style="font-family:'Arial Black';">暗通道最早是由He提出的，并且也发展得很快，具体的在这里不说，这篇文章，只是谈一下暗通道去雾的实现过程</span></p>
</p>
<p>1.该方法最原始的模型是I=Jt+A(1-t)，这个模型在我的其他文章中有详细介绍，这里不详细讲解。<br />可以看出带有雾的图像的I是由没有散射的原图像J和大气光(air light)共同组成的。这里刚好看到t，1-t和为1，实则不然，只是凑巧这样，原作者有详细的推导，这个是推导而来并非为了凑和为1。</p>
<p>2.看了上面的模型进一步会想，要是我们把t变为1，不就I=J，完事了么……理论上是这样的，不过一般给你的图像都是拍好的，你要想t=1，那你得找没有雾的图，没有雾的图你去啥雾。在这里我们不是改变t的值，而是对t进行求解。</p>
</p>
<p>3.为了求t，He老大的暗通道出现了，他发现一个规律，在没有雾的图像中，图像的RGB通道，总会有一个通道的值偏低。该区域的最小值是一个很小的值。颜色鲜艳和阴影都会表现出暗通道极小的值。下图是选了三通道最小值后又做了个最小值滤波。<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180418165038641" alt="">该公式就可以表示暗通道的性质了。c为通道，Ω局部窗范围（最小滤波）。这里还有一点可以看出，天空那不是黑的。在He.2009年的论文中也说明了，在带有天空的图像中，暗通道方法并不是很适用，后期应该是他已经改进了，想要了解的可以去查一下。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180418164457868" alt=""></p>
</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/2018041816473912" alt=""></p>
</p>
<p>4.J=0，你有没有看出点什么。我们未知参数就只有一个t了。<br /><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180418165530386" alt=""><br />通过上面的公式我们的t就求出来了，t出来了我们真实的J就可以求解出来了。</p>
</p>
<p>5.这里说明几个问题<br />①求出的t咋用？这里说明一点，在图像中，不同点的t是不同的，因此每个区域都有一个t，把t的矩阵带入到最上面即可。<br />②大气光A怎么求？<span style="font:16.19px/25.99px '-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;text-align:justify;color:rgb(79,79,79);text-transform:none;text-indent:0px;letter-spacing:normal;word-spacing:0px;">首先从暗通道图像中按照亮度的大小提取最亮的前0.1%像素。然后，在原始有雾图像I</span><span class="MathJax" style="font:16.19px/normal '-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;border:0px;text-align:left;color:rgb(79,79,79);text-transform:none;text-indent:0px;letter-spacing:normal;word-spacing:0px;min-height:0px;min-width:0px;"><span class="MJX_Assistive_MathML" style="border:0px;width:1px;line-height:normal;font-weight:400;text-decoration:none;vertical-align:0px;">I</span></span><span style="font:16.19px/25.99px '-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;text-align:justify;color:rgb(79,79,79);text-transform:none;text-indent:0px;letter-spacing:normal;word-spacing:0px;">中寻找对应位置上的具有最高亮度的点的值，并以此作为A</span><span class="MathJax" style="font:16.19px/normal '-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;border:0px;text-align:left;color:rgb(79,79,79);text-transform:none;text-indent:0px;letter-spacing:normal;word-spacing:0px;min-height:0px;min-width:0px;"><span class="MJX_Assistive_MathML" style="border:0px;width:1px;line-height:normal;font-weight:400;text-decoration:none;vertical-align:0px;">A</span></span><span style="font:16.19px/25.99px '-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;text-align:justify;color:rgb(79,79,79);text-transform:none;text-indent:0px;letter-spacing:normal;word-spacing:0px;">的值。<br />③三个通道怎么处理呢？一个一个处理，t是一样的，只是不同的Ic和Ac……（c：R、G、B）<br />④网上提到了<span style="color:#4b4b4b;font:12.72px/17.27px verdana, Arial, helvetica, 'sans-seriff';text-transform:none;text-indent:0px;letter-spacing:normal;word-spacing:0px;">soft matting</span>和导向图是用来干啥的？咱们用最小滤波出来的暗通道会是一块一块的，那么咱们的t估计也好不到哪去…………为了得到更好地t，我们就要优化我们的暗通道图。He最初用soft matting……网上的评论一致认为很慢，我没试，也就不说了。导向图是用来替代soft matting的，也是听他们说很快。</span></p>
<p><span style="font:16.19px/25.99px '-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;text-align:justify;color:rgb(79,79,79);text-transform:none;text-indent:0px;letter-spacing:normal;word-spacing:0px;"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180418173756507" alt=""></span></p>
<p>图片来自大神博客：<a href="http://www.cnblogs.com/Imageshop/p/3281703.html" rel="nofollow" data-token="bccd54c2de35be21e418a5e2fea791b8">http://www.cnblogs.com/Imageshop/p/3281703.html</a></p>
</p>
</p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>图像检索系列一：Deep Learning of Binary Hash Codes for Fast Image Retrieval</title>
		<link>https://uzzz.org/article/1715.html</link>
				<pubDate>Wed, 23 Nov 2016 06:44:06 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[darknet]]></category>
		<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[图像处理]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1715.html</guid>
				<description><![CDATA[Deep Learning of Binary Hash Codes for Fast Image Retrieval 这篇文章发表在2015CVPR workshop 文章链接：http://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W03/papers/Lin_Deep_Le]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<p>Deep Learning of Binary Hash Codes for Fast Image Retrieval 这篇文章发表在2015CVPR workshop</p>
<p>文章链接：<a href="http://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W03/papers/Lin_Deep_Learning_of_2015_CVPR_paper.pdf" rel="nofollow" data-token="925f534a6179f5aeb33a97507bdb6a84">http://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W03/papers/Lin_Deep_Learning_of_2015_CVPR_paper.pdf</a></p>
<p>代码链接：<a href="https://github.com/kevinlin311tw/caffe-cvprw15" rel="nofollow" data-token="865bf79e90efcb12774b7e26b0df400c">https://github.com/kevinlin311tw/caffe-cvprw15</a></p>
<p style="text-align:center;"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20161123144809544" alt=""></p>
<p style="text-align:center;">图一 算法框架流程</p>
<p>这篇文章的想法很巧妙，在一个深层CNN的最后一个全连接层（fc8）和倒数第二个全连接层（fc7）之间加了一层全连接隐层，就是图一中绿色的latent layer （H）。这样一来，既可以得到深层的CNN特征，文中主要用的是fc7的特征，还可以得到二分的哈希编码，即来自H。这个隐层H不仅是对fc7的一个特征概括，而且是一个连接CNN网络的中层特征与高层特征的桥梁。</p>
<p><span style="color:#3366ff;">1. Domain Adaption</span></p>
<p>为了让一个网络能够对某一类物体高鲁棒，即target domain adaption，用一类主题目标数据集来整定(fine-tune)整个网络。fc8的节点数由目标类别数决定，H的节点数在文中有两种尝试：48和128。这两个层在fine-tune时，是随机初始化的，其中H的初始化参考了LSH[1]的方法，即通过随机映射来构造哈希位。通过这样训练，得到的网络能够产生对特定物体的描述子以及对应的哈希编码。</p>
<p></p>
<p><span style="color:#3366ff;">2. Image Retrieval</span></p>
<p>主要提出了一种从粗糙到细致的检索方案（coarse-to-fine）。H层首先被二值化：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20161123152530221" alt=""></p>
<p>粗糙检索是用H层的二分哈希码，相似性用hamming距离衡量。待检索图像设为I，将I和所有的图像的对应H层编码进行比对后，选择出hamming距离小于一个阈值的m个构成一个池，其中包含了这m个比较相似的图像。</p>
<p>细致检索则用到的是fc7层的特征，相似性用欧氏距离衡量。距离越小，则越相似。从粗糙检索得到的m个图像池中选出最相似的前k个图像作为最后的检索结果。每两张图128维的H层哈希码距离计算速度是0.113ms，4096维的fc7层特征的距离计算需要109.767ms，因此可见二值化哈希码检索的速度优势。</p>
<p></p>
<p><span style="color:#3366ff;">3. 实验结果</span></p>
<p>作者在MINIST，CIFAR-10，YAHOO-1M三个数据集上做了实验，并且在分类和检索上都做了实验，结果都很不错，特别是在CIFAR-10上图像检索的精度有30%的提升。</p>
<p>（1）MINIST</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20161123153630259" alt=""></p>
<p>左边第一列是待检索图像，右边是48和128位H层节点分别得到的结果。可以看到检索出的数字都是正确的，并且在这个数据集上48位的效果更好，128位的太高，容易引起过拟合。</p>
<p>（2）CIFAR-10</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20161123153925066" alt=""></p>
<p>在这个数据集上128位的H层节点比48位的效果更好，比如128检索出更多的马头，而48位的更多的全身的马。</p>
<p>（3）YAHOO-1M</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20161123154147429" alt=""></p>
<p>作者在这个数据集上比较了只用fc7,只用H和同时用两者（粗糙到细致）的结果，实验结果表明是两者都用的效果更好。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20161123154347949" alt=""></p>
<p>可以看到如果只用alexnet而不进行fine-tune的话，检索出的结果精度很低。</p>
<p></p>
<p><span style="color:#3366ff;">4. 总结</span></p>
<p>这个方法整篇文章看下来给人的感觉比较工程，全篇讲理论和方法的部分很少，几乎没有什么数学公式，但是效果好，这个最重要。想法很简单，但是很巧妙，值得学习。代码已经开源，准备尝试。</p>
<p><span style="color:rgb(34,34,34);font-family:Arial, sans-serif;font-size:13px;line-height:16.12px;">[1] Gionis A, Indyk P, Motwani R. Similarity search in high dimensions via hashing[C]//VLDB. 1999, 99(6): 518-529.</span></p>
</p></div>
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
