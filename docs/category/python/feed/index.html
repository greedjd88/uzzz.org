<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Python &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/category/python/feed" rel="self" type="application/rss+xml" />
	<link>https://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Thu, 05 Sep 2019 06:46:06 +0000</lastBuildDate>
	<language>zh-CN</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>Python &#8211; 有组织在!</title>
	<link>https://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>在scrapy使用Tor代理Ip的两种方法</title>
		<link>https://uzzz.org/article/1068.html</link>
				<pubDate>Thu, 05 Sep 2019 06:46:06 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Python]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1068.html</guid>
				<description><![CDATA[第一个：按照https://www.cnblogs.com/kylinlin/archive/2016/03/04/5242266.html 大佬的写法，实现了 1、首先将本地的代理服务器进行设置，，这一步是为了与polipo对接。。 2、因为我们在polipo的配置文件中写了 洋葱头代理的地址和端口（9050），我们使用socks请求端口9150发出每次请]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<p>第一个：按照https://www.cnblogs.com/kylinlin/archive/2016/03/04/5242266.html 大佬的写法，实现了<br /> 1、首先将本地的代理服务器进行设置，，这一步是为了与polipo对接。。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190905144552932.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM1MTkzNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190905144704671.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM1MTkzNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 2、因为我们在polipo的配置文件中写了 洋葱头代理的地址和端口（9050），我们使用socks请求端口9150发出每次请求。<br /> socksParentProxy = “localhost:9050”</p>
<p>socksProxyType = socks5</p>
<p>diskCacheRoot = “”<br /> http://zhihan.me/network/2017/09/24/socks5-protocol/ 这个是关于sockes5的介绍。。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019090514524033.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM1MTkzNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 3、然后在中间件中写入<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190905145358609.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM1MTkzNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 4、在stttings中<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190905145446412.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM1MTkzNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 我是在start_requests() 中加了一个检测ip的语句<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190905150026936.png" alt="在这里插入图片描述"><br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190905150052889.png" alt="在这里插入图片描述"><br /> 这样是可以运行的。。但是他的IP不会换。。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190905151036943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM1MTkzNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>第二种：<br /> 先把这个关掉。。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190905150428188.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM1MTkzNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 在spider中书写，，<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190905150544898.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM1MTkzNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190905150628422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM1MTkzNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 这个IP地址会变化的。。更加稳定。。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019090515111274.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM1MTkzNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190905155350573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM1MTkzNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190905155403827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM1MTkzNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这种方法，我暂时不会设置使用MySQL；会起冲突，我以为的是本地的localhost和本地ip4的地址配置问题</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>Dark web爬虫</title>
		<link>https://uzzz.org/article/795.html</link>
				<pubDate>Fri, 30 Aug 2019 07:39:16 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Python]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/795.html</guid>
				<description><![CDATA[deep_web&#8211;python 如何进入 环境搭建 开始 demo 如何进入 url多以onion结尾，访问的方式与普通的域名访问方式也不相同，访问他们需要一款名叫Tor的浏览器。也叫洋葱浏览器 环境搭建 针对于win10系统: tor浏览器。 Vidalia控制器。 Tor控制器。 cow 。 还要有一款支持socks5协议的工具 开始 首先打]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<div class="toc">
<h3>deep_web&#8211;python</h3>
<ul>
<li><a href="#_2" rel="nofollow" data-token="22f4fb37863c57e3f17343ed845db9e6">如何进入</a></li>
<ul>
<li><a href="#_6" rel="nofollow" data-token="c73bec32456401c45e0aa4e6fcc2948e">环境搭建</a></li>
<li><a href="#_15" rel="nofollow" data-token="f93e6c402fce0ccf2d5ff74e2b7d5e5c">开始</a></li>
<li><a href="#demo_32" rel="nofollow" data-token="06bd62ebcb94d7d2db73e99c0f7b1742">demo</a></li>
</ul>
</ul></div>
</p>
<h1><a id="_2"></a>如何进入</h1>
<p>url多以onion结尾，访问的方式与普通的域名访问方式也不相同，访问他们需要一款名叫Tor的浏览器。也叫洋葱浏览器</p>
<h2><a id="_6"></a>环境搭建</h2>
<p>针对于win10系统:</p>
<ol>
<li>tor浏览器。</li>
<li>Vidalia控制器。</li>
<li>Tor控制器。</li>
<li>cow 。</li>
<li>还要有一款支持socks5协议的工具</li>
</ol>
<h2><a id="_15"></a>开始</h2>
<ol>
<li>
<p>首先打开tor浏览器，点击配置，点击选择内置网桥，选择中国可用的选项。出现以下界面说明你的tor连接成功了。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190426142422799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA0Njc4MQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</li>
<li>
<p>打开Vidalia控制面板，如图说明你的连接洋葱浏览器成功。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019042614262299.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA0Njc4MQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 点击设定在该目录中找到解压的tor控制器中的tor.exe文件<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/201904261428274.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA0Njc4MQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 3.网络设置<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190426143056868.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA0Njc4MQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 4.打开socks5软件，端口监听1080,好像ssr之类的默认就是1080<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190426143241954.png" alt="在这里插入图片描述"><br /> 5.打开cow文件夹中的rc.txt文件 修改为<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190426143418631.png" alt="在这里插入图片描述"><br /> 以后打开cow.exe<br /> 到此环境搭建完成</p>
</li>
</ol>
<h2><a id="demo_32"></a>demo</h2>
<pre><code class="prism language-javascript"><span class="token keyword">import</span> requests
proxies <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'http'</span><span class="token punctuation">:</span> <span class="token string">'http://127.0.0.1:7777'</span><span class="token punctuation">,</span> <span class="token string">'https'</span><span class="token punctuation">:</span> <span class="token string">'http://127.0.0.1:7777'</span><span class="token punctuation">}</span>
s <span class="token operator">=</span> requests<span class="token punctuation">.</span><span class="token function">Session</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
r <span class="token operator">=</span> s<span class="token punctuation">.</span><span class="token keyword">get</span><span class="token punctuation">(</span><span class="token string">"http://anonymzn3twqpxq5.onion/list.php?2"</span><span class="token punctuation">,</span> proxies <span class="token operator">=</span> proxies<span class="token punctuation">)</span>
<span class="token function">print</span><span class="token punctuation">(</span>r<span class="token punctuation">.</span>text<span class="token punctuation">)</span>

</code></pre>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190915172025956.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA0Njc4MQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>Python 语言介绍、IDE安装、新建第一个Python程序</title>
		<link>https://uzzz.org/article/1733.html</link>
				<pubDate>Fri, 12 Jul 2019 14:59:27 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Python]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1733.html</guid>
				<description><![CDATA[Python&#160;能够用于开发哪些应用？ web开发 数据分析 machine learning algorithm deep learning&#160; 后台服务 数据可视化&#160; data visualization&#160; &#160; Python&#8217; s disadvantages: 运行速度慢 代码不能加密 &#038;nbsp]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<p><strong>Python&nbsp;能够用于开发哪些应用？</strong></p>
<ol>
<li>web开发</li>
<li>数据分析</li>
<li>machine learning algorithm</li>
<li><span style="color:#f33b45;">deep learning&nbsp;</span></li>
<li>后台服务</li>
<li>数据可视化&nbsp; data visualization&nbsp;</li>
</ol>
<p>&nbsp;</p>
<p><strong>Python&#8217; s disadvantages:</strong></p>
<ol>
<li>运行速度慢</li>
<li>代码不能加密</li>
</ol>
<p>&nbsp;</p>
<p><strong>Python开发环境安装：</strong></p>
<ul>
<li>&nbsp;网页版可以安装anaconda&nbsp;使用 jupyter notebook进行开发</li>
</ul>
<p>下载地址：&nbsp;<a href="https://www.anaconda.com/distribution/" rel="nofollow" data-token="cfe700da413e42008fc7a1fa2c5772ff">https://www.anaconda.com/distribution/</a></p>
<ul>
<li>本地版可以安装pyCharm IDE&nbsp;集成环境</li>
</ul>
<p>下载地址：&nbsp;<a href="http://www.jetbrains.com/pycharm/download/#section=windows" rel="nofollow" data-token="a08857c448557a08bdbbd14f3f216439">http://www.jetbrains.com/pycharm/download/#section=windows</a></p>
<ul>
<li>可以安装轻量级的Wing&nbsp;IDE</li>
</ul>
<p>下载地址： wingware.com&nbsp;&nbsp;</p>
<p>&nbsp;</p>
<p><strong>创建一个Python程序：</strong></p>
<p>&nbsp;</p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>反反爬虫相关机制（面试必问，后续陆续添加）</title>
		<link>https://uzzz.org/article/933.html</link>
				<pubDate>Wed, 26 Jun 2019 15:05:52 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Python]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/933.html</guid>
				<description><![CDATA[通常防止爬虫被反主要有以下几个策略： 一.BAN IP 原因：某一个某一个时刻IP访问量特别特别大 ，或者是超出正常用户使用权限，导致服务器会偶尔把该IP放入黑名单 ，过一段时间再将其放出来 解决办法：分布式爬虫（分布式【分散url的手动分布式，以及框架分布式】）以及购买代理IP（Tor代理~~能买暗网代理），转化成app有的也有效 二.BAN USERAG]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<h3>通常防止爬虫被反主要有以下几个策略：</h3>
<h3><em>一.BAN IP</em></h3>
<p><u>原因：</u>某一个某一个时刻IP访问量特别特别大 ，或者是超出正常用户使用权限，导致服务器会偶尔把该IP放入黑名单 ，过一段时间再将其放出来</p>
<p><u>解决办法：</u>分布式爬虫（分布式【分散url的手动分布式，以及框架分布式】）以及购买代理IP（Tor代理~~能买<a href="https://cloud.tencent.com/developer/news/313473" rel="nofollow" data-token="92db766e7922b555c7acd27ff1e2b40e">暗网代理</a>），转化成app有的也有效</p>
<h3><strong>二.BAN USERAGENT</strong></h3>
<p><u>原因：</u>爬虫请求头就是默认的 python-requests/2.18.4 等等类型在headers的数据包，会直接拒绝访问，返回403错误</p>
<p><u>解决办法：伪装</u>浏览器头 ，添加其请求头，再发送请求</p>
<h3><strong>三.BAN COOKIES</strong></h3>
<p><u>原因：</u>服务器对每一个访问网页的人都set-cookie，给其一个cookies，当该cookies访问超过某一个阀值（请求次数或者timeout值）时就BAN掉该COOKIE</p>
<p><u>解决办法：</u></p>
<p>1,控制访问速度 ,</p>
<p>2,在某宝上买多个账号，生成多个cookies，在每一次访问时带上cookies</p>
<p>3,手动获取页面返回cookies</p>
<p>4，解析js拿到其js生成的cookies再带入请求（超级稳）</p>
<p>5，再没有遇到逆天js加载情况下无所不能的selenuim</p>
<p>案例：亚马逊等大型电商平台，马蜂窝</p>
<h3><strong>四.验证码验证</strong></h3>
<p><u>原因：</u>当某一用户访问次数过多后，就自动让请求跳转到一个验证码页面，只有在输入正确的验证码之后才能继续访问网站</p>
<p>解决办法：python可以通过一些第三方库如(pytesser,PIL)来对验证码进行处理，识别出正确的验证码，复杂的验证码可以通过机器学习让爬虫自动识别复杂验证码，让程序自动识别验证码并自动输入验证码继续抓取 ，还有最终手段打码平台（兔宝贝，超级鹰等）打码平台99%的能搞定，那1%也可以用打码平台的人工打码搞定。哈哈</p>
<p>短信验证：易码（专业手机短信验证好几年）</p>
<p>二维码验证：打码平台有支持扫码（神一样的验证，我没有遇到过，但听说过）</p>
<p>案例：淘宝，12306</p>
<h3><strong>五.javascript渲染</strong></h3>
<p><u>原因：</u>网页开发者将重要信息放在网页中但不写入html标签中，而浏览器会自动渲染&lt;script&gt;标签的js代码将信息展现在浏览器当中，而爬虫是不具备执行js代码的能力，所以无法将js事件产生的信息读取出来</p>
<p>解决办法：</p>
<p>1，通过分析提取script中的js代码来通过正则匹配提取信息内容或通过webdriver+phantomjs直接进行无头浏览器渲染网页。</p>
<p>2，通过解析js获取正确的资源地址</p>
<p>案例：前程无忧网 ，信用中国</p>
<h3><strong>六.ajax异步传输</strong></h3>
<p><u>原因：</u>访问网页的时候服务器将网页框架返回给客户端，在与客户端交互的过程中通过异步ajax技术传输数据包到客户端，呈现在网页上，爬虫直接抓取的话信息为空</p>
<p>解决办法：通过fiddler或是wireshark抓包分析ajax请求的界面，然后自己通过规律仿造服务器构造一个请求访问服务器得到返回的真实数据包。</p>
<p>案例：拉勾网</p>
<p>打开拉勾网的某一个工作招聘页，可以看到许许多多的招聘信息数据，点击下一页后发现页面框架不变化，url地址不变，而其中的每个招聘数据发生了变化，通过chrome开发者工具抓包找到了一个叫请求了一个叫做<a href="http://www.lagou.com/zhaopin/Java/2/?filterOption=3" rel="nofollow" data-token="57be332e22bc9fee020d07b1cf959651">http://www.lagou.com/zhaopin/Java/2/?filterOption=3</a>的网页，打开改网页发现为第二页真正的数据源，通过仿造请求可以抓取每一页的数据。</p>
<p>&nbsp;</p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>“深网” &#038;&#038; “暗网”</title>
		<link>https://uzzz.org/article/742.html</link>
				<pubDate>Sat, 13 Apr 2019 08:40:36 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Python]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/742.html</guid>
				<description><![CDATA[深网是网络的一部分，与浅网（surface Web）对立。浅网是互联网上搜索引擎可以抓到的那部分网络。据不完全统计，互联网中其实约 90% 的网络都是深网。因为谷歌不能做像表单提交这类事情，也找不到那些没有直接链接到顶层域名上的网页，或者因为有 robots.txt 禁止而不能查看网站，所以浅网的数量相对深网还是比较少的。 暗网，也被称为 Darknet 或]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<ul>
<li>
<p>深网是网络的一部分，与浅网（surface Web）对立。浅网是互联网上搜索引擎可以抓到的那部分网络。据不完全统计，互联网中其实约 90% 的网络都是深网。因为谷歌不能做像表单提交这类事情，也找不到那些没有直接链接到顶层域名上的网页，或者因为有 robots.txt 禁止而不能查看网站，所以浅网的数量相对深网还是比较少的。</p>
</li>
<li>
<p>暗网，也被称为 Darknet 或 dark Internet，完全是另一种“怪兽”。它们也建立在已有的网络基础上，但是使用 Tor 客户端，带有运行在 HTTP 之上的新协议，提供了一个信息交换的安全隧道。</p>
</li>
</ul></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>“深网” &#038;&#038; “暗网”</title>
		<link>https://uzzz.org/article/1349.html</link>
				<pubDate>Sat, 13 Apr 2019 08:40:36 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Python]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1349.html</guid>
				<description><![CDATA[深网是网络的一部分，与浅网（surface Web）对立。浅网是互联网上搜索引擎可以抓到的那部分网络。据不完全统计，互联网中其实约 90% 的网络都是深网。因为谷歌不能做像表单提交这类事情，也找不到那些没有直接链接到顶层域名上的网页，或者因为有 robots.txt 禁止而不能查看网站，所以浅网的数量相对深网还是比较少的。 暗网，也被称为 Darknet 或]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<ul>
<li>
<p>深网是网络的一部分，与浅网（surface Web）对立。浅网是互联网上搜索引擎可以抓到的那部分网络。据不完全统计，互联网中其实约 90% 的网络都是深网。因为谷歌不能做像表单提交这类事情，也找不到那些没有直接链接到顶层域名上的网页，或者因为有 robots.txt 禁止而不能查看网站，所以浅网的数量相对深网还是比较少的。</p>
</li>
<li>
<p>暗网，也被称为 Darknet 或 dark Internet，完全是另一种“怪兽”。它们也建立在已有的网络基础上，但是使用 Tor 客户端，带有运行在 HTTP 之上的新协议，提供了一个信息交换的安全隧道。</p>
</li>
</ul></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>使用darknet识别点选验证码详细过程（附带源码）</title>
		<link>https://uzzz.org/article/1407.html</link>
				<pubDate>Thu, 21 Mar 2019 15:06:33 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Python]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1407.html</guid>
				<description><![CDATA[项目源码：https://github.com/nickliqian/darknet_captcha darknet_captcha 项目基于darknet开发了一系列的快速启动脚本，旨在让图像识别新手或者开发人员能够快速的启动一个目标检测（定位）的项目。 如果有没有讲清楚的地方，欢迎提issue和PR，希望能和大家共同完善！ 本项目分为两个部分： 提供两个]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<p>项目源码：<a href="https://github.com/nickliqian/darknet_captcha" rel="nofollow" data-token="bbe2c7667b3860ca3d312bfc79eac8c5">https://github.com/nickliqian/darknet_captcha</a></p>
<h1><a id="darknet_captcha_1"></a>darknet_captcha</h1>
<p>项目基于darknet开发了一系列的快速启动脚本，旨在让图像识别新手或者开发人员能够快速的启动一个目标检测（定位）的项目。<br /> 如果有没有讲清楚的地方，欢迎提issue和PR，希望能和大家共同完善！</p>
<p>本项目分为两个部分：</p>
<ol>
<li>提供两个目标检测（<strong>单分类和多分类点选验证码</strong>）的例子，你可以通过例子熟悉定位yolo3定位网络的使用方式</li>
<li>基于darknet提供一系列API，用于使用<strong>自己的数据</strong>进行目标检测模型的训练，并提供web server的代码<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230022127.jpg" alt="在这里插入图片描述"><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230043287.jpg" alt="在这里插入图片描述"><br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230056211.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTE5ODQwNg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230103266.jpg" alt="在这里插入图片描述"></li>
</ol>
<h1><a id="_11"></a>目录</h1>
<ul>
<li><a href="#%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84" rel="nofollow" data-token="e3cbb68166d7c73612de3c0f2d8b45c6">项目结构</a></li>
<li><a href="#%E5%BC%80%E5%A7%8B%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90%EF%BC%9A%E5%8D%95%E7%B1%BB%E5%9E%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B" rel="nofollow" data-token="560cba2a0ed3ef673a7f8f820d8d4a1e">开始一个例子：单类型目标检测</a></li>
<li><a href="#%E7%AC%AC%E4%BA%8C%E4%B8%AA%E4%BE%8B%E5%AD%90%EF%BC%9A%E5%A4%9A%E7%B1%BB%E5%9E%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B" rel="nofollow" data-token="5dd41eef749a59fc22928cd1b18d037b">第二个例子：多类型目标检测</a></li>
<li><a href="#%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE" rel="nofollow" data-token="840083843945e74ca8091f34e29a5a50">训练自己的数据</a></li>
<li><a href="#web%E6%9C%8D%E5%8A%A1" rel="nofollow" data-token="f21cb879da123717190c98419efeebf1">Web服务</a></li>
<li><a href="#API%E6%96%87%E6%A1%A3" rel="nofollow" data-token="f87e5f2dde5b4e7171b2527339b00bac">API文档</a></li>
<li><a href="#%E5%85%B6%E4%BB%96%E9%97%AE%E9%A2%98" rel="nofollow" data-token="34df0cdf9eb93c33be13c1355c511280">其他问题</a>
<ul>
<li><a href="#%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91OSS%E5%8A%A0%E9%80%9F%E4%B8%8B%E8%BD%BD" rel="nofollow" data-token="661678172549864fcbc2e6f6281463c6">使用阿里云OSS加速下载</a></li>
<li><a href="#GPU%E4%BA%91%E6%8E%A8%E8%8D%90" rel="nofollow" data-token="4c5b24d02e2778a6aed6853623fd5195">GPU云推荐</a></li>
<li><a href="#CPU%E5%92%8CGPU%E8%AF%86%E5%88%AB%E9%80%9F%E5%BA%A6%E5%AF%B9%E6%AF%94" rel="nofollow" data-token="9ff9cbe779fc908b6dfe29bd8421dc09">CPU和GPU识别速度对比</a></li>
</ul>
</li>
<li><a href="#%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95" rel="nofollow" data-token="b8b4502a5899c83abe3e0bbf6b10e0ec">报错解决办法</a></li>
<li><a href="#TODO" rel="nofollow" data-token="da9cead761e3db3ac27337578bd89ec3">TODO</a></li>
</ul>
<h1><a id="_25"></a>项目结构</h1>
<p>项目分为<code>darknet、extent、app</code>三部分</p>
<ol>
<li>darknet: 这部分是<a href="https://github.com/pjreddie/darknet" rel="nofollow" data-token="993f4a46bae6e583288b4ebc86111b92">darknet</a>项目源码，没有作任何改动。</li>
<li>extent: 扩展部分，包含<strong>生成配置</strong>、<strong>生成样本</strong>、<strong>训练</strong>、<strong>识别demo</strong>、<strong>api程序</strong>。</li>
<li>app: 每一个新的识别需求都以app区分，其中包含配置文件、样本和标签文件等。</li>
</ol>
<h1><a id="_31"></a>开始一个例子：单类型目标检测</h1>
<p><strong>以点选验证码为例</strong><br /> darknet实际上给我们提供了一系列的深度学习算法，我们要做的就是使用比较简单的步骤来调用darknet训练我们的识别模型。</p>
<ul>
<li>推荐使用的操作系统是<code>ubuntu</code>，遇到的坑会少很多。</li>
<li>如果使用windowns系统，需要先安装<code>cygwin</code>，便于编译darknet。（参考我的博客：<a href="https://blog.csdn.net/weixin_39198406/article/details/83020632" rel="nofollow" data-token="d3dffad050d9d40d9ff00266a708da7e">安装cygwin</a>）</li>
</ul>
<p>下面的步骤都已经通过<code>ubuntu16.04</code>测试。</p>
<h4><a id="1_38"></a>1.下载项目</h4>
<pre><code>git clone https://github.com/nickliqian/darknet_captcha.git
</code></pre>
<h4><a id="2darknet_42"></a>2.编译darknet</h4>
<p>进入<code>darknet_captcha</code>目录，下载<code>darknet</code>项目，覆盖<code>darknet</code>目录：</p>
<pre><code>cd darknet_captcha
git clone https://github.com/pjreddie/darknet.git
</code></pre>
<p>进入<code>darknet</code>目录，修改<code>darknet/Makefile</code>配置文件</p>
<pre><code>cd darknet
vim Makefile
</code></pre>
<ul>
<li>如果使用GPU训练则下面的GPU=1</li>
<li>使用CPU训练则下面的GPU=0</li>
</ul>
<pre><code>GPU=1
CUDNN=0
OPENCV=0
OPENMP=0
DEBUG=0
</code></pre>
<p>然后使用<code>make</code>编译<code>darknet</code>：</p>
<pre><code>make
</code></pre>
<blockquote>
<p>不建议使用CPU进行训练，因为使用CPU不管是训练还是预测，耗时都非常久。<br /> 如果你需要租用临时且价格低的GPU主机进行测试，后面介绍了一些推荐的GPU云服务。<br /> 如果在编译过程中会出错，可以在darknet的issue找一下解决办法，也可以发邮件找我要旧版本的darknet。</p>
</blockquote>
<h4><a id="3python3_70"></a>3.安装python3环境</h4>
<p>使用pip执行下面的语句，并确保你的系统上已经安装了tk：</p>
<pre><code>pip install -r requirement.txt
sudo apt-get install python3-tk
</code></pre>
<h4><a id="4_77"></a>4.创建一个应用</h4>
<p>进入根目录，运行下面的程序生成一个应用的基本配置：</p>
<pre><code>cd darknet_captcha
python3 extend/create_app_config.py my_captcha 1
</code></pre>
<p>这里的类别默认生成<code>classes_1</code>，你可以修改类别名称；<br /> 打开<code>app/my_captcha/my_captcha.names</code>修改<code>classes_1</code>为主机想要的名称即可。</p>
<p>如何查看<code>create_app_config.py</code>的命令行参数解释？<br /> 直接运行<code>python create_app_config.py</code>便可以在控制台查看，下面的程序也是如此。</p>
<blockquote>
<p>如果你对darknet相关配置有一定的了解，可以直接打开文件修改参数的值，这里我们保持原样即可。</p>
</blockquote>
<h4><a id="5_91"></a>5.生成样本</h4>
<p>生成样本使用另外一个项目 <a href="https://github.com/nickliqian/generate_click_captcha" rel="nofollow" data-token="7b93780a4f9d60f2d43ef3b928cc2f56">nickliqian/generate_click_captcha</a><br /> 这里我已经集成进去了，执行下面的命令生成样本和对应标签到指定应用中<code>yolo</code>规定的目录：</p>
<pre><code>python3 extend/generate_click_captcha.py my_captcha
</code></pre>
<p>运行<code>python generate_click_captcha.py</code>查看参数解释。</p>
<h4><a id="6_99"></a>6.划分训练集和验证集</h4>
<p>运行下面的程序，划分训练集和验证集，同时将标签的值转换为<code>yolo</code>认识的格式：</p>
<pre><code>python3 extend/output_label.py my_captcha 1
</code></pre>
<p>这里填写的种类需要与上面一致。<br /> 运行<code>python output_label.py</code>查看参数解释。</p>
<h4><a id="7_107"></a>7.开始训练</h4>
<p>到这里，我们要准备的东西还差一样，我们需要下载darknet提供的预训练模型放在<code>darknet_captcha</code>目录下：</p>
<pre><code>wget https://pjreddie.com/media/files/darknet53.conv.74
</code></pre>
<p>在<code>darknet_captcha</code>目录下，执行下面的命令开始训练：</p>
<pre><code>./darknet/darknet detector train app/my_captcha/my_captcha.data app/my_captcha/my_captcha_train.yolov3.cfg darknet53.conv.74
</code></pre>
<p>训练过程中模型会每一百次迭代储存一次，储存在<code>app/my_captcha/backup/</code>下，可以进行查看。</p>
<h4><a id="8_118"></a>8.识别效果</h4>
<p>使用<code>GTX 1060</code>训练大概1.5小时，训练迭代到1000次，会有比较明显的效果。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230224316.jpg" alt="在这里插入图片描述"><br /> 我们找一张验证集的图片使用不同进度下的模型进行识别测试，执行下面的语句开始识别：</p>
<pre><code>python3 extend/rec.py my_captcha 100
</code></pre>
<p>这里的100是选择<code>app/my_captcha/images_data/JPEGImages</code>目录下的第一百张图片进行识别。<br /> 运行<code>python rec.py</code>查看参数解释。</p>
<p>迭代300次：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230246699.jpg" alt="在这里插入图片描述"><br /> 迭代800次：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230254808.jpg" alt="在这里插入图片描述"><br /> 迭代1000次：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230303211.jpg" alt="在这里插入图片描述"><br /> 迭代1200次：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230312599.jpg" alt="在这里插入图片描述"></p>
<h4><a id="9_137"></a>9.图片切割</h4>
<p>这部分比较简单，网上有很多示例代码，可以调用<code>darknet_interface.cut_and_save</code>方法把定位到的字符切割下来。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230421943.png" alt="在这里插入图片描述"></p>
<h4><a id="10_141"></a>10.分类器</h4>
<p>到分类这一步就比较容易了，可以使用darknet自带的分类器，也可以使用<a href="https://github.com/nickliqian/cnn_captcha" rel="nofollow" data-token="1beeb7f0521c4b0077accb44f90b0df3">cnn_captcha</a>一个使用卷积神经网络识别验证码的项目。</p>
<h4><a id="11_143"></a>11.总结</h4>
<p>我们识别点选验证码的大致流程如下：</p>
<ol>
<li>搜集样本</li>
<li>打标签（标注坐标和字符）</li>
<li>训练定位器</li>
<li>检测位置，切割图片</li>
<li>训练分类器</li>
<li>使用定位器+分类器识别点选验证码上字符的位置和字符类别</li>
</ol>
<h2><a id="_152"></a>第二个例子：多类型目标检测</h2>
<p>步骤和上面基本上一致，直接把命令列出来：</p>
<pre><code># 生成配置文件
python3 extend/create_app_config.py dummy_captcha 2
# 生成图片
python3 extend/generate_click_captcha.py dummy_captcha 500 True
# 输出标签到txt
python3 extend/output_label.py dummy_captcha 2
# 开始训练w
./darknet/darknet detector train app/dummy_captcha/dummy_captcha.data app/dummy_captcha/dummy_captcha_train.yolov3.cfg darknet53.conv.74
# 识别测试
python3 extend/rec.py dummy_captcha 100
</code></pre>
<h2><a id="_167"></a>训练自己的数据</h2>
<p>下面的过程教你如何训练自己数据。<br /> 假定我们要创建一个识别路上的车和人的应用，因此类别数量为2。<br /> 假定你现在有一些原始图片，首先你需要给这些图片打上标签，推荐使用<a href="https://github.com/tzutalin/labelImg" rel="nofollow" data-token="1e1d91f57353c3b2de80c67156a204c0">labelImg</a>进行打标工作。<br /> 使用教程可以自行谷歌，软件界面大致如下：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190321230438799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTE5ODQwNg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>给图片中的人和车分别打上person和car的标签，会生成xml标签文件。<br /> 接下来，我们创建一个应用，应用名称是<code>car</code>，类别为<code>2</code>类，同时生成一些配置文件：</p>
<pre><code>python3 extend/create_app_config.py car 2
</code></pre>
<p>然后把你的原始图片放到指定的路径<code>app/car/JPEGImages</code>，把xml标签文件放在<code>app/car/Annotations</code><br /> yolo训练的时候需要图片中目标的相对坐标，所以这里需要把xml的坐标计算为相对坐标的形式。<br /> 同时car.data中需要分别定义训练集和验证集的样本路径，这里会划分出训练集和验证集，同时生成两个txt文件记录其路径。</p>
<pre><code>python3 extend/output_label.py car 2
</code></pre>
<p>要提到的是，这里可以打开car.names，把里面的class_1和class_2分别修改为car和person，这里识别结果就会输出car和person。<br /> 然后就可以开始训练了：</p>
<pre><code>./darknet/darknet detector train app/car/car.data app/car/car_train.yolov3.cfg darknet53.conv.74
</code></pre>
<p>识别测试和上面也没有上面区别：</p>
<pre><code># 识别测试
python3 extend/rec.py car 100
</code></pre>
<h2><a id="web_195"></a>web服务</h2>
<p>启动web服务：</p>
<pre><code>python3 extend/web_server.py
</code></pre>
<p>启动前需要按需修改配置参数：</p>
<pre><code># 生成识别对象，需要配置参数
app_name = "car"  # 应用名称
config_file = "app/{}/{}_train.yolov3.cfg".format(app_name, app_name)  # 配置文件路径
model_file = "app/{}/backup/{}_train.backup".format(app_name, app_name)  # 模型路径
data_config_file = "app/{}/{}.data".format(app_name, app_name)  # 数据配置文件路径
dr = DarknetRecognize(
    config_file=config_file,
    model_file=model_file,
    data_config_file=data_config_file
)
save_path = "api_images"  # 保存图片的路径
</code></pre>
<p>使用下面的脚本<code>request_api.py</code>进行web服务的识别测试（注意修改图片路径）:</p>
<pre><code>python3 extend/request_api.py
</code></pre>
<p>返回响应，响应包含目标类别和中心点位置：</p>
<pre><code>接口响应: {
  "speed_time(ms)": 16469, 
  "time": "15472704635706885", 
  "value": [
    [
      "word", 
      0.9995613694190979, 
      [
        214.47508239746094, 
        105.97418212890625, 
        24.86412811279297, 
        33.40662384033203
      ]
    ],
    ...
}
</code></pre>
<h2><a id="API_237"></a>API文档</h2>
<p>暂无</p>
<h2><a id="_240"></a>其他问题</h2>
<h3><a id="OSS_241"></a>使用阿里云OSS加速下载</h3>
<p>如果你使用国外云主机进行训练，训练好的模型的下载速度确实是一个问题。<br /> 这里推荐使用阿里云oss，在云主机上把文件上传上去，然后使用oss下载下来。<br /> 配置秘钥：</p>
<pre><code># 从环境变量获取密钥
AccessKeyId = os.getenv("AccessKeyId")
AccessKeySecret = os.getenv("AccessKeySecret")
BucketName = os.getenv("BucketName")
</code></pre>
<p>上传图片：</p>
<pre><code>python3 extend/upload2oss.py app/my_captcha/images_data/JPEGImages/1_15463317590530567.jpg
python3 extend/upload2oss.py text.jpg
</code></pre>
<h3><a id="GPU_257"></a>GPU云推荐</h3>
<p>使用租用 vectordash GPU云主机，ssh连接集成了Nvidia深度学习环境的ubuntu16.04系统<br /> 包含以下工具或框架：</p>
<pre><code>CUDA 9.0, cuDNN, Tensorflow, PyTorch, Caffe, Keras
</code></pre>
<p>vectordash提供了一个客户端，具备远程连接、上传和下载文件、管理多个云主机等。<br /> 下面是几种显卡的租用价格：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019032123045338.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTE5ODQwNg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 创建实例后，面板会提供一个秘钥，输入秘钥后，就可以使用客户端操作了：</p>
<pre><code># 安装客户端
pip install vectordash --upgrade
# 登录
vectordash login
# 列出主机
vectordash list
# ssh登录
vectordash ssh &lt;instance_id&gt;
# 打开jupyter
vectordash jupyter &lt;instance_id&gt;
# 上传文件
vectordash push &lt;instance_id&gt; &lt;from_path&gt; &lt;to_path&gt;
# 下载文件
vectordash pull &lt;instance_id&gt; &lt;from_path&gt; &lt;to_path&gt;
</code></pre>
<p>由于vectordash主机在国外，所以上传和下载都很慢，建议临时租用一台阿里云竞价突发型实例（约7分钱一小时）作为中转使用。</p>
<h3><a id="CPUGPU_285"></a>CPU和GPU识别速度对比</h3>
<p>GTX 1060, 识别耗时1s</p>
<pre><code>[load model] speed time: 4.691879987716675s
[detect image - i] speed time: 1.002530813217163s
</code></pre>
<p>CPU, 识别耗时13s</p>
<pre><code>[load model] speed time: 3.313053846359253s
[detect image - i] speed time: 13.256595849990845s
</code></pre>
<h2><a id="_297"></a>报错解决办法</h2>
<ol>
<li>UnicodeEncodeError: ‘ascii’ codec can’t encode character ‘\U0001f621’ in posit<br /> <a href="https://blog.csdn.net/u011415481/article/details/80794567" rel="nofollow" data-token="39d2177d12495f5471ded8c7eb62aa46">参考链接</a></li>
<li>pip install, locale.Error: unsupported locale setting<br /> <a href="https://blog.csdn.net/qq_33232071/article/details/51108062" rel="nofollow" data-token="99aa19d8b859c366914ec7034be979f2">参考链接</a></li>
</ol>
<h2><a id="TODO_303"></a>TODO</h2>
<ol>
<li>支持多类别检测的识别和训练 <strong>Done</strong></li>
<li>WebServer API调用 <strong>Done</strong></li>
<li>分类器</li>
</ol></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>暗文密码输入</title>
		<link>https://uzzz.org/article/954.html</link>
				<pubDate>Sun, 24 Feb 2019 07:19:46 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Python]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/954.html</guid>
				<description><![CDATA[import getpass _usrname = "Alex" _password = "123" username = input("username:") password = getpass.getpass("password:") if _usrname == username and _password == password: print("W]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<pre class="has">
<code class="language-python">import getpass

_usrname = "Alex"
_password = "123"

username = input("username:")
password = getpass.getpass("password:")

if _usrname == username and _password == password:
    print("Welcome user {name} login...".format(name=username))
else:
    print("Invalid username or password!")


#在pycharm中getpass不好使，会卡机</code></pre>
<p>&nbsp;</p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>4.python-爬虫的基础认知，爬虫的几大分类？</title>
		<link>https://uzzz.org/article/1565.html</link>
				<pubDate>Fri, 01 Feb 2019 06:21:56 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Python]]></category>
		<category><![CDATA[搜索推荐]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1565.html</guid>
				<description><![CDATA[分类 来自：百度百科 &#160; &#160; &#160; &#160; 网络爬虫按照系统结构和实现技术，大致可以分为以下几种类型：通用网络爬虫（General Purpose Web Crawler）、聚焦网络爬虫（Focused Web Crawler）、增量式网络爬虫（Incremental Web Crawler）、深层网络爬虫（Deep Web]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<h2><strong>分类</strong></h2>
<p><u><em><strong>来自：百度百科</strong></em></u></p>
<hr>
<blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 网络爬虫按照系统结构和实现技术，大致可以分为以下几种类型：通用网络爬虫（General Purpose Web Crawler）、聚焦网络爬虫（Focused Web Crawler）、增量式网络爬虫（Incremental Web Crawler）、深层网络爬虫（Deep Web Crawler）。 实际的网络爬虫系统通常是几种爬虫技术相结合实现的&nbsp;<a name="ref_[1]_284853">&nbsp;</a>&nbsp;<a name="ref_1"></a>。</p>
</blockquote>
<hr>
<h2>&nbsp;<strong>通用网络爬虫</strong></h2>
<hr>
<blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 通用网络爬虫又称全网爬虫（Scalable Web Crawler），爬行对象从一些种子 URL 扩充到整个 Web，主要为门户站点搜索引擎和大型 Web 服务提供商采集数据。 由于商业原因，它们的技术细节很少公布出来。 这类网络爬虫的爬行范围和数量巨大，对于爬行速度和存储空间要求较高，对于爬行页面的顺序要求相对较低，同时由于待刷新的页面太多，通常采用并行工作方式，但需要较长时间才能刷新一次页面。 虽然存在一定缺陷，通用网络爬虫适用于为搜索引擎搜索广泛的主题，有较强的应用价值。</p>
</blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 通用网络爬虫的结构大致可以分为页面爬行模块 、页面分析模块、链接过滤模块、页面数据库、URL 队列、初始 URL 集合几个部分。为提高工作效率，通用网络爬虫会采取一定的爬行策略。 常用的爬行策略有：深度优先策略、广度优先策略&nbsp;。</p>
<p>1) 深度优先策略：其基本方法是按照深度由低到高的顺序，依次访问下一级网页链接，直到不能再深入为止。 爬虫在完成一个爬行分支后返回到上一链接节点进一步搜索其它链接。 当所有链接遍历完后，爬行任务结束。 这种策略比较适合垂直搜索或站内搜索， 但爬行页面内容层次较深的站点时会造成资源的巨大浪费&nbsp;&nbsp;。</p>
<p>2) 广度优先策略：此策略按照网页内容目录层次深浅来爬行页面，处于较浅目录层次的页面首先被爬行。 当同一层次中的页面爬行完毕后，爬虫再深入下一层继续爬行。 这种策略能够有效控制页面的爬行深度，避免遇到一个无穷深层分支时无法结束爬行的问题，实现方便，无需存储大量中间节点，不足之处在于需较长时间才能爬行到目录层次较深的页面&nbsp;&nbsp;。</p>
<hr>
<h2><strong>聚焦网络爬虫</strong></h2>
<hr>
<blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 聚焦网络爬虫（Focused Crawler），又称主题网络爬虫（Topical Crawler），是指选择性地爬行那些与预先定义好的主题相关页面的网络爬虫[8]。 和通用网络爬虫相比，聚焦爬虫只需要爬行与主题相关的页面，极大地节省了硬件和网络资源，保存的页面也由于数量少而更新快，还可以很好地满足一些特定人群对特定领域信息的需求&nbsp;&nbsp;。</p>
</blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 聚焦网络爬虫和通用网络爬虫相比，增加了链接评价模块以及内容评价模块。聚焦爬虫爬行策略实现的关键是评价页面内容和链接的重要性，不同的方法计算出的重要性不同，由此导致链接的访问顺序也不同&nbsp;<a>&nbsp;</a>&nbsp;。</p>
<p>1) 基于内容评价的爬行策略：DeBra将文本相似度的计算方法引入到网络爬虫中，提出了 Fish Search 算法，它将用户输入的查询词作为主题，包含查询词的页面被视为与主题相关，其局限性在于无法评价页面与主题相关 度 的 高 低 。 Herseovic对 Fish Search 算 法 进 行 了 改 进 ，提 出 了 Sharksearch 算法，利用空间向量模型计算页面与主题的相关度大小&nbsp;&nbsp;。</p>
<p>2) 基于链接结构评价的爬行策略 ：Web 页面作为一种半结构化文档，包含很多结构信息，可用来评价链接重要性。 PageRank 算法最初用于搜索引擎信息检索中对查询结果进行排序，也可用于评价链接重要性，具体做法就是每次选择 PageRank 值较大页面中的链接来访问。 另一个利用 Web结构评价链接价值的方法是 HITS 方法，它通过计算每个已访问页面的 Authority 权重和 Hub 权重，并以此决定链接的访问顺序&nbsp;&nbsp;。</p>
<p>3) 基于增强学习的爬行策略：Rennie 和 McCallum 将增强学习引入聚焦爬虫，利用贝叶斯分类器，根据整个网页文本和链接文本对超链接进行分类，为每个链接计算出重要性，从而决定链接的访问顺序&nbsp;&nbsp;。</p>
<p>4) 基于语境图的爬行策略：Diligenti 等人提出了一种通过建立语境图（Context Graphs）学习网页之间的相关度，训练一个机器学习系统，通过该系统可计算当前页面到相关 Web 页面的距离，距离越近的页面中的链接优先访问。印度理工大学（IIT）和 IBM 研究中心的研究人员开发了一个典型的聚焦网络爬虫。 该爬虫对主题的定义既不是采用关键词也不是加权矢量，而是一组具有相同主题的网页。 它包含两个重要模块：一个是分类器，用来计算所爬行的页面与主题的相关度，确定是否与主题相关；另一个是净化器，用来识别通过较少链接连接到大量相关页面的中心页面&nbsp;&nbsp;。</p>
<hr>
<h2><strong>增量式网络爬虫</strong></h2>
<hr>
<blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 增量式网络爬虫（Incremental Web Crawler）是 指 对 已 下 载 网 页 采 取 增 量式更新和只爬行新产生的或者已经发生变化网页的爬虫，它能够在一定程度上保证所爬行的页面是尽可能新的页面。 和周期性爬行和刷新页面的网络爬虫相比，增量式爬虫只会在需要的时候爬行新产生或发生更新的页面 ，并不重新下载没有发生变化的页面，可有效减少数据下载量，及时更新已爬行的网页，减小时间和空间上的耗费，但是增加了爬行算法的复杂度和实现难度。增量式网络爬虫的体系结构[包含爬行模块、排序模块、更新模块、本地页面集、待爬行 URL 集以及本地页面URL 集&nbsp;<a>&nbsp;</a>&nbsp;。</p>
</blockquote>
<p>增量式爬虫有两个目标：保持本地页面集中存储的页面为最新页面和提高本地页面集中页面的质量。 为实现第一个目标，增量式爬虫需要通过重新访问网页来更新本地页面集中页面内容，常用的方法有：1) 统一更新法：爬虫以相同的频率访问所有网页，不考虑网页的改变频率；2) 个体更新法：爬虫根据个体网页的改变频率来重新访问各页面；3) 基于分类的更新法：爬虫根据网页改变频率将其分为更新较快网页子集和更新较慢网页子集两类，然后以不同的频率访问这两类网页&nbsp;。</p>
<p>为实现第二个目标，增量式爬虫需要对网页的重要性排序，常用的策略有：广度优先策略、PageRank 优先策略等。IBM 开发的 WebFountain是一个功能强大的增量式网络爬虫，它采用一个优化模型控制爬行过程，并没有对页面变化过程做任何统计假设，而是采用一种自适应的方法根据先前爬行周期里爬行结果和网页实际变化速度对页面更新频率进行调整。北京大学的天网增量爬行系统旨在爬行国内 Web，将网页分为变化网页和新网页两类，分别采用不同爬行策略。 为缓解对大量网页变化历史维护导致的性能瓶颈，它根据网页变化时间局部性规律，在短时期内直接爬行多次变化的网页 ，为尽快获取新网页，它利用索引型网页跟踪新出现网页&nbsp;<a>&nbsp;</a>&nbsp;。</p>
<hr>
<h2><strong>Deep Web 爬虫</strong></h2>
<hr>
<blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Web 页面按存在方式可以分为表层网页（Surface Web）和深层网页（Deep Web，也称 Invisible Web Pages 或 Hidden Web）。 表层网页是指传统搜索引擎可以索引的页面，以超链接可以到达的静态网页为主构成的 Web 页面。Deep Web 是那些大部分内容不能通过静态链接获取的、隐藏在搜索表单后的，只有用户提交一些关键词才能获得的 Web 页面。例如那些用户注册后内容才可见的网页就属于 Deep Web。 2000 年 Bright Planet 指出：Deep Web 中可访问信息容量是 Surface Web 的几百倍，是互联网上最大、发展最快的新型信息资源&nbsp;。</p>
</blockquote>
<p>Deep Web 爬虫体系结构包含六个基本功能模块 （爬行控制器、解析器、表单分析器、表单处理器、响应分析器、LVS 控制器）和两个爬虫内部数据结构（URL 列表、LVS 表）。 其中 LVS（Label Value Set）表示标签/数值集合，用来表示填充表单的数据源&nbsp;&nbsp;。</p>
<p>Deep Web 爬虫爬行过程中最重要部分就是表单填写，包含两种类型：</p>
<p>1) 基于领域知识的表单填写：此方法一般会维持一个本体库，通过语义分析来选取合适的关键词填写表单。 Yiyao Lu[25]等人提出一种获取 Form 表单信息的多注解方法，将数据表单按语义分配到各个组中 ，对每组从多方面注解，结合各种注解结果来预测一个最终的注解标签；郑冬冬等人利用一个预定义的领域本体知识库来识别 Deep Web 页面内容， 同时利用一些来自 Web 站点导航模式来识别自动填写表单时所需进行的路径导航&nbsp;&nbsp;。</p>
<p>2) 基于网页结构分析的表单填写： 此方法一般无领域知识或仅有有限的领域知识，将网页表单表示成 DOM 树，从中提取表单各字段值。 Desouky 等人提出一种 LEHW 方法，该方法将 HTML 网页表示为DOM 树形式，将表单区分为单属性表单和多属性表单，分别进行处理；孙彬等人提出一种基于 XQuery 的搜索系统，它能够模拟表单和特殊页面标记切换，把网页关键字切换信息描述为三元组单元，按照一定规则排除无效表单，将 Web 文档构造成 DOM 树，利用 XQuery 将文字属性映射到表单字段。</p>
<p>Raghavan 等人提出的 HIWE 系统中，爬行管理器负责管理整个爬行过程，分析下载的页面，将包含表单的页面提交表单处理器处理，表单处理器先从页面中提取表单，从预先准备好的数据集中选择数据自动填充并提交表单，由爬行控制器下载相应的结果页面。</p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>4.python-爬虫的基础认知，爬虫的几大分类？</title>
		<link>https://uzzz.org/article/1633.html</link>
				<pubDate>Fri, 01 Feb 2019 06:21:56 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Python]]></category>
		<category><![CDATA[搜索推荐]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1633.html</guid>
				<description><![CDATA[分类 来自：百度百科 &#160; &#160; &#160; &#160; 网络爬虫按照系统结构和实现技术，大致可以分为以下几种类型：通用网络爬虫（General Purpose Web Crawler）、聚焦网络爬虫（Focused Web Crawler）、增量式网络爬虫（Incremental Web Crawler）、深层网络爬虫（Deep Web]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<h2><strong>分类</strong></h2>
<p><u><em><strong>来自：百度百科</strong></em></u></p>
<hr>
<blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 网络爬虫按照系统结构和实现技术，大致可以分为以下几种类型：通用网络爬虫（General Purpose Web Crawler）、聚焦网络爬虫（Focused Web Crawler）、增量式网络爬虫（Incremental Web Crawler）、深层网络爬虫（Deep Web Crawler）。 实际的网络爬虫系统通常是几种爬虫技术相结合实现的&nbsp;<a name="ref_[1]_284853">&nbsp;</a>&nbsp;<a name="ref_1"></a>。</p>
</blockquote>
<hr>
<h2>&nbsp;<strong>通用网络爬虫</strong></h2>
<hr>
<blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 通用网络爬虫又称全网爬虫（Scalable Web Crawler），爬行对象从一些种子 URL 扩充到整个 Web，主要为门户站点搜索引擎和大型 Web 服务提供商采集数据。 由于商业原因，它们的技术细节很少公布出来。 这类网络爬虫的爬行范围和数量巨大，对于爬行速度和存储空间要求较高，对于爬行页面的顺序要求相对较低，同时由于待刷新的页面太多，通常采用并行工作方式，但需要较长时间才能刷新一次页面。 虽然存在一定缺陷，通用网络爬虫适用于为搜索引擎搜索广泛的主题，有较强的应用价值。</p>
</blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 通用网络爬虫的结构大致可以分为页面爬行模块 、页面分析模块、链接过滤模块、页面数据库、URL 队列、初始 URL 集合几个部分。为提高工作效率，通用网络爬虫会采取一定的爬行策略。 常用的爬行策略有：深度优先策略、广度优先策略&nbsp;。</p>
<p>1) 深度优先策略：其基本方法是按照深度由低到高的顺序，依次访问下一级网页链接，直到不能再深入为止。 爬虫在完成一个爬行分支后返回到上一链接节点进一步搜索其它链接。 当所有链接遍历完后，爬行任务结束。 这种策略比较适合垂直搜索或站内搜索， 但爬行页面内容层次较深的站点时会造成资源的巨大浪费&nbsp;&nbsp;。</p>
<p>2) 广度优先策略：此策略按照网页内容目录层次深浅来爬行页面，处于较浅目录层次的页面首先被爬行。 当同一层次中的页面爬行完毕后，爬虫再深入下一层继续爬行。 这种策略能够有效控制页面的爬行深度，避免遇到一个无穷深层分支时无法结束爬行的问题，实现方便，无需存储大量中间节点，不足之处在于需较长时间才能爬行到目录层次较深的页面&nbsp;&nbsp;。</p>
<hr>
<h2><strong>聚焦网络爬虫</strong></h2>
<hr>
<blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 聚焦网络爬虫（Focused Crawler），又称主题网络爬虫（Topical Crawler），是指选择性地爬行那些与预先定义好的主题相关页面的网络爬虫[8]。 和通用网络爬虫相比，聚焦爬虫只需要爬行与主题相关的页面，极大地节省了硬件和网络资源，保存的页面也由于数量少而更新快，还可以很好地满足一些特定人群对特定领域信息的需求&nbsp;&nbsp;。</p>
</blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 聚焦网络爬虫和通用网络爬虫相比，增加了链接评价模块以及内容评价模块。聚焦爬虫爬行策略实现的关键是评价页面内容和链接的重要性，不同的方法计算出的重要性不同，由此导致链接的访问顺序也不同&nbsp;<a>&nbsp;</a>&nbsp;。</p>
<p>1) 基于内容评价的爬行策略：DeBra将文本相似度的计算方法引入到网络爬虫中，提出了 Fish Search 算法，它将用户输入的查询词作为主题，包含查询词的页面被视为与主题相关，其局限性在于无法评价页面与主题相关 度 的 高 低 。 Herseovic对 Fish Search 算 法 进 行 了 改 进 ，提 出 了 Sharksearch 算法，利用空间向量模型计算页面与主题的相关度大小&nbsp;&nbsp;。</p>
<p>2) 基于链接结构评价的爬行策略 ：Web 页面作为一种半结构化文档，包含很多结构信息，可用来评价链接重要性。 PageRank 算法最初用于搜索引擎信息检索中对查询结果进行排序，也可用于评价链接重要性，具体做法就是每次选择 PageRank 值较大页面中的链接来访问。 另一个利用 Web结构评价链接价值的方法是 HITS 方法，它通过计算每个已访问页面的 Authority 权重和 Hub 权重，并以此决定链接的访问顺序&nbsp;&nbsp;。</p>
<p>3) 基于增强学习的爬行策略：Rennie 和 McCallum 将增强学习引入聚焦爬虫，利用贝叶斯分类器，根据整个网页文本和链接文本对超链接进行分类，为每个链接计算出重要性，从而决定链接的访问顺序&nbsp;&nbsp;。</p>
<p>4) 基于语境图的爬行策略：Diligenti 等人提出了一种通过建立语境图（Context Graphs）学习网页之间的相关度，训练一个机器学习系统，通过该系统可计算当前页面到相关 Web 页面的距离，距离越近的页面中的链接优先访问。印度理工大学（IIT）和 IBM 研究中心的研究人员开发了一个典型的聚焦网络爬虫。 该爬虫对主题的定义既不是采用关键词也不是加权矢量，而是一组具有相同主题的网页。 它包含两个重要模块：一个是分类器，用来计算所爬行的页面与主题的相关度，确定是否与主题相关；另一个是净化器，用来识别通过较少链接连接到大量相关页面的中心页面&nbsp;&nbsp;。</p>
<hr>
<h2><strong>增量式网络爬虫</strong></h2>
<hr>
<blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 增量式网络爬虫（Incremental Web Crawler）是 指 对 已 下 载 网 页 采 取 增 量式更新和只爬行新产生的或者已经发生变化网页的爬虫，它能够在一定程度上保证所爬行的页面是尽可能新的页面。 和周期性爬行和刷新页面的网络爬虫相比，增量式爬虫只会在需要的时候爬行新产生或发生更新的页面 ，并不重新下载没有发生变化的页面，可有效减少数据下载量，及时更新已爬行的网页，减小时间和空间上的耗费，但是增加了爬行算法的复杂度和实现难度。增量式网络爬虫的体系结构[包含爬行模块、排序模块、更新模块、本地页面集、待爬行 URL 集以及本地页面URL 集&nbsp;<a>&nbsp;</a>&nbsp;。</p>
</blockquote>
<p>增量式爬虫有两个目标：保持本地页面集中存储的页面为最新页面和提高本地页面集中页面的质量。 为实现第一个目标，增量式爬虫需要通过重新访问网页来更新本地页面集中页面内容，常用的方法有：1) 统一更新法：爬虫以相同的频率访问所有网页，不考虑网页的改变频率；2) 个体更新法：爬虫根据个体网页的改变频率来重新访问各页面；3) 基于分类的更新法：爬虫根据网页改变频率将其分为更新较快网页子集和更新较慢网页子集两类，然后以不同的频率访问这两类网页&nbsp;。</p>
<p>为实现第二个目标，增量式爬虫需要对网页的重要性排序，常用的策略有：广度优先策略、PageRank 优先策略等。IBM 开发的 WebFountain是一个功能强大的增量式网络爬虫，它采用一个优化模型控制爬行过程，并没有对页面变化过程做任何统计假设，而是采用一种自适应的方法根据先前爬行周期里爬行结果和网页实际变化速度对页面更新频率进行调整。北京大学的天网增量爬行系统旨在爬行国内 Web，将网页分为变化网页和新网页两类，分别采用不同爬行策略。 为缓解对大量网页变化历史维护导致的性能瓶颈，它根据网页变化时间局部性规律，在短时期内直接爬行多次变化的网页 ，为尽快获取新网页，它利用索引型网页跟踪新出现网页&nbsp;<a>&nbsp;</a>&nbsp;。</p>
<hr>
<h2><strong>Deep Web 爬虫</strong></h2>
<hr>
<blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Web 页面按存在方式可以分为表层网页（Surface Web）和深层网页（Deep Web，也称 Invisible Web Pages 或 Hidden Web）。 表层网页是指传统搜索引擎可以索引的页面，以超链接可以到达的静态网页为主构成的 Web 页面。Deep Web 是那些大部分内容不能通过静态链接获取的、隐藏在搜索表单后的，只有用户提交一些关键词才能获得的 Web 页面。例如那些用户注册后内容才可见的网页就属于 Deep Web。 2000 年 Bright Planet 指出：Deep Web 中可访问信息容量是 Surface Web 的几百倍，是互联网上最大、发展最快的新型信息资源&nbsp;。</p>
</blockquote>
<p>Deep Web 爬虫体系结构包含六个基本功能模块 （爬行控制器、解析器、表单分析器、表单处理器、响应分析器、LVS 控制器）和两个爬虫内部数据结构（URL 列表、LVS 表）。 其中 LVS（Label Value Set）表示标签/数值集合，用来表示填充表单的数据源&nbsp;&nbsp;。</p>
<p>Deep Web 爬虫爬行过程中最重要部分就是表单填写，包含两种类型：</p>
<p>1) 基于领域知识的表单填写：此方法一般会维持一个本体库，通过语义分析来选取合适的关键词填写表单。 Yiyao Lu[25]等人提出一种获取 Form 表单信息的多注解方法，将数据表单按语义分配到各个组中 ，对每组从多方面注解，结合各种注解结果来预测一个最终的注解标签；郑冬冬等人利用一个预定义的领域本体知识库来识别 Deep Web 页面内容， 同时利用一些来自 Web 站点导航模式来识别自动填写表单时所需进行的路径导航&nbsp;&nbsp;。</p>
<p>2) 基于网页结构分析的表单填写： 此方法一般无领域知识或仅有有限的领域知识，将网页表单表示成 DOM 树，从中提取表单各字段值。 Desouky 等人提出一种 LEHW 方法，该方法将 HTML 网页表示为DOM 树形式，将表单区分为单属性表单和多属性表单，分别进行处理；孙彬等人提出一种基于 XQuery 的搜索系统，它能够模拟表单和特殊页面标记切换，把网页关键字切换信息描述为三元组单元，按照一定规则排除无效表单，将 Web 文档构造成 DOM 树，利用 XQuery 将文字属性映射到表单字段。</p>
<p>Raghavan 等人提出的 HIWE 系统中，爬行管理器负责管理整个爬行过程，分析下载的页面，将包含表单的页面提交表单处理器处理，表单处理器先从页面中提取表单，从预先准备好的数据集中选择数据自动填充并提交表单，由爬行控制器下载相应的结果页面。</p>
</p></div>
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
